{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47715422-e6e5-4d33-b19a-b55ab20b9119",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b46827f9-deea-4aae-a1bb-2868aa75d262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import prada\n",
    "import pandas as pd\n",
    "import veritas\n",
    "import tree_compress\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.ticker import (MultipleLocator, AutoMinorLocator)\n",
    "\n",
    "\n",
    "SMALL_SIZE = 8\n",
    "MEDIUM_SIZE = 10\n",
    "BIGGER_SIZE = 12\n",
    "\n",
    "plt.rc('font', size=MEDIUM_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "plt.rc('lines', markersize=5)\n",
    "plt.rc(\"lines\", linewidth=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba23f115-1b7d-4d6b-9a60-9056518c9046",
   "metadata": {},
   "source": [
    "## Train models\n",
    "\n",
    "We use hyperparameter tuning using the grid below to find a reasonable model for each of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a1648d6-9a04-491d-8661-3bfa275b5528",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnames = [\"Chaahat\"]\n",
    "param_dict = {\n",
    "    \"n_estimators\": [50, 100],\n",
    "    \"learning_rate\": [0.5, 0.2],\n",
    "    \"subsample\": [0.5, 0.75, 1.0],\n",
    "    \"max_depth\": [6],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77dc5deb-22e3-4a47-90a1-80ebf30956e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST ChaahatRegTarget0 train mse 0.219, valid mse 0.219, test mse 0.325\n",
      "params {'n_estimators': 100, 'learning_rate': 0.2, 'subsample': 0.75, 'max_depth': 6}\n",
      "\n",
      "BEST ChaahatRegTarget1 train mse 0.066, valid mse 0.066, test mse 0.102\n",
      "params {'n_estimators': 100, 'learning_rate': 0.2, 'subsample': 1.0, 'max_depth': 6}\n",
      "\n",
      "BEST ChaahatRegTarget2 train mse 0.043, valid mse 0.043, test mse 0.071\n",
      "params {'n_estimators': 100, 'learning_rate': 0.5, 'subsample': 1.0, 'max_depth': 6}\n",
      "\n",
      "BEST ChaahatRegTarget3 train mse 0.042, valid mse 0.042, test mse 0.080\n",
      "params {'n_estimators': 100, 'learning_rate': 0.5, 'subsample': 1.0, 'max_depth': 6}\n",
      "\n",
      "BEST ChaahatRegTarget4 train mse 0.065, valid mse 0.065, test mse 0.109\n",
      "params {'n_estimators': 100, 'learning_rate': 0.5, 'subsample': 0.75, 'max_depth': 6}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_models = {}\n",
    "dmts = {}\n",
    "for dname in dnames:\n",
    "    dmt = prada.get_dataset(dname, nfolds=5, silent=True, seed=12)\n",
    "    dmt.load_dataset()\n",
    "    dmt.robust_normalize()\n",
    "    dmt.astype(np.float64)\n",
    "\n",
    "    dmts[dname] = dmt\n",
    "\n",
    "    for target in range(dmt.yscores.shape[1]):\n",
    "        d = dmt.to_singletarget(target)\n",
    "\n",
    "        dtrain, dtest = d.train_and_test_fold(0)\n",
    "        dtrain, dvalid = dtrain.train_and_test_fold(0)\n",
    "        \n",
    "        # `model_class` can be any sklearn compatible classifier.\n",
    "        # There is built-in support for\n",
    "        #   - rf:  sklearn RandomForest\n",
    "        #   - xgb: xgboost\n",
    "        #   - lgb: lightgbm\n",
    "        model_type = \"xgb\" # or \"rf\", \"lgb\"\n",
    "        model_class = d.get_model_class(model_type)\n",
    "    \n",
    "        models = []\n",
    "        for i, params in enumerate(d.paramgrid(**param_dict)):\n",
    "            clf, train_time = dtrain.train(xgb.XGBRegressor, params)\n",
    "        \n",
    "            mtrain = dtrain.metric(clf)\n",
    "            mtest  = dtest.metric(clf)\n",
    "            mvalid = dvalid.metric(clf)\n",
    "    \n",
    "            models.append((d, clf, mtrain, mtest, mvalid, params))\n",
    "    \n",
    "        d, clf, mtrain, mtest, mvalid, params = min(models, key=lambda m: m[3])\n",
    "    \n",
    "        best_models[d.name()] = (d, clf, mtrain, mtest, mvalid, params)\n",
    "        print(f\"BEST {d.name():15s} train mse {mtrain:.3f}, valid mse {mtrain:.3f}, test mse {mtest:.3f}\")\n",
    "        print(\"params\", params)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e7d1ec-51d0-49fc-a656-3f4b63229b65",
   "metadata": {},
   "source": [
    "# Compress the models using `tree_compress`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22b9b065-7b57-45cc-8423-867f89c668df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| XGBOOST's base_score\n",
      "|   base_score diff std      1.9500172359770658e-05 (!) NOT OK\n",
      "|   base_score reported      142.01552\n",
      "|   versus manually detected 142.01551913749415\n",
      "|   abs err                  8.625058569577959e-07\n",
      "|   rel err                  6.073321119817016e-09\n",
      "|   base_score OK with relative tolerance 0.001\n",
      "\n",
      "MODEL PERF: mtr -0.544 mte -0.586 mva -0.592\n",
      "\n",
      "\n",
      "ROUND 1\n",
      "Level 0 xxtrain.shape (11478, 100) dense transform time: 0.01s, 4.49s relerr 0.1\n",
      " -0.544  -0.592 -> \u001b[32m -0.381\u001b[0m \u001b[32m -0.445\u001b[0m,   0% removed (alpha=   0.0026, nnz=100/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.381\u001b[0m \u001b[32m -0.445\u001b[0m,   0% removed (alpha=   0.0067, nnz=100/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.381\u001b[0m \u001b[32m -0.445\u001b[0m,   1% removed (alpha=   0.0172, nnz= 99/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.381\u001b[0m \u001b[32m -0.445\u001b[0m,   1% removed (alpha=   0.0444, nnz= 99/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.381\u001b[0m \u001b[32m -0.445\u001b[0m,   2% removed (alpha=   0.1145, nnz= 98/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.382\u001b[0m \u001b[32m -0.445\u001b[0m,   5% removed (alpha=   0.2955, nnz= 95/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.384\u001b[0m \u001b[32m -0.446\u001b[0m,  11% removed (alpha=   0.7627, nnz= 89/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.392\u001b[0m \u001b[32m -0.454\u001b[0m,  20% removed (alpha=   1.9684, nnz= 80/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.420\u001b[0m \u001b[32m -0.479\u001b[0m,  38% removed (alpha=   5.0802, nnz= 62/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.466\u001b[0m \u001b[32m -0.525\u001b[0m,  55% removed (alpha=  13.1113, nnz= 45/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.524\u001b[0m \u001b[32m -0.583\u001b[0m,  67% removed (alpha=  33.8386, nnz= 33/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.594\u001b[0m \u001b[32m -0.646\u001b[0m,  72% removed (alpha=  87.3326, nnz= 28/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -0.722\u001b[0m \u001b[33m -0.784\u001b[0m,  79% removed (alpha= 225.3934, nnz= 21/100) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -0.964\u001b[0m \u001b[33m -1.045\u001b[0m,  83% removed (alpha= 581.7091, nnz= 17/100) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -1.323\u001b[0m \u001b[33m -1.429\u001b[0m,  87% removed (alpha=1501.3107, nnz= 13/100) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -1.959\u001b[0m \u001b[33m -2.176\u001b[0m,  91% removed (alpha=3874.6751, nnz=  9/100) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.545\u001b[0m \u001b[32m -0.596\u001b[0m,  67% removed (alpha=  41.7749, nnz= 33/100) \u001b[32mfit ok\u001b[0m [ 33.8386 225.3934]\n",
      " -0.544  -0.592 -> \u001b[32m -0.556\u001b[0m \u001b[32m -0.607\u001b[0m,  70% removed (alpha=  51.5725, nnz= 30/100) \u001b[32mfit ok\u001b[0m [ 33.8386 225.3934]\n",
      " -0.544  -0.592 -> \u001b[32m -0.569\u001b[0m \u001b[32m -0.620\u001b[0m,  70% removed (alpha=  63.6681, nnz= 30/100) \u001b[32mfit ok\u001b[0m [ 33.8386 225.3934]\n",
      " -0.544  -0.592 -> \u001b[32m -0.588\u001b[0m \u001b[32m -0.640\u001b[0m,  72% removed (alpha=  78.6004, nnz= 28/100) \u001b[32mfit ok\u001b[0m [ 33.8386 225.3934]\n",
      " -0.544  -0.592 -> \u001b[31m -0.609\u001b[0m \u001b[31m -0.661\u001b[0m,  72% removed (alpha=  97.0349, nnz= 28/100) \u001b[33munder \u001b[0m [ 33.8386 225.3934]\n",
      " -0.544  -0.592 -> \u001b[31m -0.635\u001b[0m \u001b[31m -0.688\u001b[0m,  75% removed (alpha= 119.7930, nnz= 25/100) \u001b[33munder \u001b[0m [ 33.8386 225.3934]\n",
      " -0.544  -0.592 -> \u001b[33m -0.664\u001b[0m \u001b[33m -0.719\u001b[0m,  75% removed (alpha= 147.8886, nnz= 25/100) \u001b[33munder \u001b[0m [ 33.8386 225.3934]\n",
      " -0.544  -0.592 -> \u001b[33m -0.693\u001b[0m \u001b[33m -0.750\u001b[0m,  78% removed (alpha= 182.5736, nnz= 22/100) \u001b[33munder \u001b[0m [ 33.8386 225.3934]\n",
      " -0.544  -0.592 -> \u001b[32m -0.572\u001b[0m \u001b[32m -0.623\u001b[0m,  70% removed (alpha=  69.2664, nnz= 30/100) \u001b[32mfit ok\u001b[0m [63.6681 97.0349]\n",
      " -0.544  -0.592 -> \u001b[32m -0.579\u001b[0m \u001b[32m -0.630\u001b[0m,  70% removed (alpha=  75.3571, nnz= 30/100) \u001b[32mfit ok\u001b[0m [63.6681 97.0349]\n",
      " -0.544  -0.592 -> \u001b[32m -0.587\u001b[0m \u001b[32m -0.638\u001b[0m,  72% removed (alpha=  81.9833, nnz= 28/100) \u001b[32mfit ok\u001b[0m [63.6681 97.0349]\n",
      " -0.544  -0.592 -> \u001b[32m -0.595\u001b[0m \u001b[32m -0.646\u001b[0m,  72% removed (alpha=  89.1922, nnz= 28/100) \u001b[32mfit ok\u001b[0m [63.6681 97.0349]\n",
      "METRICS orig   tr -0.544\u001b[0m, va -0.592\u001b[0m \u001b[1m[te -0.586]\u001b[0m,  ntrees 100,  nnodes  8958,  nleafs  4529,  nnz  4527\n",
      "METRICS prev   \u001b[32mtr -0.544\u001b[0m, \u001b[32mva -0.592\u001b[0m \u001b[1m\u001b[32m[te -0.586]\u001b[0m,  ntrees 100,  nnodes  8958,  nleafs  4529,  nnz  4527\n",
      "METRICS now    \u001b[32mtr -0.594\u001b[0m, \u001b[32mva -0.646\u001b[0m \u001b[1m\u001b[31m[te -0.671]\u001b[0m,  ntrees  28,  nnodes  2426,  nleafs  1227,  nnz  1227\n",
      "\n",
      "Level 1 xxtrain.shape (11478, 112) dense transform time: 0.01s, 1.41s relerr 0.1\n",
      " -0.544  -0.592 -> \u001b[32m -0.531\u001b[0m \u001b[32m -0.581\u001b[0m,   0% removed (alpha=   0.0026, nnz=112/112) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.531\u001b[0m \u001b[32m -0.581\u001b[0m,   0% removed (alpha=   0.0067, nnz=112/112) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.531\u001b[0m \u001b[32m -0.580\u001b[0m,   0% removed (alpha=   0.0172, nnz=112/112) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.529\u001b[0m \u001b[32m -0.579\u001b[0m,   0% removed (alpha=   0.0444, nnz=112/112) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.527\u001b[0m \u001b[32m -0.577\u001b[0m,   0% removed (alpha=   0.1145, nnz=112/112) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.524\u001b[0m \u001b[32m -0.574\u001b[0m,   0% removed (alpha=   0.2955, nnz=112/112) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.519\u001b[0m \u001b[32m -0.570\u001b[0m,   0% removed (alpha=   0.7627, nnz=112/112) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.514\u001b[0m \u001b[32m -0.565\u001b[0m,   0% removed (alpha=   1.9684, nnz=112/112) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.513\u001b[0m \u001b[32m -0.567\u001b[0m,   0% removed (alpha=   5.0802, nnz=112/112) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.527\u001b[0m \u001b[32m -0.584\u001b[0m,   3% removed (alpha=  13.1113, nnz=109/112) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[31m -0.634\u001b[0m \u001b[31m -0.670\u001b[0m,  19% removed (alpha=  33.8386, nnz= 91/112) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[31m -0.614\u001b[0m \u001b[31m -0.669\u001b[0m,  58% removed (alpha=  87.3326, nnz= 47/112) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -0.790\u001b[0m \u001b[33m -0.835\u001b[0m,  63% removed (alpha= 225.3934, nnz= 41/112) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -1.111\u001b[0m \u001b[33m -1.170\u001b[0m,  75% removed (alpha= 581.7091, nnz= 28/112) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -1.654\u001b[0m \u001b[33m -1.728\u001b[0m,  85% removed (alpha=1501.3107, nnz= 17/112) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -2.539\u001b[0m \u001b[33m -2.765\u001b[0m,  91% removed (alpha=3874.6751, nnz= 10/112) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.532\u001b[0m \u001b[32m -0.573\u001b[0m,   6% removed (alpha=   6.2717, nnz=105/112) \u001b[32mfit ok\u001b[0m [ 5.0802 33.8386]\n",
      " -0.544  -0.592 -> \u001b[32m -0.524\u001b[0m \u001b[32m -0.569\u001b[0m,   7% removed (alpha=   7.7426, nnz=104/112) \u001b[32mfit ok\u001b[0m [ 5.0802 33.8386]\n",
      " -0.544  -0.592 -> \u001b[32m -0.506\u001b[0m \u001b[32m -0.557\u001b[0m,  13% removed (alpha=   9.5586, nnz= 97/112) \u001b[32mfit ok\u001b[0m [ 5.0802 33.8386]\n",
      " -0.544  -0.592 -> \u001b[32m -0.512\u001b[0m \u001b[32m -0.570\u001b[0m,  48% removed (alpha=  11.8004, nnz= 58/112) \u001b[32mfit ok\u001b[0m [ 5.0802 33.8386]\n",
      " -0.544  -0.592 -> \u001b[32m -0.519\u001b[0m \u001b[32m -0.577\u001b[0m,  49% removed (alpha=  14.5680, nnz= 57/112) \u001b[32mfit ok\u001b[0m [ 5.0802 33.8386]\n",
      " -0.544  -0.592 -> \u001b[32m -0.528\u001b[0m \u001b[32m -0.587\u001b[0m,  53% removed (alpha=  17.9846, nnz= 53/112) \u001b[32mfit ok\u001b[0m [ 5.0802 33.8386]\n",
      " -0.544  -0.592 -> \u001b[32m -0.539\u001b[0m \u001b[32m -0.598\u001b[0m,  54% removed (alpha=  22.2027, nnz= 52/112) \u001b[32mfit ok\u001b[0m [ 5.0802 33.8386]\n",
      " -0.544  -0.592 -> \u001b[32m -0.544\u001b[0m \u001b[32m -0.604\u001b[0m,  54% removed (alpha=  27.4100, nnz= 51/112) \u001b[32mfit ok\u001b[0m [ 5.0802 33.8386]\n",
      " -0.544  -0.592 -> \u001b[32m -0.541\u001b[0m \u001b[32m -0.600\u001b[0m,  54% removed (alpha=  24.1550, nnz= 51/112) \u001b[32mfit ok\u001b[0m [22.2027 33.8386]\n",
      " -0.544  -0.592 -> \u001b[32m -0.543\u001b[0m \u001b[32m -0.602\u001b[0m,  54% removed (alpha=  26.2789, nnz= 51/112) \u001b[32mfit ok\u001b[0m [22.2027 33.8386]\n",
      " -0.544  -0.592 -> \u001b[32m -0.545\u001b[0m \u001b[32m -0.604\u001b[0m,  54% removed (alpha=  28.5897, nnz= 51/112) \u001b[32mfit ok\u001b[0m [22.2027 33.8386]\n",
      " -0.544  -0.592 -> \u001b[32m -0.547\u001b[0m \u001b[32m -0.606\u001b[0m,  54% removed (alpha=  31.1036, nnz= 51/112) \u001b[32mfit ok\u001b[0m [22.2027 33.8386]\n",
      "METRICS orig   tr -0.544\u001b[0m, va -0.592\u001b[0m \u001b[1m[te -0.586]\u001b[0m,  ntrees 100,  nnodes  8958,  nleafs  4529,  nnz  4527\n",
      "METRICS prev   \u001b[32mtr -0.594\u001b[0m, \u001b[32mva -0.646\u001b[0m \u001b[1m\u001b[31m[te -0.671]\u001b[0m,  ntrees  28,  nnodes  2426,  nleafs  1227,  nnz  1227\n",
      "METRICS now    \u001b[32mtr -0.544\u001b[0m, \u001b[32mva -0.604\u001b[0m \u001b[1m\u001b[32m[te -0.615]\u001b[0m,  ntrees  24,  nnodes  2016,  nleafs  1020,  nnz  1020\n",
      "\n",
      "Level 2 xxtrain.shape (11478, 192) dense transform time: 0.01s, 1.22s relerr 0.1\n",
      " -0.544  -0.592 -> \u001b[32m -0.543\u001b[0m \u001b[32m -0.614\u001b[0m,   0% removed (alpha=   0.0026, nnz=192/192) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.543\u001b[0m \u001b[32m -0.613\u001b[0m,   0% removed (alpha=   0.0067, nnz=192/192) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.542\u001b[0m \u001b[32m -0.613\u001b[0m,   0% removed (alpha=   0.0172, nnz=192/192) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.542\u001b[0m \u001b[32m -0.613\u001b[0m,   0% removed (alpha=   0.0444, nnz=192/192) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.542\u001b[0m \u001b[32m -0.612\u001b[0m,   0% removed (alpha=   0.1145, nnz=192/192) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.541\u001b[0m \u001b[32m -0.612\u001b[0m,   0% removed (alpha=   0.2955, nnz=192/192) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.541\u001b[0m \u001b[32m -0.611\u001b[0m,   1% removed (alpha=   0.7627, nnz=191/192) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.529\u001b[0m \u001b[32m -0.598\u001b[0m,   5% removed (alpha=   1.9684, nnz=183/192) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.528\u001b[0m \u001b[32m -0.596\u001b[0m,   6% removed (alpha=   5.0802, nnz=181/192) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.546\u001b[0m \u001b[32m -0.613\u001b[0m,  30% removed (alpha=  13.1113, nnz=135/192) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[31m -0.604\u001b[0m \u001b[31m -0.661\u001b[0m,  50% removed (alpha=  33.8386, nnz= 96/192) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[31m -0.652\u001b[0m \u001b[33m -0.711\u001b[0m,  62% removed (alpha=  87.3326, nnz= 73/192) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -0.891\u001b[0m \u001b[33m -0.943\u001b[0m,  72% removed (alpha= 225.3934, nnz= 54/192) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -1.386\u001b[0m \u001b[33m -1.449\u001b[0m,  80% removed (alpha= 581.7091, nnz= 39/192) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -2.222\u001b[0m \u001b[33m -2.345\u001b[0m,  90% removed (alpha=1501.3107, nnz= 19/192) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -2.835\u001b[0m \u001b[33m -3.111\u001b[0m,  95% removed (alpha=3874.6751, nnz=  9/192) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.534\u001b[0m \u001b[32m -0.600\u001b[0m,  28% removed (alpha=   6.2717, nnz=138/192) \u001b[32mfit ok\u001b[0m [ 5.0802 33.8386]\n",
      " -0.544  -0.592 -> \u001b[32m -0.535\u001b[0m \u001b[32m -0.601\u001b[0m,  29% removed (alpha=   7.7426, nnz=136/192) \u001b[32mfit ok\u001b[0m [ 5.0802 33.8386]\n",
      " -0.544  -0.592 -> \u001b[32m -0.529\u001b[0m \u001b[32m -0.586\u001b[0m,  39% removed (alpha=   9.5586, nnz=117/192) \u001b[32mfit ok\u001b[0m [ 5.0802 33.8386]\n",
      " -0.544  -0.592 -> \u001b[32m -0.532\u001b[0m \u001b[32m -0.589\u001b[0m,  39% removed (alpha=  11.8004, nnz=117/192) \u001b[32mfit ok\u001b[0m [ 5.0802 33.8386]\n",
      " -0.544  -0.592 -> \u001b[32m -0.556\u001b[0m \u001b[32m -0.621\u001b[0m,  44% removed (alpha=  14.5680, nnz=108/192) \u001b[32mfit ok\u001b[0m [ 5.0802 33.8386]\n",
      " -0.544  -0.592 -> \u001b[32m -0.528\u001b[0m \u001b[32m -0.596\u001b[0m,  51% removed (alpha=  17.9846, nnz= 94/192) \u001b[32mfit ok\u001b[0m [ 5.0802 33.8386]\n",
      " -0.544  -0.592 -> \u001b[32m -0.536\u001b[0m \u001b[32m -0.603\u001b[0m,  53% removed (alpha=  22.2027, nnz= 91/192) \u001b[32mfit ok\u001b[0m [ 5.0802 33.8386]\n",
      " -0.544  -0.592 -> \u001b[32m -0.544\u001b[0m \u001b[32m -0.609\u001b[0m,  56% removed (alpha=  27.4100, nnz= 84/192) \u001b[32mfit ok\u001b[0m [ 5.0802 33.8386]\n",
      " -0.544  -0.592 -> \u001b[32m -0.539\u001b[0m \u001b[32m -0.605\u001b[0m,  56% removed (alpha=  24.1550, nnz= 85/192) \u001b[32mfit ok\u001b[0m [22.2027 33.8386]\n",
      " -0.544  -0.592 -> \u001b[32m -0.541\u001b[0m \u001b[32m -0.608\u001b[0m,  56% removed (alpha=  26.2789, nnz= 85/192) \u001b[32mfit ok\u001b[0m [22.2027 33.8386]\n",
      " -0.544  -0.592 -> \u001b[32m -0.544\u001b[0m \u001b[32m -0.611\u001b[0m,  56% removed (alpha=  28.5897, nnz= 84/192) \u001b[32mfit ok\u001b[0m [22.2027 33.8386]\n",
      " -0.544  -0.592 -> \u001b[32m -0.548\u001b[0m \u001b[32m -0.614\u001b[0m,  56% removed (alpha=  31.1036, nnz= 84/192) \u001b[32mfit ok\u001b[0m [22.2027 33.8386]\n",
      "METRICS orig   tr -0.544\u001b[0m, va -0.592\u001b[0m \u001b[1m[te -0.586]\u001b[0m,  ntrees 100,  nnodes  8958,  nleafs  4529,  nnz  4527\n",
      "METRICS prev   \u001b[32mtr -0.544\u001b[0m, \u001b[32mva -0.604\u001b[0m \u001b[1m\u001b[32m[te -0.615]\u001b[0m,  ntrees  24,  nnodes  2016,  nleafs  1020,  nnz  1020\n",
      "METRICS now    \u001b[32mtr -0.544\u001b[0m, \u001b[32mva -0.609\u001b[0m \u001b[1m\u001b[32m[te -0.613]\u001b[0m,  ntrees  24,  nnodes  1768,  nleafs   896,  nnz   886\n",
      "\n",
      "Level 3 xxtrain.shape (11478, 330) dense transform time: 0.01s, 1.21s relerr 0.1\n",
      " -0.544  -0.592 -> \u001b[32m -0.494\u001b[0m \u001b[32m -0.562\u001b[0m,   0% removed (alpha=   0.0026, nnz=330/330) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.493\u001b[0m \u001b[32m -0.561\u001b[0m,   0% removed (alpha=   0.0067, nnz=330/330) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.492\u001b[0m \u001b[32m -0.560\u001b[0m,   0% removed (alpha=   0.0172, nnz=330/330) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.491\u001b[0m \u001b[32m -0.559\u001b[0m,   0% removed (alpha=   0.0444, nnz=330/330) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.489\u001b[0m \u001b[32m -0.558\u001b[0m,   0% removed (alpha=   0.1145, nnz=330/330) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.487\u001b[0m \u001b[32m -0.556\u001b[0m,   0% removed (alpha=   0.2955, nnz=330/330) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.483\u001b[0m \u001b[32m -0.553\u001b[0m,   1% removed (alpha=   0.7627, nnz=327/330) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.475\u001b[0m \u001b[32m -0.540\u001b[0m,  12% removed (alpha=   1.9684, nnz=292/330) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.507\u001b[0m \u001b[32m -0.567\u001b[0m,  29% removed (alpha=   5.0802, nnz=235/330) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.577\u001b[0m \u001b[32m -0.645\u001b[0m,  51% removed (alpha=  13.1113, nnz=163/330) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13201.831433195679, tolerance: 8994.688410835686\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13112.071674870258, tolerance: 8994.688410835686\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.544  -0.592 -> \u001b[33m -0.783\u001b[0m \u001b[33m -0.847\u001b[0m,  62% removed (alpha=  33.8386, nnz=127/330) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -1.293\u001b[0m \u001b[33m -1.398\u001b[0m,  68% removed (alpha=  87.3326, nnz=107/330) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -1.685\u001b[0m \u001b[33m -1.778\u001b[0m,  75% removed (alpha= 225.3934, nnz= 81/330) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -2.189\u001b[0m \u001b[33m -2.272\u001b[0m,  86% removed (alpha= 581.7091, nnz= 47/330) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -2.743\u001b[0m \u001b[33m -2.854\u001b[0m,  92% removed (alpha=1501.3107, nnz= 25/330) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -3.321\u001b[0m \u001b[33m -3.517\u001b[0m,  96% removed (alpha=3874.6751, nnz= 14/330) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.595\u001b[0m \u001b[32m -0.648\u001b[0m,  37% removed (alpha=   6.2717, nnz=208/330) \u001b[32mfit ok\u001b[0m [ 5.0802 33.8386]\n",
      " -0.544  -0.592 -> \u001b[32m -0.580\u001b[0m \u001b[32m -0.631\u001b[0m,  43% removed (alpha=   7.7426, nnz=187/330) \u001b[32mfit ok\u001b[0m [ 5.0802 33.8386]\n",
      " -0.544  -0.592 -> \u001b[32m -0.584\u001b[0m \u001b[32m -0.635\u001b[0m,  42% removed (alpha=   9.5586, nnz=191/330) \u001b[32mfit ok\u001b[0m [ 5.0802 33.8386]\n",
      " -0.544  -0.592 -> \u001b[32m -0.598\u001b[0m \u001b[32m -0.646\u001b[0m,  46% removed (alpha=  11.8004, nnz=179/330) \u001b[32mfit ok\u001b[0m [ 5.0802 33.8386]\n",
      " -0.544  -0.592 -> \u001b[31m -0.627\u001b[0m \u001b[31m -0.675\u001b[0m,  47% removed (alpha=  14.5680, nnz=174/330) \u001b[33munder \u001b[0m [ 5.0802 33.8386]\n",
      " -0.544  -0.592 -> \u001b[31m -0.636\u001b[0m \u001b[31m -0.706\u001b[0m,  53% removed (alpha=  17.9846, nnz=156/330) \u001b[33munder \u001b[0m [ 5.0802 33.8386]\n",
      " -0.544  -0.592 -> \u001b[33m -0.661\u001b[0m \u001b[33m -0.739\u001b[0m,  53% removed (alpha=  22.2027, nnz=155/330) \u001b[33munder \u001b[0m [ 5.0802 33.8386]\n",
      " -0.544  -0.592 -> \u001b[33m -0.720\u001b[0m \u001b[33m -0.808\u001b[0m,  58% removed (alpha=  27.4100, nnz=140/330) \u001b[33munder \u001b[0m [ 5.0802 33.8386]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10257.984710048784, tolerance: 8994.688410835686\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.544  -0.592 -> \u001b[32m -0.591\u001b[0m \u001b[31m -0.671\u001b[0m,  49% removed (alpha=  10.3990, nnz=167/330) \u001b[31mover  \u001b[0m [ 9.5586 14.568 ]\n",
      " -0.544  -0.592 -> \u001b[32m -0.592\u001b[0m \u001b[31m -0.672\u001b[0m,  49% removed (alpha=  11.3134, nnz=169/330) \u001b[31mover  \u001b[0m [ 9.5586 14.568 ]\n",
      " -0.544  -0.592 -> \u001b[32m -0.593\u001b[0m \u001b[31m -0.674\u001b[0m,  48% removed (alpha=  12.3082, nnz=171/330) \u001b[31mover  \u001b[0m [ 9.5586 14.568 ]\n",
      " -0.544  -0.592 -> \u001b[32m -0.596\u001b[0m \u001b[31m -0.677\u001b[0m,  49% removed (alpha=  13.3905, nnz=169/330) \u001b[31mover  \u001b[0m [ 9.5586 14.568 ]\n",
      "METRICS orig   tr -0.544\u001b[0m, va -0.592\u001b[0m \u001b[1m[te -0.586]\u001b[0m,  ntrees 100,  nnodes  8958,  nleafs  4529,  nnz  4527\n",
      "METRICS prev   \u001b[32mtr -0.544\u001b[0m, \u001b[32mva -0.609\u001b[0m \u001b[1m\u001b[32m[te -0.613]\u001b[0m,  ntrees  24,  nnodes  1768,  nleafs   896,  nnz   886\n",
      "METRICS now    \u001b[32mtr -0.577\u001b[0m, \u001b[32mva -0.645\u001b[0m \u001b[1m\u001b[32m[te -0.640]\u001b[0m,  ntrees  24,  nnodes  1466,  nleafs   745,  nnz   717\n",
      "\n",
      "Level 4 xxtrain.shape (11478, 490) dense transform time: 0.01s, 1.30s relerr 0.1\n",
      " -0.544  -0.592 -> \u001b[32m -0.494\u001b[0m \u001b[32m -0.533\u001b[0m,   0% removed (alpha=   0.0026, nnz=490/490) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.494\u001b[0m \u001b[32m -0.533\u001b[0m,   0% removed (alpha=   0.0067, nnz=490/490) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.493\u001b[0m \u001b[32m -0.532\u001b[0m,   0% removed (alpha=   0.0172, nnz=490/490) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.450\u001b[0m \u001b[32m -0.501\u001b[0m,   2% removed (alpha=   0.0444, nnz=480/490) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.440\u001b[0m \u001b[32m -0.499\u001b[0m,   6% removed (alpha=   0.1145, nnz=463/490) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.441\u001b[0m \u001b[32m -0.502\u001b[0m,  10% removed (alpha=   0.2955, nnz=440/490) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.445\u001b[0m \u001b[32m -0.507\u001b[0m,  13% removed (alpha=   0.7627, nnz=425/490) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.468\u001b[0m \u001b[32m -0.526\u001b[0m,  22% removed (alpha=   1.9684, nnz=383/490) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.531\u001b[0m \u001b[32m -0.582\u001b[0m,  45% removed (alpha=   5.0802, nnz=269/490) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11541.645863957694, tolerance: 8994.688410835686\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.544  -0.592 -> \u001b[31m -0.633\u001b[0m \u001b[31m -0.685\u001b[0m,  55% removed (alpha=  13.1113, nnz=221/490) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 17020.400219765837, tolerance: 8994.688410835686\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.544  -0.592 -> \u001b[33m -0.828\u001b[0m \u001b[33m -0.855\u001b[0m,  64% removed (alpha=  33.8386, nnz=174/490) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -1.108\u001b[0m \u001b[33m -1.121\u001b[0m,  72% removed (alpha=  87.3326, nnz=139/490) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -1.664\u001b[0m \u001b[33m -1.691\u001b[0m,  79% removed (alpha= 225.3934, nnz=105/490) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -2.495\u001b[0m \u001b[33m -2.504\u001b[0m,  86% removed (alpha= 581.7091, nnz= 70/490) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -3.952\u001b[0m \u001b[33m -4.012\u001b[0m,  90% removed (alpha=1501.3107, nnz= 48/490) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -6.954\u001b[0m \u001b[33m -7.403\u001b[0m,  93% removed (alpha=3874.6751, nnz= 35/490) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[31m -0.622\u001b[0m \u001b[31m -0.663\u001b[0m,  31% removed (alpha=   2.4301, nnz=339/490) \u001b[33munder \u001b[0m [ 1.9684 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.546\u001b[0m \u001b[32m -0.580\u001b[0m,  36% removed (alpha=   3.0000, nnz=313/490) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.550\u001b[0m \u001b[32m -0.584\u001b[0m,  37% removed (alpha=   3.7036, nnz=309/490) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.557\u001b[0m \u001b[32m -0.592\u001b[0m,  39% removed (alpha=   4.5723, nnz=297/490) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.564\u001b[0m \u001b[32m -0.597\u001b[0m,  43% removed (alpha=   5.6446, nnz=281/490) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.578\u001b[0m \u001b[32m -0.610\u001b[0m,  44% removed (alpha=   6.9685, nnz=273/490) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.585\u001b[0m \u001b[32m -0.619\u001b[0m,  49% removed (alpha=   8.6028, nnz=248/490) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.544  -0.592 -> \u001b[31m -0.599\u001b[0m \u001b[32m -0.634\u001b[0m,  51% removed (alpha=  10.6205, nnz=239/490) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.593\u001b[0m \u001b[32m -0.627\u001b[0m,  50% removed (alpha=   9.3593, nnz=244/490) \u001b[32mfit ok\u001b[0m [ 8.6028 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.595\u001b[0m \u001b[32m -0.630\u001b[0m,  51% removed (alpha=  10.1822, nnz=241/490) \u001b[32mfit ok\u001b[0m [ 8.6028 13.1113]\n",
      " -0.544  -0.592 -> \u001b[31m -0.601\u001b[0m \u001b[32m -0.636\u001b[0m,  52% removed (alpha=  11.0776, nnz=237/490) \u001b[32mfit ok\u001b[0m [ 8.6028 13.1113]\n",
      " -0.544  -0.592 -> \u001b[31m -0.608\u001b[0m \u001b[32m -0.643\u001b[0m,  52% removed (alpha=  12.0516, nnz=234/490) \u001b[32mfit ok\u001b[0m [ 8.6028 13.1113]\n",
      "METRICS orig   tr -0.544\u001b[0m, va -0.592\u001b[0m \u001b[1m[te -0.586]\u001b[0m,  ntrees 100,  nnodes  8958,  nleafs  4529,  nnz  4527\n",
      "METRICS prev   \u001b[32mtr -0.577\u001b[0m, \u001b[32mva -0.645\u001b[0m \u001b[1m\u001b[32m[te -0.640]\u001b[0m,  ntrees  24,  nnodes  1466,  nleafs   745,  nnz   717\n",
      "METRICS now    \u001b[31mtr -0.608\u001b[0m, \u001b[32mva -0.643\u001b[0m \u001b[1m\u001b[31m[te -0.652]\u001b[0m,  ntrees  24,  nnodes  1164,  nleafs   594,  nnz   519\n",
      "\n",
      "Level 5 xxtrain.shape (11478, 594) dense transform time: 0.00s, 1.30s relerr 0.1\n",
      " -0.544  -0.592 -> \u001b[32m -0.452\u001b[0m \u001b[32m -0.511\u001b[0m,   0% removed (alpha=   0.0026, nnz=593/594) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.444\u001b[0m \u001b[32m -0.503\u001b[0m,   1% removed (alpha=   0.0067, nnz=590/594) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.444\u001b[0m \u001b[32m -0.503\u001b[0m,   1% removed (alpha=   0.0172, nnz=590/594) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.443\u001b[0m \u001b[32m -0.502\u001b[0m,   1% removed (alpha=   0.0444, nnz=586/594) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.441\u001b[0m \u001b[32m -0.501\u001b[0m,   4% removed (alpha=   0.1145, nnz=568/594) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.437\u001b[0m \u001b[32m -0.499\u001b[0m,  20% removed (alpha=   0.2955, nnz=475/594) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.446\u001b[0m \u001b[32m -0.505\u001b[0m,  25% removed (alpha=   0.7627, nnz=444/594) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.469\u001b[0m \u001b[32m -0.529\u001b[0m,  32% removed (alpha=   1.9684, nnz=402/594) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.525\u001b[0m \u001b[32m -0.584\u001b[0m,  50% removed (alpha=   5.0802, nnz=298/594) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[31m -0.605\u001b[0m \u001b[31m -0.665\u001b[0m,  59% removed (alpha=  13.1113, nnz=245/594) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -0.751\u001b[0m \u001b[33m -0.804\u001b[0m,  66% removed (alpha=  33.8386, nnz=202/594) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -1.031\u001b[0m \u001b[33m -1.057\u001b[0m,  71% removed (alpha=  87.3326, nnz=170/594) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -1.562\u001b[0m \u001b[33m -1.587\u001b[0m,  79% removed (alpha= 225.3934, nnz=127/594) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -2.738\u001b[0m \u001b[33m -2.661\u001b[0m,  85% removed (alpha= 581.7091, nnz= 87/594) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -4.287\u001b[0m \u001b[33m -4.188\u001b[0m,  88% removed (alpha=1501.3107, nnz= 74/594) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -7.185\u001b[0m \u001b[33m -7.188\u001b[0m,  92% removed (alpha=3874.6751, nnz= 50/594) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.489\u001b[0m \u001b[32m -0.547\u001b[0m,  40% removed (alpha=   2.4301, nnz=356/594) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.495\u001b[0m \u001b[32m -0.553\u001b[0m,  41% removed (alpha=   3.0000, nnz=353/594) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.508\u001b[0m \u001b[32m -0.567\u001b[0m,  50% removed (alpha=   3.7036, nnz=299/594) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.516\u001b[0m \u001b[32m -0.576\u001b[0m,  50% removed (alpha=   4.5723, nnz=297/594) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.525\u001b[0m \u001b[32m -0.585\u001b[0m,  52% removed (alpha=   5.6446, nnz=285/594) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.537\u001b[0m \u001b[32m -0.599\u001b[0m,  52% removed (alpha=   6.9685, nnz=283/594) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.556\u001b[0m \u001b[32m -0.617\u001b[0m,  54% removed (alpha=   8.6028, nnz=275/594) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.575\u001b[0m \u001b[32m -0.637\u001b[0m,  56% removed (alpha=  10.6205, nnz=262/594) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.566\u001b[0m \u001b[32m -0.629\u001b[0m,  55% removed (alpha=   9.3593, nnz=266/594) \u001b[32mfit ok\u001b[0m [ 8.6028 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.571\u001b[0m \u001b[32m -0.634\u001b[0m,  54% removed (alpha=  10.1822, nnz=271/594) \u001b[32mfit ok\u001b[0m [ 8.6028 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.578\u001b[0m \u001b[32m -0.641\u001b[0m,  55% removed (alpha=  11.0776, nnz=269/594) \u001b[32mfit ok\u001b[0m [ 8.6028 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.588\u001b[0m \u001b[32m -0.650\u001b[0m,  56% removed (alpha=  12.0516, nnz=262/594) \u001b[32mfit ok\u001b[0m [ 8.6028 13.1113]\n",
      "METRICS orig   tr -0.544\u001b[0m, va -0.592\u001b[0m \u001b[1m[te -0.586]\u001b[0m,  ntrees 100,  nnodes  8958,  nleafs  4529,  nnz  4527\n",
      "METRICS prev   \u001b[31mtr -0.608\u001b[0m, \u001b[32mva -0.643\u001b[0m \u001b[1m\u001b[31m[te -0.652]\u001b[0m,  ntrees  24,  nnodes  1164,  nleafs   594,  nnz   519\n",
      "METRICS now    \u001b[32mtr -0.575\u001b[0m, \u001b[32mva -0.637\u001b[0m \u001b[1m\u001b[32m[te -0.640]\u001b[0m,  ntrees  24,  nnodes   960,  nleafs   492,  nnz   348\n",
      "\n",
      "Level 6 xxtrain.shape (11478, 492) dense transform time: 0.00s, 1.18s relerr 0.1\n",
      " -0.544  -0.592 -> \u001b[32m -0.478\u001b[0m \u001b[32m -0.534\u001b[0m,   1% removed (alpha=   0.0026, nnz=489/492) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.478\u001b[0m \u001b[32m -0.534\u001b[0m,   1% removed (alpha=   0.0067, nnz=489/492) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.478\u001b[0m \u001b[32m -0.534\u001b[0m,   1% removed (alpha=   0.0172, nnz=489/492) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.477\u001b[0m \u001b[32m -0.534\u001b[0m,   4% removed (alpha=   0.0444, nnz=472/492) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.477\u001b[0m \u001b[32m -0.533\u001b[0m,   5% removed (alpha=   0.1145, nnz=469/492) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.477\u001b[0m \u001b[32m -0.534\u001b[0m,  15% removed (alpha=   0.2955, nnz=419/492) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.478\u001b[0m \u001b[32m -0.535\u001b[0m,  16% removed (alpha=   0.7627, nnz=413/492) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.486\u001b[0m \u001b[32m -0.546\u001b[0m,  21% removed (alpha=   1.9684, nnz=388/492) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.513\u001b[0m \u001b[32m -0.566\u001b[0m,  35% removed (alpha=   5.0802, nnz=318/492) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[31m -0.603\u001b[0m \u001b[31m -0.656\u001b[0m,  47% removed (alpha=  13.1113, nnz=263/492) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -0.847\u001b[0m \u001b[33m -0.893\u001b[0m,  53% removed (alpha=  33.8386, nnz=233/492) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -1.222\u001b[0m \u001b[33m -1.283\u001b[0m,  66% removed (alpha=  87.3326, nnz=166/492) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -1.912\u001b[0m \u001b[33m -2.090\u001b[0m,  77% removed (alpha= 225.3934, nnz=115/492) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -3.086\u001b[0m \u001b[33m -3.385\u001b[0m,  84% removed (alpha= 581.7091, nnz= 79/492) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -5.113\u001b[0m \u001b[33m -5.513\u001b[0m,  89% removed (alpha=1501.3107, nnz= 52/492) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -7.240\u001b[0m \u001b[33m -7.626\u001b[0m,  93% removed (alpha=3874.6751, nnz= 35/492) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.490\u001b[0m \u001b[32m -0.546\u001b[0m,  27% removed (alpha=   2.4301, nnz=359/492) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.494\u001b[0m \u001b[32m -0.549\u001b[0m,  29% removed (alpha=   3.0000, nnz=350/492) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.500\u001b[0m \u001b[32m -0.554\u001b[0m,  31% removed (alpha=   3.7036, nnz=341/492) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.507\u001b[0m \u001b[32m -0.560\u001b[0m,  32% removed (alpha=   4.5723, nnz=336/492) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.517\u001b[0m \u001b[32m -0.570\u001b[0m,  33% removed (alpha=   5.6446, nnz=328/492) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.532\u001b[0m \u001b[32m -0.583\u001b[0m,  35% removed (alpha=   6.9685, nnz=319/492) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.552\u001b[0m \u001b[32m -0.604\u001b[0m,  37% removed (alpha=   8.6028, nnz=310/492) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.573\u001b[0m \u001b[32m -0.625\u001b[0m,  39% removed (alpha=  10.6205, nnz=300/492) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.561\u001b[0m \u001b[32m -0.613\u001b[0m,  38% removed (alpha=   9.3593, nnz=306/492) \u001b[32mfit ok\u001b[0m [ 8.6028 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.568\u001b[0m \u001b[32m -0.620\u001b[0m,  38% removed (alpha=  10.1822, nnz=304/492) \u001b[32mfit ok\u001b[0m [ 8.6028 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.577\u001b[0m \u001b[32m -0.630\u001b[0m,  40% removed (alpha=  11.0776, nnz=294/492) \u001b[32mfit ok\u001b[0m [ 8.6028 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.588\u001b[0m \u001b[32m -0.641\u001b[0m,  40% removed (alpha=  12.0516, nnz=296/492) \u001b[32mfit ok\u001b[0m [ 8.6028 13.1113]\n",
      "METRICS orig   tr -0.544\u001b[0m, va -0.592\u001b[0m \u001b[1m[te -0.586]\u001b[0m,  ntrees 100,  nnodes  8958,  nleafs  4529,  nnz  4527\n",
      "METRICS prev   \u001b[32mtr -0.575\u001b[0m, \u001b[32mva -0.637\u001b[0m \u001b[1m\u001b[32m[te -0.640]\u001b[0m,  ntrees  24,  nnodes   960,  nleafs   492,  nnz   348\n",
      "METRICS now    \u001b[32mtr -0.577\u001b[0m, \u001b[32mva -0.630\u001b[0m \u001b[1m\u001b[32m[te -0.634]\u001b[0m,  ntrees  24,  nnodes   924,  nleafs   474,  nnz   293\n",
      "\n",
      "DONE, depth of tree reached 6, 6\n",
      "\n",
      "\n",
      "ROUND 2\n",
      "Level 0 xxtrain.shape (11478, 24) dense transform time: 0.00s, 1.20s relerr 0.1\n",
      " -0.544  -0.592 -> \u001b[32m -0.569\u001b[0m \u001b[32m -0.620\u001b[0m,   0% removed (alpha=   0.0026, nnz=24/24) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.569\u001b[0m \u001b[32m -0.620\u001b[0m,   0% removed (alpha=   0.0067, nnz=24/24) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.569\u001b[0m \u001b[32m -0.620\u001b[0m,   0% removed (alpha=   0.0172, nnz=24/24) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.569\u001b[0m \u001b[32m -0.620\u001b[0m,   0% removed (alpha=   0.0444, nnz=24/24) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.569\u001b[0m \u001b[32m -0.620\u001b[0m,   0% removed (alpha=   0.1145, nnz=24/24) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.569\u001b[0m \u001b[32m -0.620\u001b[0m,   0% removed (alpha=   0.2955, nnz=24/24) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.569\u001b[0m \u001b[32m -0.620\u001b[0m,   0% removed (alpha=   0.7627, nnz=24/24) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.569\u001b[0m \u001b[32m -0.620\u001b[0m,   0% removed (alpha=   1.9684, nnz=24/24) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.569\u001b[0m \u001b[32m -0.620\u001b[0m,   0% removed (alpha=   5.0802, nnz=24/24) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.569\u001b[0m \u001b[32m -0.620\u001b[0m,   0% removed (alpha=  13.1113, nnz=24/24) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.570\u001b[0m \u001b[32m -0.621\u001b[0m,   0% removed (alpha=  33.8386, nnz=24/24) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.576\u001b[0m \u001b[32m -0.628\u001b[0m,   0% removed (alpha=  87.3326, nnz=24/24) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[31m -0.619\u001b[0m \u001b[31m -0.670\u001b[0m,   0% removed (alpha= 225.3934, nnz=24/24) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -0.805\u001b[0m \u001b[33m -0.861\u001b[0m,   4% removed (alpha= 581.7091, nnz=23/24) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -1.177\u001b[0m \u001b[33m -1.234\u001b[0m,  33% removed (alpha=1501.3107, nnz=16/24) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -1.966\u001b[0m \u001b[33m -2.006\u001b[0m,  42% removed (alpha=3874.6751, nnz=14/24) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.571\u001b[0m \u001b[32m -0.622\u001b[0m,   0% removed (alpha=  41.7749, nnz=24/24) \u001b[32mfit ok\u001b[0m [ 33.8386 225.3934]\n",
      " -0.544  -0.592 -> \u001b[32m -0.572\u001b[0m \u001b[32m -0.623\u001b[0m,   0% removed (alpha=  51.5725, nnz=24/24) \u001b[32mfit ok\u001b[0m [ 33.8386 225.3934]\n",
      " -0.544  -0.592 -> \u001b[32m -0.573\u001b[0m \u001b[32m -0.625\u001b[0m,   0% removed (alpha=  63.6681, nnz=24/24) \u001b[32mfit ok\u001b[0m [ 33.8386 225.3934]\n",
      " -0.544  -0.592 -> \u001b[32m -0.575\u001b[0m \u001b[32m -0.627\u001b[0m,   0% removed (alpha=  78.6004, nnz=24/24) \u001b[32mfit ok\u001b[0m [ 33.8386 225.3934]\n",
      " -0.544  -0.592 -> \u001b[32m -0.578\u001b[0m \u001b[32m -0.630\u001b[0m,   0% removed (alpha=  97.0349, nnz=24/24) \u001b[32mfit ok\u001b[0m [ 33.8386 225.3934]\n",
      " -0.544  -0.592 -> \u001b[32m -0.583\u001b[0m \u001b[32m -0.635\u001b[0m,   0% removed (alpha= 119.7930, nnz=24/24) \u001b[32mfit ok\u001b[0m [ 33.8386 225.3934]\n",
      " -0.544  -0.592 -> \u001b[32m -0.591\u001b[0m \u001b[32m -0.642\u001b[0m,   0% removed (alpha= 147.8886, nnz=24/24) \u001b[32mfit ok\u001b[0m [ 33.8386 225.3934]\n",
      " -0.544  -0.592 -> \u001b[31m -0.602\u001b[0m \u001b[31m -0.653\u001b[0m,   0% removed (alpha= 182.5736, nnz=24/24) \u001b[33munder \u001b[0m [ 33.8386 225.3934]\n",
      " -0.544  -0.592 -> \u001b[32m -0.586\u001b[0m \u001b[32m -0.637\u001b[0m,   0% removed (alpha= 130.3265, nnz=24/24) \u001b[32mfit ok\u001b[0m [119.793  182.5736]\n",
      " -0.544  -0.592 -> \u001b[32m -0.589\u001b[0m \u001b[32m -0.641\u001b[0m,   0% removed (alpha= 141.7862, nnz=24/24) \u001b[32mfit ok\u001b[0m [119.793  182.5736]\n",
      " -0.544  -0.592 -> \u001b[32m -0.593\u001b[0m \u001b[32m -0.644\u001b[0m,   0% removed (alpha= 154.2536, nnz=24/24) \u001b[32mfit ok\u001b[0m [119.793  182.5736]\n",
      " -0.544  -0.592 -> \u001b[32m -0.597\u001b[0m \u001b[32m -0.649\u001b[0m,   0% removed (alpha= 167.8173, nnz=24/24) \u001b[32mfit ok\u001b[0m [119.793  182.5736]\n",
      "METRICS orig   tr -0.544\u001b[0m, va -0.592\u001b[0m \u001b[1m[te -0.586]\u001b[0m,  ntrees 100,  nnodes  8958,  nleafs  4529,  nnz  4527\n",
      "METRICS prev   \u001b[32mtr -0.577\u001b[0m, \u001b[32mva -0.630\u001b[0m \u001b[1m\u001b[32m[te -0.634]\u001b[0m,  ntrees  24,  nnodes   924,  nleafs   474,  nnz   293\n",
      "METRICS now    \u001b[32mtr -0.569\u001b[0m, \u001b[32mva -0.620\u001b[0m \u001b[1m\u001b[32m[te -0.623]\u001b[0m,  ntrees  24,  nnodes   924,  nleafs   474,  nnz   293\n",
      "\n",
      "Level 1 xxtrain.shape (11478, 92) dense transform time: 0.00s, 1.26s relerr 0.1\n",
      " -0.544  -0.592 -> \u001b[33m -0.736\u001b[0m \u001b[33m -0.785\u001b[0m,   4% removed (alpha=   0.0026, nnz=88/92) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -0.736\u001b[0m \u001b[33m -0.785\u001b[0m,   4% removed (alpha=   0.0067, nnz=88/92) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -0.736\u001b[0m \u001b[33m -0.785\u001b[0m,   4% removed (alpha=   0.0172, nnz=88/92) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -0.736\u001b[0m \u001b[33m -0.785\u001b[0m,   4% removed (alpha=   0.0444, nnz=88/92) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -0.736\u001b[0m \u001b[33m -0.785\u001b[0m,   4% removed (alpha=   0.1145, nnz=88/92) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -0.736\u001b[0m \u001b[33m -0.785\u001b[0m,   4% removed (alpha=   0.2955, nnz=88/92) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -0.736\u001b[0m \u001b[33m -0.784\u001b[0m,   4% removed (alpha=   0.7627, nnz=88/92) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -0.735\u001b[0m \u001b[33m -0.784\u001b[0m,   4% removed (alpha=   1.9684, nnz=88/92) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -0.734\u001b[0m \u001b[33m -0.782\u001b[0m,   5% removed (alpha=   5.0802, nnz=87/92) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -0.735\u001b[0m \u001b[33m -0.782\u001b[0m,   9% removed (alpha=  13.1113, nnz=84/92) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[31m -0.611\u001b[0m \u001b[31m -0.655\u001b[0m,  34% removed (alpha=  33.8386, nnz=61/92) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[31m -0.602\u001b[0m \u001b[32m -0.650\u001b[0m,  57% removed (alpha=  87.3326, nnz=40/92) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -0.677\u001b[0m \u001b[33m -0.716\u001b[0m,  60% removed (alpha= 225.3934, nnz=37/92) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -0.968\u001b[0m \u001b[33m -0.990\u001b[0m,  63% removed (alpha= 581.7091, nnz=34/92) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -1.597\u001b[0m \u001b[33m -1.621\u001b[0m,  76% removed (alpha=1501.3107, nnz=22/92) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -2.327\u001b[0m \u001b[33m -2.349\u001b[0m,  83% removed (alpha=3874.6751, nnz=16/92) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.580\u001b[0m \u001b[32m -0.632\u001b[0m,  54% removed (alpha=  41.7749, nnz=42/92) \u001b[32mfit ok\u001b[0m [ 33.8386 225.3934]\n",
      " -0.544  -0.592 -> \u001b[32m -0.584\u001b[0m \u001b[32m -0.636\u001b[0m,  54% removed (alpha=  51.5725, nnz=42/92) \u001b[32mfit ok\u001b[0m [ 33.8386 225.3934]\n",
      " -0.544  -0.592 -> \u001b[32m -0.590\u001b[0m \u001b[32m -0.641\u001b[0m,  57% removed (alpha=  63.6681, nnz=40/92) \u001b[32mfit ok\u001b[0m [ 33.8386 225.3934]\n",
      " -0.544  -0.592 -> \u001b[32m -0.595\u001b[0m \u001b[32m -0.645\u001b[0m,  57% removed (alpha=  78.6004, nnz=40/92) \u001b[32mfit ok\u001b[0m [ 33.8386 225.3934]\n",
      " -0.544  -0.592 -> \u001b[31m -0.602\u001b[0m \u001b[32m -0.651\u001b[0m,  57% removed (alpha=  97.0349, nnz=40/92) \u001b[32mfit ok\u001b[0m [ 33.8386 225.3934]\n",
      " -0.544  -0.592 -> \u001b[31m -0.613\u001b[0m \u001b[31m -0.660\u001b[0m,  58% removed (alpha= 119.7930, nnz=39/92) \u001b[33munder \u001b[0m [ 33.8386 225.3934]\n",
      " -0.544  -0.592 -> \u001b[31m -0.628\u001b[0m \u001b[31m -0.673\u001b[0m,  58% removed (alpha= 147.8886, nnz=39/92) \u001b[33munder \u001b[0m [ 33.8386 225.3934]\n",
      " -0.544  -0.592 -> \u001b[31m -0.649\u001b[0m \u001b[31m -0.692\u001b[0m,  59% removed (alpha= 182.5736, nnz=38/92) \u001b[33munder \u001b[0m [ 33.8386 225.3934]\n",
      " -0.544  -0.592 -> \u001b[32m -0.597\u001b[0m \u001b[32m -0.647\u001b[0m,  57% removed (alpha=  85.5118, nnz=40/92) \u001b[32mfit ok\u001b[0m [ 78.6004 119.793 ]\n",
      " -0.544  -0.592 -> \u001b[31m -0.600\u001b[0m \u001b[32m -0.649\u001b[0m,  57% removed (alpha=  93.0309, nnz=40/92) \u001b[32mfit ok\u001b[0m [ 78.6004 119.793 ]\n",
      " -0.544  -0.592 -> \u001b[31m -0.604\u001b[0m \u001b[31m -0.652\u001b[0m,  57% removed (alpha= 101.2112, nnz=40/92) \u001b[33munder \u001b[0m [ 78.6004 119.793 ]\n",
      " -0.544  -0.592 -> \u001b[31m -0.608\u001b[0m \u001b[31m -0.656\u001b[0m,  58% removed (alpha= 110.1108, nnz=39/92) \u001b[33munder \u001b[0m [ 78.6004 119.793 ]\n",
      "METRICS orig   tr -0.544\u001b[0m, va -0.592\u001b[0m \u001b[1m[te -0.586]\u001b[0m,  ntrees 100,  nnodes  8958,  nleafs  4529,  nnz  4527\n",
      "METRICS prev   \u001b[32mtr -0.569\u001b[0m, \u001b[32mva -0.620\u001b[0m \u001b[1m\u001b[32m[te -0.623]\u001b[0m,  ntrees  24,  nnodes   924,  nleafs   474,  nnz   293\n",
      "METRICS now    \u001b[31mtr -0.602\u001b[0m, \u001b[32mva -0.650\u001b[0m \u001b[1m\u001b[31m[te -0.652]\u001b[0m,  ntrees  24,  nnodes   888,  nleafs   456,  nnz   281\n",
      "\n",
      "Level 2 xxtrain.shape (11478, 158) dense transform time: 0.00s, 1.24s relerr 0.1\n",
      " -0.544  -0.592 -> \u001b[33m -0.730\u001b[0m \u001b[33m -0.763\u001b[0m,   1% removed (alpha=   0.0026, nnz=157/158) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -0.729\u001b[0m \u001b[33m -0.763\u001b[0m,   1% removed (alpha=   0.0067, nnz=157/158) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -0.728\u001b[0m \u001b[33m -0.763\u001b[0m,   1% removed (alpha=   0.0172, nnz=157/158) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -0.728\u001b[0m \u001b[33m -0.762\u001b[0m,   1% removed (alpha=   0.0444, nnz=157/158) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -0.726\u001b[0m \u001b[33m -0.761\u001b[0m,   1% removed (alpha=   0.1145, nnz=157/158) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -0.726\u001b[0m \u001b[33m -0.760\u001b[0m,   1% removed (alpha=   0.2955, nnz=156/158) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -0.725\u001b[0m \u001b[33m -0.760\u001b[0m,   2% removed (alpha=   0.7627, nnz=155/158) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -0.692\u001b[0m \u001b[33m -0.731\u001b[0m,  22% removed (alpha=   1.9684, nnz=123/158) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -0.693\u001b[0m \u001b[33m -0.733\u001b[0m,  21% removed (alpha=   5.0802, nnz=125/158) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -0.672\u001b[0m \u001b[33m -0.713\u001b[0m,  32% removed (alpha=  13.1113, nnz=107/158) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9553.892401724857, tolerance: 8994.688410835686\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.544  -0.592 -> \u001b[31m -0.613\u001b[0m \u001b[31m -0.658\u001b[0m,  47% removed (alpha=  33.8386, nnz= 83/158) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -0.696\u001b[0m \u001b[33m -0.727\u001b[0m,  53% removed (alpha=  87.3326, nnz= 75/158) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -0.957\u001b[0m \u001b[33m -0.969\u001b[0m,  61% removed (alpha= 225.3934, nnz= 62/158) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -1.580\u001b[0m \u001b[33m -1.550\u001b[0m,  71% removed (alpha= 581.7091, nnz= 46/158) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -2.557\u001b[0m \u001b[33m -2.514\u001b[0m,  82% removed (alpha=1501.3107, nnz= 28/158) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -4.242\u001b[0m \u001b[33m -4.162\u001b[0m,  88% removed (alpha=3874.6751, nnz= 19/158) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      "METRICS orig   tr -0.544\u001b[0m, va -0.592\u001b[0m \u001b[1m[te -0.586]\u001b[0m,  ntrees 100,  nnodes  8958,  nleafs  4529,  nnz  4527\n",
      "METRICS prev   \u001b[31mtr -0.602\u001b[0m, \u001b[32mva -0.650\u001b[0m \u001b[1m\u001b[31m[te -0.652]\u001b[0m,  ntrees  24,  nnodes   888,  nleafs   456,  nnz   281\n",
      "METRICS now    \u001b[31mtr -0.602\u001b[0m, \u001b[32mva -0.650\u001b[0m \u001b[1m\u001b[31m[te -0.652]\u001b[0m,  ntrees  24,  nnodes   888,  nleafs   456,  nnz   281\n",
      "\n",
      "Level 3 xxtrain.shape (11478, 262) dense transform time: 0.00s, 1.22s relerr 0.1\n",
      " -0.544  -0.592 -> \u001b[32m -0.545\u001b[0m \u001b[32m -0.594\u001b[0m,   0% removed (alpha=   0.0026, nnz=261/262) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.545\u001b[0m \u001b[32m -0.594\u001b[0m,   1% removed (alpha=   0.0067, nnz=260/262) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.546\u001b[0m \u001b[32m -0.595\u001b[0m,   1% removed (alpha=   0.0172, nnz=260/262) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.546\u001b[0m \u001b[32m -0.595\u001b[0m,   1% removed (alpha=   0.0444, nnz=260/262) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.546\u001b[0m \u001b[32m -0.595\u001b[0m,   2% removed (alpha=   0.1145, nnz=258/262) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.546\u001b[0m \u001b[32m -0.596\u001b[0m,   2% removed (alpha=   0.2955, nnz=257/262) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.548\u001b[0m \u001b[32m -0.598\u001b[0m,   6% removed (alpha=   0.7627, nnz=245/262) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.549\u001b[0m \u001b[32m -0.601\u001b[0m,  21% removed (alpha=   1.9684, nnz=208/262) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.556\u001b[0m \u001b[32m -0.610\u001b[0m,  30% removed (alpha=   5.0802, nnz=184/262) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.574\u001b[0m \u001b[32m -0.630\u001b[0m,  47% removed (alpha=  13.1113, nnz=138/262) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[31m -0.634\u001b[0m \u001b[31m -0.680\u001b[0m,  51% removed (alpha=  33.8386, nnz=129/262) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -0.790\u001b[0m \u001b[33m -0.820\u001b[0m,  61% removed (alpha=  87.3326, nnz=103/262) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -1.120\u001b[0m \u001b[33m -1.139\u001b[0m,  66% removed (alpha= 225.3934, nnz= 90/262) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -1.851\u001b[0m \u001b[33m -1.869\u001b[0m,  75% removed (alpha= 581.7091, nnz= 65/262) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -3.341\u001b[0m \u001b[33m -3.372\u001b[0m,  84% removed (alpha=1501.3107, nnz= 42/262) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -6.659\u001b[0m \u001b[33m -6.752\u001b[0m,  87% removed (alpha=3874.6751, nnz= 35/262) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.557\u001b[0m \u001b[32m -0.612\u001b[0m,  35% removed (alpha=   6.2717, nnz=171/262) \u001b[32mfit ok\u001b[0m [ 5.0802 33.8386]\n",
      " -0.544  -0.592 -> \u001b[32m -0.560\u001b[0m \u001b[32m -0.615\u001b[0m,  36% removed (alpha=   7.7426, nnz=168/262) \u001b[32mfit ok\u001b[0m [ 5.0802 33.8386]\n",
      " -0.544  -0.592 -> \u001b[32m -0.564\u001b[0m \u001b[32m -0.619\u001b[0m,  36% removed (alpha=   9.5586, nnz=168/262) \u001b[32mfit ok\u001b[0m [ 5.0802 33.8386]\n",
      " -0.544  -0.592 -> \u001b[32m -0.572\u001b[0m \u001b[32m -0.628\u001b[0m,  39% removed (alpha=  11.8004, nnz=160/262) \u001b[32mfit ok\u001b[0m [ 5.0802 33.8386]\n",
      " -0.544  -0.592 -> \u001b[32m -0.578\u001b[0m \u001b[32m -0.633\u001b[0m,  37% removed (alpha=  14.5680, nnz=164/262) \u001b[32mfit ok\u001b[0m [ 5.0802 33.8386]\n",
      " -0.544  -0.592 -> \u001b[32m -0.589\u001b[0m \u001b[32m -0.640\u001b[0m,  37% removed (alpha=  17.9846, nnz=166/262) \u001b[32mfit ok\u001b[0m [ 5.0802 33.8386]\n",
      " -0.544  -0.592 -> \u001b[31m -0.605\u001b[0m \u001b[31m -0.652\u001b[0m,  38% removed (alpha=  22.2027, nnz=162/262) \u001b[33munder \u001b[0m [ 5.0802 33.8386]\n",
      " -0.544  -0.592 -> \u001b[31m -0.620\u001b[0m \u001b[31m -0.665\u001b[0m,  41% removed (alpha=  27.4100, nnz=155/262) \u001b[33munder \u001b[0m [ 5.0802 33.8386]\n",
      " -0.544  -0.592 -> \u001b[32m -0.588\u001b[0m \u001b[32m -0.639\u001b[0m,  31% removed (alpha=  15.8489, nnz=182/262) \u001b[32mfit ok\u001b[0m [14.568  22.2027]\n",
      " -0.544  -0.592 -> \u001b[32m -0.592\u001b[0m \u001b[32m -0.641\u001b[0m,  27% removed (alpha=  17.2425, nnz=191/262) \u001b[32mfit ok\u001b[0m [14.568  22.2027]\n",
      " -0.544  -0.592 -> \u001b[32m -0.596\u001b[0m \u001b[32m -0.644\u001b[0m,  26% removed (alpha=  18.7587, nnz=195/262) \u001b[32mfit ok\u001b[0m [14.568  22.2027]\n",
      " -0.544  -0.592 -> \u001b[31m -0.601\u001b[0m \u001b[32m -0.648\u001b[0m,  26% removed (alpha=  20.4082, nnz=195/262) \u001b[32mfit ok\u001b[0m [14.568  22.2027]\n",
      "METRICS orig   tr -0.544\u001b[0m, va -0.592\u001b[0m \u001b[1m[te -0.586]\u001b[0m,  ntrees 100,  nnodes  8958,  nleafs  4529,  nnz  4527\n",
      "METRICS prev   \u001b[31mtr -0.602\u001b[0m, \u001b[32mva -0.650\u001b[0m \u001b[1m\u001b[31m[te -0.652]\u001b[0m,  ntrees  24,  nnodes   888,  nleafs   456,  nnz   281\n",
      "METRICS now    \u001b[32mtr -0.574\u001b[0m, \u001b[32mva -0.630\u001b[0m \u001b[1m\u001b[32m[te -0.630]\u001b[0m,  ntrees  24,  nnodes   850,  nleafs   437,  nnz   289\n",
      "\n",
      "Level 4 xxtrain.shape (11478, 356) dense transform time: 0.00s, 1.21s relerr 0.1\n",
      " -0.544  -0.592 -> \u001b[32m -0.529\u001b[0m \u001b[32m -0.586\u001b[0m,   0% removed (alpha=   0.0026, nnz=355/356) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.528\u001b[0m \u001b[32m -0.586\u001b[0m,   0% removed (alpha=   0.0067, nnz=355/356) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.528\u001b[0m \u001b[32m -0.585\u001b[0m,   0% removed (alpha=   0.0172, nnz=355/356) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.527\u001b[0m \u001b[32m -0.585\u001b[0m,   0% removed (alpha=   0.0444, nnz=355/356) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.521\u001b[0m \u001b[32m -0.578\u001b[0m,   3% removed (alpha=   0.1145, nnz=347/356) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.521\u001b[0m \u001b[32m -0.578\u001b[0m,   6% removed (alpha=   0.2955, nnz=336/356) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.522\u001b[0m \u001b[32m -0.579\u001b[0m,  10% removed (alpha=   0.7627, nnz=319/356) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.530\u001b[0m \u001b[32m -0.591\u001b[0m,  21% removed (alpha=   1.9684, nnz=280/356) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.549\u001b[0m \u001b[32m -0.609\u001b[0m,  37% removed (alpha=   5.0802, nnz=225/356) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.596\u001b[0m \u001b[32m -0.648\u001b[0m,  48% removed (alpha=  13.1113, nnz=186/356) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -0.758\u001b[0m \u001b[33m -0.774\u001b[0m,  53% removed (alpha=  33.8386, nnz=169/356) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -1.038\u001b[0m \u001b[33m -1.073\u001b[0m,  61% removed (alpha=  87.3326, nnz=139/356) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -1.540\u001b[0m \u001b[33m -1.604\u001b[0m,  74% removed (alpha= 225.3934, nnz= 93/356) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -2.307\u001b[0m \u001b[33m -2.337\u001b[0m,  82% removed (alpha= 581.7091, nnz= 63/356) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -3.911\u001b[0m \u001b[33m -3.981\u001b[0m,  88% removed (alpha=1501.3107, nnz= 43/356) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -6.797\u001b[0m \u001b[33m -6.884\u001b[0m,  92% removed (alpha=3874.6751, nnz= 30/356) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.556\u001b[0m \u001b[32m -0.616\u001b[0m,  37% removed (alpha=   6.2717, nnz=223/356) \u001b[32mfit ok\u001b[0m [ 5.0802 33.8386]\n",
      " -0.544  -0.592 -> \u001b[32m -0.563\u001b[0m \u001b[32m -0.621\u001b[0m,  39% removed (alpha=   7.7426, nnz=218/356) \u001b[32mfit ok\u001b[0m [ 5.0802 33.8386]\n",
      " -0.544  -0.592 -> \u001b[32m -0.570\u001b[0m \u001b[32m -0.627\u001b[0m,  40% removed (alpha=   9.5586, nnz=214/356) \u001b[32mfit ok\u001b[0m [ 5.0802 33.8386]\n",
      " -0.544  -0.592 -> \u001b[32m -0.580\u001b[0m \u001b[32m -0.636\u001b[0m,  39% removed (alpha=  11.8004, nnz=216/356) \u001b[32mfit ok\u001b[0m [ 5.0802 33.8386]\n",
      " -0.544  -0.592 -> \u001b[31m -0.603\u001b[0m \u001b[31m -0.658\u001b[0m,  42% removed (alpha=  14.5680, nnz=206/356) \u001b[33munder \u001b[0m [ 5.0802 33.8386]\n",
      " -0.544  -0.592 -> \u001b[31m -0.634\u001b[0m \u001b[31m -0.679\u001b[0m,  43% removed (alpha=  17.9846, nnz=202/356) \u001b[33munder \u001b[0m [ 5.0802 33.8386]\n",
      " -0.544  -0.592 -> \u001b[33m -0.664\u001b[0m \u001b[31m -0.702\u001b[0m,  43% removed (alpha=  22.2027, nnz=202/356) \u001b[33munder \u001b[0m [ 5.0802 33.8386]\n",
      " -0.544  -0.592 -> \u001b[33m -0.706\u001b[0m \u001b[33m -0.734\u001b[0m,  48% removed (alpha=  27.4100, nnz=184/356) \u001b[33munder \u001b[0m [ 5.0802 33.8386]\n",
      " -0.544  -0.592 -> \u001b[32m -0.580\u001b[0m \u001b[32m -0.633\u001b[0m,  42% removed (alpha=  10.3990, nnz=205/356) \u001b[32mfit ok\u001b[0m [ 9.5586 14.568 ]\n",
      " -0.544  -0.592 -> \u001b[32m -0.583\u001b[0m \u001b[32m -0.636\u001b[0m,  42% removed (alpha=  11.3134, nnz=205/356) \u001b[32mfit ok\u001b[0m [ 9.5586 14.568 ]\n",
      " -0.544  -0.592 -> \u001b[32m -0.588\u001b[0m \u001b[32m -0.642\u001b[0m,  42% removed (alpha=  12.3082, nnz=208/356) \u001b[32mfit ok\u001b[0m [ 9.5586 14.568 ]\n",
      " -0.544  -0.592 -> \u001b[32m -0.597\u001b[0m \u001b[32m -0.649\u001b[0m,  43% removed (alpha=  13.3905, nnz=203/356) \u001b[32mfit ok\u001b[0m [ 9.5586 14.568 ]\n",
      "METRICS orig   tr -0.544\u001b[0m, va -0.592\u001b[0m \u001b[1m[te -0.586]\u001b[0m,  ntrees 100,  nnodes  8958,  nleafs  4529,  nnz  4527\n",
      "METRICS prev   \u001b[32mtr -0.574\u001b[0m, \u001b[32mva -0.630\u001b[0m \u001b[1m\u001b[32m[te -0.630]\u001b[0m,  ntrees  24,  nnodes   850,  nleafs   437,  nnz   289\n",
      "METRICS now    \u001b[32mtr -0.596\u001b[0m, \u001b[32mva -0.648\u001b[0m \u001b[1m\u001b[32m[te -0.642]\u001b[0m,  ntrees  24,  nnodes   802,  nleafs   413,  nnz   273\n",
      "\n",
      "Level 5 xxtrain.shape (11478, 413) dense transform time: 0.00s, 1.20s relerr 0.1\n",
      " -0.544  -0.592 -> \u001b[32m -0.515\u001b[0m \u001b[32m -0.567\u001b[0m,   0% removed (alpha=   0.0026, nnz=412/413) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.515\u001b[0m \u001b[32m -0.566\u001b[0m,   0% removed (alpha=   0.0067, nnz=412/413) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.514\u001b[0m \u001b[32m -0.566\u001b[0m,   0% removed (alpha=   0.0172, nnz=412/413) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.514\u001b[0m \u001b[32m -0.566\u001b[0m,   0% removed (alpha=   0.0444, nnz=412/413) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.513\u001b[0m \u001b[32m -0.566\u001b[0m,   8% removed (alpha=   0.1145, nnz=378/413) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.513\u001b[0m \u001b[32m -0.566\u001b[0m,   9% removed (alpha=   0.2955, nnz=376/413) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.515\u001b[0m \u001b[32m -0.569\u001b[0m,  19% removed (alpha=   0.7627, nnz=335/413) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.521\u001b[0m \u001b[32m -0.577\u001b[0m,  24% removed (alpha=   1.9684, nnz=312/413) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.545\u001b[0m \u001b[32m -0.603\u001b[0m,  37% removed (alpha=   5.0802, nnz=261/413) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[31m -0.604\u001b[0m \u001b[31m -0.667\u001b[0m,  47% removed (alpha=  13.1113, nnz=219/413) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -0.738\u001b[0m \u001b[33m -0.808\u001b[0m,  54% removed (alpha=  33.8386, nnz=189/413) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -1.028\u001b[0m \u001b[33m -1.110\u001b[0m,  61% removed (alpha=  87.3326, nnz=162/413) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -1.635\u001b[0m \u001b[33m -1.750\u001b[0m,  69% removed (alpha= 225.3934, nnz=126/413) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -2.695\u001b[0m \u001b[33m -2.907\u001b[0m,  79% removed (alpha= 581.7091, nnz= 87/413) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -4.194\u001b[0m \u001b[33m -4.323\u001b[0m,  86% removed (alpha=1501.3107, nnz= 59/413) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -7.236\u001b[0m \u001b[33m -7.287\u001b[0m,  92% removed (alpha=3874.6751, nnz= 34/413) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.524\u001b[0m \u001b[32m -0.581\u001b[0m,  30% removed (alpha=   2.4301, nnz=290/413) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.527\u001b[0m \u001b[32m -0.584\u001b[0m,  30% removed (alpha=   3.0000, nnz=291/413) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.531\u001b[0m \u001b[32m -0.589\u001b[0m,  30% removed (alpha=   3.7036, nnz=289/413) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.538\u001b[0m \u001b[32m -0.597\u001b[0m,  37% removed (alpha=   4.5723, nnz=260/413) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.545\u001b[0m \u001b[32m -0.605\u001b[0m,  38% removed (alpha=   5.6446, nnz=254/413) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.555\u001b[0m \u001b[32m -0.616\u001b[0m,  39% removed (alpha=   6.9685, nnz=250/413) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.566\u001b[0m \u001b[32m -0.629\u001b[0m,  41% removed (alpha=   8.6028, nnz=245/413) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.583\u001b[0m \u001b[32m -0.645\u001b[0m,  43% removed (alpha=  10.6205, nnz=237/413) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.574\u001b[0m \u001b[32m -0.635\u001b[0m,  41% removed (alpha=   9.3593, nnz=244/413) \u001b[32mfit ok\u001b[0m [ 8.6028 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.579\u001b[0m \u001b[32m -0.641\u001b[0m,  42% removed (alpha=  10.1822, nnz=239/413) \u001b[32mfit ok\u001b[0m [ 8.6028 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.586\u001b[0m \u001b[32m -0.648\u001b[0m,  42% removed (alpha=  11.0776, nnz=239/413) \u001b[32mfit ok\u001b[0m [ 8.6028 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.594\u001b[0m \u001b[31m -0.657\u001b[0m,  42% removed (alpha=  12.0516, nnz=240/413) \u001b[31mover  \u001b[0m [ 8.6028 13.1113]\n",
      "METRICS orig   tr -0.544\u001b[0m, va -0.592\u001b[0m \u001b[1m[te -0.586]\u001b[0m,  ntrees 100,  nnodes  8958,  nleafs  4529,  nnz  4527\n",
      "METRICS prev   \u001b[32mtr -0.596\u001b[0m, \u001b[32mva -0.648\u001b[0m \u001b[1m\u001b[32m[te -0.642]\u001b[0m,  ntrees  24,  nnodes   802,  nleafs   413,  nnz   273\n",
      "METRICS now    \u001b[32mtr -0.583\u001b[0m, \u001b[32mva -0.645\u001b[0m \u001b[1m\u001b[32m[te -0.642]\u001b[0m,  ntrees  24,  nnodes   760,  nleafs   392,  nnz   262\n",
      "\n",
      "Level 6 xxtrain.shape (11478, 392) dense transform time: 0.00s, 1.17s relerr 0.1\n",
      " -0.544  -0.592 -> \u001b[32m -0.524\u001b[0m \u001b[32m -0.577\u001b[0m,   1% removed (alpha=   0.0026, nnz=390/392) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.524\u001b[0m \u001b[32m -0.577\u001b[0m,   1% removed (alpha=   0.0067, nnz=390/392) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.523\u001b[0m \u001b[32m -0.577\u001b[0m,   1% removed (alpha=   0.0172, nnz=390/392) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.523\u001b[0m \u001b[32m -0.577\u001b[0m,   1% removed (alpha=   0.0444, nnz=388/392) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.523\u001b[0m \u001b[32m -0.577\u001b[0m,   4% removed (alpha=   0.1145, nnz=378/392) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.523\u001b[0m \u001b[32m -0.578\u001b[0m,  12% removed (alpha=   0.2955, nnz=343/392) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.524\u001b[0m \u001b[32m -0.579\u001b[0m,  15% removed (alpha=   0.7627, nnz=335/392) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.527\u001b[0m \u001b[32m -0.585\u001b[0m,  17% removed (alpha=   1.9684, nnz=324/392) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.544\u001b[0m \u001b[32m -0.603\u001b[0m,  30% removed (alpha=   5.0802, nnz=274/392) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[31m -0.617\u001b[0m \u001b[31m -0.671\u001b[0m,  42% removed (alpha=  13.1113, nnz=227/392) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -0.856\u001b[0m \u001b[33m -0.898\u001b[0m,  46% removed (alpha=  33.8386, nnz=210/392) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -1.221\u001b[0m \u001b[33m -1.285\u001b[0m,  58% removed (alpha=  87.3326, nnz=164/392) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -1.921\u001b[0m \u001b[33m -2.125\u001b[0m,  70% removed (alpha= 225.3934, nnz=119/392) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -3.078\u001b[0m \u001b[33m -3.435\u001b[0m,  80% removed (alpha= 581.7091, nnz= 77/392) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -5.070\u001b[0m \u001b[33m -5.456\u001b[0m,  85% removed (alpha=1501.3107, nnz= 57/392) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[33m -7.227\u001b[0m \u001b[33m -7.615\u001b[0m,  91% removed (alpha=3874.6751, nnz= 37/392) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.544  -0.592 -> \u001b[32m -0.529\u001b[0m \u001b[32m -0.587\u001b[0m,  26% removed (alpha=   2.4301, nnz=290/392) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.531\u001b[0m \u001b[32m -0.589\u001b[0m,  26% removed (alpha=   3.0000, nnz=290/392) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.535\u001b[0m \u001b[32m -0.592\u001b[0m,  28% removed (alpha=   3.7036, nnz=283/392) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.540\u001b[0m \u001b[32m -0.597\u001b[0m,  31% removed (alpha=   4.5723, nnz=272/392) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.547\u001b[0m \u001b[32m -0.604\u001b[0m,  31% removed (alpha=   5.6446, nnz=272/392) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.557\u001b[0m \u001b[32m -0.614\u001b[0m,  32% removed (alpha=   6.9685, nnz=267/392) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.572\u001b[0m \u001b[32m -0.628\u001b[0m,  34% removed (alpha=   8.6028, nnz=258/392) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.591\u001b[0m \u001b[32m -0.647\u001b[0m,  35% removed (alpha=  10.6205, nnz=253/392) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.579\u001b[0m \u001b[32m -0.635\u001b[0m,  35% removed (alpha=   9.3593, nnz=256/392) \u001b[32mfit ok\u001b[0m [ 8.6028 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.586\u001b[0m \u001b[32m -0.642\u001b[0m,  35% removed (alpha=  10.1822, nnz=255/392) \u001b[32mfit ok\u001b[0m [ 8.6028 13.1113]\n",
      " -0.544  -0.592 -> \u001b[32m -0.595\u001b[0m \u001b[32m -0.651\u001b[0m,  37% removed (alpha=  11.0776, nnz=247/392) \u001b[32mfit ok\u001b[0m [ 8.6028 13.1113]\n",
      " -0.544  -0.592 -> \u001b[31m -0.605\u001b[0m \u001b[31m -0.661\u001b[0m,  38% removed (alpha=  12.0516, nnz=244/392) \u001b[33munder \u001b[0m [ 8.6028 13.1113]\n",
      "METRICS orig   tr -0.544\u001b[0m, va -0.592\u001b[0m \u001b[1m[te -0.586]\u001b[0m,  ntrees 100,  nnodes  8958,  nleafs  4529,  nnz  4527\n",
      "METRICS prev   \u001b[32mtr -0.583\u001b[0m, \u001b[32mva -0.645\u001b[0m \u001b[1m\u001b[32m[te -0.642]\u001b[0m,  ntrees  24,  nnodes   760,  nleafs   392,  nnz   262\n",
      "METRICS now    \u001b[32mtr -0.595\u001b[0m, \u001b[32mva -0.651\u001b[0m \u001b[1m\u001b[32m[te -0.644]\u001b[0m,  ntrees  24,  nnodes   752,  nleafs   388,  nnz   247\n",
      "\n",
      "DONE, depth of tree reached 6, 6\n",
      "ChaahatRegTarget0 number of non-zero leaves: 4527 -> 247, test accuracy: -58.6% -> -64.4%\n",
      "\n",
      "| XGBOOST's base_score\n",
      "|   base_score diff std      3.590264912223747e-05 (!) NOT OK\n",
      "|   base_score reported      144.68694\n",
      "|   versus manually detected 144.6868847773189\n",
      "|   abs err                  5.522268108393291e-05\n",
      "|   rel err                  3.816701153810628e-07\n",
      "|   base_score OK with relative tolerance 0.001\n",
      "\n",
      "MODEL PERF: mtr -0.172 mte -0.186 mva -0.212\n",
      "\n",
      "\n",
      "ROUND 1\n",
      "Level 0 xxtrain.shape (11478, 100) dense transform time: 0.01s, 4.93s relerr 0.1\n",
      " -0.172  -0.212 -> \u001b[32m -0.142\u001b[0m \u001b[32m -0.185\u001b[0m,   0% removed (alpha=   0.0026, nnz=100/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.141\u001b[0m \u001b[32m -0.184\u001b[0m,   5% removed (alpha=   0.0067, nnz= 95/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.141\u001b[0m \u001b[32m -0.184\u001b[0m,  11% removed (alpha=   0.0172, nnz= 89/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.141\u001b[0m \u001b[32m -0.185\u001b[0m,  17% removed (alpha=   0.0444, nnz= 83/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.140\u001b[0m \u001b[32m -0.185\u001b[0m,  22% removed (alpha=   0.1145, nnz= 78/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.143\u001b[0m \u001b[32m -0.188\u001b[0m,  32% removed (alpha=   0.2955, nnz= 68/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.152\u001b[0m \u001b[32m -0.195\u001b[0m,  42% removed (alpha=   0.7627, nnz= 58/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.171\u001b[0m \u001b[32m -0.214\u001b[0m,  54% removed (alpha=   1.9684, nnz= 46/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[31m -0.190\u001b[0m \u001b[32m -0.233\u001b[0m,  69% removed (alpha=   5.0802, nnz= 31/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.213\u001b[0m \u001b[33m -0.258\u001b[0m,  73% removed (alpha=  13.1113, nnz= 27/100) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.268\u001b[0m \u001b[33m -0.309\u001b[0m,  78% removed (alpha=  33.8386, nnz= 22/100) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.365\u001b[0m \u001b[33m -0.398\u001b[0m,  82% removed (alpha=  87.3326, nnz= 18/100) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.521\u001b[0m \u001b[33m -0.540\u001b[0m,  88% removed (alpha= 225.3934, nnz= 12/100) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.764\u001b[0m \u001b[33m -0.781\u001b[0m,  92% removed (alpha= 581.7091, nnz=  8/100) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -1.061\u001b[0m \u001b[33m -1.103\u001b[0m,  97% removed (alpha=1501.3107, nnz=  3/100) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -1.235\u001b[0m \u001b[33m -1.300\u001b[0m,  99% removed (alpha=3874.6751, nnz=  1/100) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.180\u001b[0m \u001b[32m -0.222\u001b[0m,  59% removed (alpha=   2.4301, nnz= 41/100) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.172  -0.212 -> \u001b[32m -0.184\u001b[0m \u001b[32m -0.227\u001b[0m,  64% removed (alpha=   3.0000, nnz= 36/100) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.172  -0.212 -> \u001b[32m -0.187\u001b[0m \u001b[32m -0.230\u001b[0m,  67% removed (alpha=   3.7036, nnz= 33/100) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.172  -0.212 -> \u001b[31m -0.190\u001b[0m \u001b[31m -0.233\u001b[0m,  69% removed (alpha=   4.5723, nnz= 31/100) \u001b[33munder \u001b[0m [ 1.9684 13.1113]\n",
      " -0.172  -0.212 -> \u001b[31m -0.193\u001b[0m \u001b[31m -0.237\u001b[0m,  69% removed (alpha=   5.6446, nnz= 31/100) \u001b[33munder \u001b[0m [ 1.9684 13.1113]\n",
      " -0.172  -0.212 -> \u001b[31m -0.198\u001b[0m \u001b[31m -0.243\u001b[0m,  70% removed (alpha=   6.9685, nnz= 30/100) \u001b[33munder \u001b[0m [ 1.9684 13.1113]\n",
      " -0.172  -0.212 -> \u001b[31m -0.204\u001b[0m \u001b[31m -0.249\u001b[0m,  71% removed (alpha=   8.6028, nnz= 29/100) \u001b[33munder \u001b[0m [ 1.9684 13.1113]\n",
      " -0.172  -0.212 -> \u001b[33m -0.208\u001b[0m \u001b[31m -0.254\u001b[0m,  73% removed (alpha=  10.6205, nnz= 27/100) \u001b[33munder \u001b[0m [ 1.9684 13.1113]\n",
      " -0.172  -0.212 -> \u001b[32m -0.184\u001b[0m \u001b[32m -0.227\u001b[0m,  66% removed (alpha=   3.2638, nnz= 34/100) \u001b[32mfit ok\u001b[0m [3.     4.5723]\n",
      " -0.172  -0.212 -> \u001b[32m -0.185\u001b[0m \u001b[32m -0.229\u001b[0m,  66% removed (alpha=   3.5508, nnz= 34/100) \u001b[32mfit ok\u001b[0m [3.     4.5723]\n",
      " -0.172  -0.212 -> \u001b[32m -0.186\u001b[0m \u001b[32m -0.230\u001b[0m,  69% removed (alpha=   3.8630, nnz= 31/100) \u001b[32mfit ok\u001b[0m [3.     4.5723]\n",
      " -0.172  -0.212 -> \u001b[32m -0.187\u001b[0m \u001b[32m -0.231\u001b[0m,  69% removed (alpha=   4.2027, nnz= 31/100) \u001b[32mfit ok\u001b[0m [3.     4.5723]\n",
      "METRICS orig   tr -0.172\u001b[0m, va -0.212\u001b[0m \u001b[1m[te -0.186]\u001b[0m,  ntrees 100,  nnodes  9586,  nleafs  4843,  nnz  4837\n",
      "METRICS prev   \u001b[32mtr -0.172\u001b[0m, \u001b[32mva -0.212\u001b[0m \u001b[1m\u001b[32m[te -0.186]\u001b[0m,  ntrees 100,  nnodes  9586,  nleafs  4843,  nnz  4837\n",
      "METRICS now    \u001b[31mtr -0.190\u001b[0m, \u001b[32mva -0.233\u001b[0m \u001b[1m\u001b[31m[te -0.208]\u001b[0m,  ntrees  31,  nnodes  2861,  nleafs  1446,  nnz  1446\n",
      "\n",
      "Level 1 xxtrain.shape (11478, 124) dense transform time: 0.01s, 1.58s relerr 0.1\n",
      " -0.172  -0.212 -> \u001b[33m -0.255\u001b[0m \u001b[33m -0.294\u001b[0m,   0% removed (alpha=   0.0026, nnz=124/124) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.251\u001b[0m \u001b[33m -0.291\u001b[0m,   0% removed (alpha=   0.0067, nnz=124/124) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.247\u001b[0m \u001b[33m -0.287\u001b[0m,   0% removed (alpha=   0.0172, nnz=124/124) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.241\u001b[0m \u001b[33m -0.282\u001b[0m,   0% removed (alpha=   0.0444, nnz=124/124) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.238\u001b[0m \u001b[33m -0.279\u001b[0m,   2% removed (alpha=   0.1145, nnz=122/124) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.236\u001b[0m \u001b[33m -0.276\u001b[0m,   2% removed (alpha=   0.2955, nnz=122/124) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.232\u001b[0m \u001b[33m -0.273\u001b[0m,   2% removed (alpha=   0.7627, nnz=122/124) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.222\u001b[0m \u001b[33m -0.265\u001b[0m,   5% removed (alpha=   1.9684, nnz=118/124) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[31m -0.196\u001b[0m \u001b[31m -0.243\u001b[0m,  15% removed (alpha=   5.0802, nnz=106/124) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.242\u001b[0m \u001b[33m -0.283\u001b[0m,  36% removed (alpha=  13.1113, nnz= 79/124) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.283\u001b[0m \u001b[33m -0.324\u001b[0m,  70% removed (alpha=  33.8386, nnz= 37/124) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.437\u001b[0m \u001b[33m -0.462\u001b[0m,  77% removed (alpha=  87.3326, nnz= 28/124) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.691\u001b[0m \u001b[33m -0.705\u001b[0m,  87% removed (alpha= 225.3934, nnz= 16/124) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.945\u001b[0m \u001b[33m -0.980\u001b[0m,  93% removed (alpha= 581.7091, nnz=  9/124) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -1.116\u001b[0m \u001b[33m -1.168\u001b[0m,  97% removed (alpha=1501.3107, nnz=  4/124) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -1.241\u001b[0m \u001b[33m -1.306\u001b[0m,  98% removed (alpha=3874.6751, nnz=  2/124) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      "METRICS orig   tr -0.172\u001b[0m, va -0.212\u001b[0m \u001b[1m[te -0.186]\u001b[0m,  ntrees 100,  nnodes  9586,  nleafs  4843,  nnz  4837\n",
      "METRICS prev   \u001b[31mtr -0.190\u001b[0m, \u001b[32mva -0.233\u001b[0m \u001b[1m\u001b[31m[te -0.208]\u001b[0m,  ntrees  31,  nnodes  2861,  nleafs  1446,  nnz  1446\n",
      "METRICS now    \u001b[31mtr -0.190\u001b[0m, \u001b[32mva -0.233\u001b[0m \u001b[1m\u001b[31m[te -0.208]\u001b[0m,  ntrees  31,  nnodes  2861,  nleafs  1446,  nnz  1446\n",
      "\n",
      "Level 2 xxtrain.shape (11478, 246) dense transform time: 0.01s, 1.61s relerr 0.1\n",
      " -0.172  -0.212 -> \u001b[32m -0.149\u001b[0m \u001b[32m -0.199\u001b[0m,   0% removed (alpha=   0.0026, nnz=246/246) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.149\u001b[0m \u001b[32m -0.199\u001b[0m,   0% removed (alpha=   0.0067, nnz=246/246) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.149\u001b[0m \u001b[32m -0.198\u001b[0m,   0% removed (alpha=   0.0172, nnz=246/246) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.148\u001b[0m \u001b[32m -0.198\u001b[0m,   0% removed (alpha=   0.0444, nnz=246/246) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.147\u001b[0m \u001b[32m -0.198\u001b[0m,   2% removed (alpha=   0.1145, nnz=242/246) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.148\u001b[0m \u001b[32m -0.199\u001b[0m,   2% removed (alpha=   0.2955, nnz=240/246) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.155\u001b[0m \u001b[32m -0.207\u001b[0m,   6% removed (alpha=   0.7627, nnz=232/246) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.163\u001b[0m \u001b[32m -0.217\u001b[0m,  12% removed (alpha=   1.9684, nnz=217/246) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.223\u001b[0m \u001b[33m -0.266\u001b[0m,  41% removed (alpha=   5.0802, nnz=146/246) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.297\u001b[0m \u001b[33m -0.346\u001b[0m,  63% removed (alpha=  13.1113, nnz= 92/246) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.431\u001b[0m \u001b[33m -0.472\u001b[0m,  70% removed (alpha=  33.8386, nnz= 73/246) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.606\u001b[0m \u001b[33m -0.632\u001b[0m,  83% removed (alpha=  87.3326, nnz= 42/246) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.785\u001b[0m \u001b[33m -0.820\u001b[0m,  88% removed (alpha= 225.3934, nnz= 29/246) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -1.102\u001b[0m \u001b[33m -1.162\u001b[0m,  96% removed (alpha= 581.7091, nnz= 10/246) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -1.243\u001b[0m \u001b[33m -1.302\u001b[0m,  98% removed (alpha=1501.3107, nnz=  4/246) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -1.370\u001b[0m \u001b[33m -1.468\u001b[0m,  98% removed (alpha=3874.6751, nnz=  4/246) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[31m -0.190\u001b[0m \u001b[31m -0.235\u001b[0m,  13% removed (alpha=   0.9416, nnz=214/246) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.172  -0.212 -> \u001b[31m -0.191\u001b[0m \u001b[31m -0.236\u001b[0m,  14% removed (alpha=   1.1624, nnz=212/246) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.172  -0.212 -> \u001b[32m -0.188\u001b[0m \u001b[31m -0.233\u001b[0m,  19% removed (alpha=   1.4350, nnz=199/246) \u001b[31mover  \u001b[0m [0.7627 5.0802]\n",
      " -0.172  -0.212 -> \u001b[31m -0.190\u001b[0m \u001b[31m -0.235\u001b[0m,  20% removed (alpha=   1.7716, nnz=198/246) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.172  -0.212 -> \u001b[31m -0.193\u001b[0m \u001b[31m -0.237\u001b[0m,  21% removed (alpha=   2.1871, nnz=194/246) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.172  -0.212 -> \u001b[31m -0.194\u001b[0m \u001b[31m -0.239\u001b[0m,  23% removed (alpha=   2.7001, nnz=189/246) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.172  -0.212 -> \u001b[32m -0.184\u001b[0m \u001b[32m -0.231\u001b[0m,  39% removed (alpha=   3.3333, nnz=149/246) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.172  -0.212 -> \u001b[31m -0.201\u001b[0m \u001b[31m -0.249\u001b[0m,  41% removed (alpha=   4.1151, nnz=144/246) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.172  -0.212 -> \u001b[32m -0.185\u001b[0m \u001b[31m -0.234\u001b[0m,  46% removed (alpha=   2.9375, nnz=134/246) \u001b[31mover  \u001b[0m [2.7001 4.1151]\n",
      " -0.172  -0.212 -> \u001b[32m -0.187\u001b[0m \u001b[31m -0.235\u001b[0m,  44% removed (alpha=   3.1958, nnz=138/246) \u001b[31mover  \u001b[0m [2.7001 4.1151]\n",
      " -0.172  -0.212 -> \u001b[32m -0.189\u001b[0m \u001b[31m -0.237\u001b[0m,  44% removed (alpha=   3.4768, nnz=137/246) \u001b[31mover  \u001b[0m [2.7001 4.1151]\n",
      " -0.172  -0.212 -> \u001b[31m -0.191\u001b[0m \u001b[31m -0.239\u001b[0m,  45% removed (alpha=   3.7825, nnz=136/246) \u001b[33munder \u001b[0m [2.7001 4.1151]\n",
      "METRICS orig   tr -0.172\u001b[0m, va -0.212\u001b[0m \u001b[1m[te -0.186]\u001b[0m,  ntrees 100,  nnodes  9586,  nleafs  4843,  nnz  4837\n",
      "METRICS prev   \u001b[31mtr -0.190\u001b[0m, \u001b[32mva -0.233\u001b[0m \u001b[1m\u001b[31m[te -0.208]\u001b[0m,  ntrees  31,  nnodes  2861,  nleafs  1446,  nnz  1446\n",
      "METRICS now    \u001b[32mtr -0.184\u001b[0m, \u001b[32mva -0.231\u001b[0m \u001b[1m\u001b[32m[te -0.199]\u001b[0m,  ntrees  31,  nnodes  2201,  nleafs  1116,  nnz  1102\n",
      "\n",
      "Level 3 xxtrain.shape (11478, 401) dense transform time: 0.01s, 1.60s relerr 0.1\n",
      " -0.172  -0.212 -> \u001b[32m -0.165\u001b[0m \u001b[32m -0.216\u001b[0m,   0% removed (alpha=   0.0026, nnz=400/401) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.161\u001b[0m \u001b[32m -0.213\u001b[0m,   0% removed (alpha=   0.0067, nnz=400/401) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.158\u001b[0m \u001b[32m -0.211\u001b[0m,   0% removed (alpha=   0.0172, nnz=399/401) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.156\u001b[0m \u001b[32m -0.209\u001b[0m,   1% removed (alpha=   0.0444, nnz=395/401) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.154\u001b[0m \u001b[32m -0.209\u001b[0m,   3% removed (alpha=   0.1145, nnz=388/401) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.154\u001b[0m \u001b[32m -0.211\u001b[0m,   7% removed (alpha=   0.2955, nnz=371/401) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.163\u001b[0m \u001b[32m -0.220\u001b[0m,  27% removed (alpha=   0.7627, nnz=294/401) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.189\u001b[0m \u001b[31m -0.244\u001b[0m,  40% removed (alpha=   1.9684, nnz=242/401) \u001b[31mover  \u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1510.4727234994657, tolerance: 1418.6648350597\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2176.1222640992846, tolerance: 1418.6648350597\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.172  -0.212 -> \u001b[33m -0.241\u001b[0m \u001b[33m -0.284\u001b[0m,  61% removed (alpha=   5.0802, nnz=157/401) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.310\u001b[0m \u001b[33m -0.366\u001b[0m,  72% removed (alpha=  13.1113, nnz=111/401) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.448\u001b[0m \u001b[33m -0.519\u001b[0m,  82% removed (alpha=  33.8386, nnz= 72/401) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.664\u001b[0m \u001b[33m -0.833\u001b[0m,  85% removed (alpha=  87.3326, nnz= 60/401) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -1.040\u001b[0m \u001b[33m -1.314\u001b[0m,  90% removed (alpha= 225.3934, nnz= 40/401) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -1.344\u001b[0m \u001b[33m -1.572\u001b[0m,  94% removed (alpha= 581.7091, nnz= 23/401) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -1.699\u001b[0m \u001b[33m -1.835\u001b[0m,  96% removed (alpha=1501.3107, nnz= 15/401) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -2.450\u001b[0m \u001b[33m -2.499\u001b[0m,  97% removed (alpha=3874.6751, nnz= 12/401) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.178\u001b[0m \u001b[32m -0.227\u001b[0m,  19% removed (alpha=   0.3648, nnz=324/401) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.178\u001b[0m \u001b[32m -0.226\u001b[0m,  20% removed (alpha=   0.4504, nnz=321/401) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.178\u001b[0m \u001b[32m -0.227\u001b[0m,  20% removed (alpha=   0.5560, nnz=319/401) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.176\u001b[0m \u001b[32m -0.225\u001b[0m,  26% removed (alpha=   0.6864, nnz=295/401) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.174\u001b[0m \u001b[32m -0.222\u001b[0m,  33% removed (alpha=   0.8474, nnz=268/401) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.176\u001b[0m \u001b[32m -0.223\u001b[0m,  37% removed (alpha=   1.0462, nnz=251/401) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.179\u001b[0m \u001b[32m -0.226\u001b[0m,  41% removed (alpha=   1.2915, nnz=238/401) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.183\u001b[0m \u001b[32m -0.232\u001b[0m,  41% removed (alpha=   1.5945, nnz=236/401) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.181\u001b[0m \u001b[32m -0.230\u001b[0m,  38% removed (alpha=   1.4051, nnz=249/401) \u001b[32mfit ok\u001b[0m [1.2915 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.182\u001b[0m \u001b[32m -0.231\u001b[0m,  39% removed (alpha=   1.5287, nnz=245/401) \u001b[32mfit ok\u001b[0m [1.2915 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.183\u001b[0m \u001b[32m -0.232\u001b[0m,  41% removed (alpha=   1.6631, nnz=237/401) \u001b[32mfit ok\u001b[0m [1.2915 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.185\u001b[0m \u001b[31m -0.234\u001b[0m,  42% removed (alpha=   1.8093, nnz=233/401) \u001b[31mover  \u001b[0m [1.2915 1.9684]\n",
      "METRICS orig   tr -0.172\u001b[0m, va -0.212\u001b[0m \u001b[1m[te -0.186]\u001b[0m,  ntrees 100,  nnodes  9586,  nleafs  4843,  nnz  4837\n",
      "METRICS prev   \u001b[32mtr -0.184\u001b[0m, \u001b[32mva -0.231\u001b[0m \u001b[1m\u001b[32m[te -0.199]\u001b[0m,  ntrees  31,  nnodes  2201,  nleafs  1116,  nnz  1102\n",
      "METRICS now    \u001b[32mtr -0.183\u001b[0m, \u001b[32mva -0.232\u001b[0m \u001b[1m\u001b[32m[te -0.202]\u001b[0m,  ntrees  31,  nnodes  1793,  nleafs   912,  nnz   878\n",
      "\n",
      "Level 4 xxtrain.shape (11478, 578) dense transform time: 0.01s, 1.60s relerr 0.1\n",
      " -0.172  -0.212 -> \u001b[32m -0.152\u001b[0m \u001b[32m -0.225\u001b[0m,   1% removed (alpha=   0.0026, nnz=571/578) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.150\u001b[0m \u001b[32m -0.224\u001b[0m,   2% removed (alpha=   0.0067, nnz=566/578) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.149\u001b[0m \u001b[32m -0.222\u001b[0m,   4% removed (alpha=   0.0172, nnz=557/578) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.150\u001b[0m \u001b[32m -0.222\u001b[0m,   5% removed (alpha=   0.0444, nnz=550/578) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.152\u001b[0m \u001b[32m -0.221\u001b[0m,   9% removed (alpha=   0.1145, nnz=525/578) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.159\u001b[0m \u001b[32m -0.220\u001b[0m,  25% removed (alpha=   0.2955, nnz=436/578) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.174\u001b[0m \u001b[32m -0.231\u001b[0m,  38% removed (alpha=   0.7627, nnz=360/578) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.206\u001b[0m \u001b[31m -0.247\u001b[0m,  52% removed (alpha=   1.9684, nnz=278/578) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2097.001638303267, tolerance: 1418.6648350597\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.172  -0.212 -> \u001b[33m -0.267\u001b[0m \u001b[33m -0.300\u001b[0m,  65% removed (alpha=   5.0802, nnz=204/578) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3554.8360842864604, tolerance: 1418.6648350597\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6335.343299577675, tolerance: 1418.6648350597\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.172  -0.212 -> \u001b[33m -0.359\u001b[0m \u001b[33m -0.393\u001b[0m,  72% removed (alpha=  13.1113, nnz=162/578) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.481\u001b[0m \u001b[33m -0.515\u001b[0m,  82% removed (alpha=  33.8386, nnz=105/578) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.659\u001b[0m \u001b[33m -0.695\u001b[0m,  86% removed (alpha=  87.3326, nnz= 83/578) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.953\u001b[0m \u001b[33m -1.023\u001b[0m,  90% removed (alpha= 225.3934, nnz= 55/578) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -1.463\u001b[0m \u001b[33m -1.609\u001b[0m,  93% removed (alpha= 581.7091, nnz= 39/578) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -2.405\u001b[0m \u001b[33m -2.554\u001b[0m,  97% removed (alpha=1501.3107, nnz= 19/578) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -4.215\u001b[0m \u001b[33m -4.379\u001b[0m,  98% removed (alpha=3874.6751, nnz= 14/578) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.177\u001b[0m \u001b[31m -0.244\u001b[0m,  31% removed (alpha=   0.3648, nnz=401/578) \u001b[31mover  \u001b[0m [0.2955 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.178\u001b[0m \u001b[31m -0.244\u001b[0m,  30% removed (alpha=   0.4504, nnz=403/578) \u001b[31mover  \u001b[0m [0.2955 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.173\u001b[0m \u001b[32m -0.233\u001b[0m,  38% removed (alpha=   0.5560, nnz=356/578) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.176\u001b[0m \u001b[31m -0.235\u001b[0m,  39% removed (alpha=   0.6864, nnz=351/578) \u001b[31mover  \u001b[0m [0.2955 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.179\u001b[0m \u001b[31m -0.237\u001b[0m,  42% removed (alpha=   0.8474, nnz=335/578) \u001b[31mover  \u001b[0m [0.2955 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.182\u001b[0m \u001b[31m -0.240\u001b[0m,  44% removed (alpha=   1.0462, nnz=326/578) \u001b[31mover  \u001b[0m [0.2955 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.188\u001b[0m \u001b[31m -0.244\u001b[0m,  46% removed (alpha=   1.2915, nnz=312/578) \u001b[31mover  \u001b[0m [0.2955 1.9684]\n",
      " -0.172  -0.212 -> \u001b[31m -0.192\u001b[0m \u001b[31m -0.248\u001b[0m,  46% removed (alpha=   1.5945, nnz=310/578) \u001b[33munder \u001b[0m [0.2955 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.172\u001b[0m \u001b[32m -0.230\u001b[0m,  36% removed (alpha=   0.4900, nnz=370/578) \u001b[32mfit ok\u001b[0m [0.4504 0.6864]\n",
      " -0.172  -0.212 -> \u001b[32m -0.173\u001b[0m \u001b[32m -0.230\u001b[0m,  37% removed (alpha=   0.5331, nnz=364/578) \u001b[32mfit ok\u001b[0m [0.4504 0.6864]\n",
      " -0.172  -0.212 -> \u001b[32m -0.174\u001b[0m \u001b[32m -0.230\u001b[0m,  39% removed (alpha=   0.5800, nnz=355/578) \u001b[32mfit ok\u001b[0m [0.4504 0.6864]\n",
      " -0.172  -0.212 -> \u001b[32m -0.175\u001b[0m \u001b[32m -0.231\u001b[0m,  40% removed (alpha=   0.6310, nnz=346/578) \u001b[32mfit ok\u001b[0m [0.4504 0.6864]\n",
      "METRICS orig   tr -0.172\u001b[0m, va -0.212\u001b[0m \u001b[1m[te -0.186]\u001b[0m,  ntrees 100,  nnodes  9586,  nleafs  4843,  nnz  4837\n",
      "METRICS prev   \u001b[32mtr -0.183\u001b[0m, \u001b[32mva -0.232\u001b[0m \u001b[1m\u001b[32m[te -0.202]\u001b[0m,  ntrees  31,  nnodes  1793,  nleafs   912,  nnz   878\n",
      "METRICS now    \u001b[32mtr -0.175\u001b[0m, \u001b[32mva -0.231\u001b[0m \u001b[1m\u001b[32m[te -0.189]\u001b[0m,  ntrees  31,  nnodes  1453,  nleafs   742,  nnz   670\n",
      "\n",
      "Level 5 xxtrain.shape (11478, 742) dense transform time: 0.01s, 1.57s relerr 0.1\n",
      " -0.172  -0.212 -> \u001b[32m -0.144\u001b[0m \u001b[32m -0.210\u001b[0m,   2% removed (alpha=   0.0026, nnz=727/742) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.144\u001b[0m \u001b[32m -0.210\u001b[0m,   2% removed (alpha=   0.0067, nnz=724/742) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.142\u001b[0m \u001b[32m -0.208\u001b[0m,   6% removed (alpha=   0.0172, nnz=699/742) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.143\u001b[0m \u001b[32m -0.208\u001b[0m,  11% removed (alpha=   0.0444, nnz=659/742) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.145\u001b[0m \u001b[32m -0.209\u001b[0m,  19% removed (alpha=   0.1145, nnz=599/742) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.150\u001b[0m \u001b[32m -0.211\u001b[0m,  30% removed (alpha=   0.2955, nnz=519/742) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.163\u001b[0m \u001b[32m -0.220\u001b[0m,  44% removed (alpha=   0.7627, nnz=416/742) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[31m -0.192\u001b[0m \u001b[31m -0.245\u001b[0m,  58% removed (alpha=   1.9684, nnz=309/742) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1637.4054603199818, tolerance: 1418.6648350597\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.172  -0.212 -> \u001b[33m -0.248\u001b[0m \u001b[33m -0.291\u001b[0m,  68% removed (alpha=   5.0802, nnz=241/742) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2011.5984013172715, tolerance: 1418.6648350597\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.172  -0.212 -> \u001b[33m -0.317\u001b[0m \u001b[33m -0.347\u001b[0m,  75% removed (alpha=  13.1113, nnz=183/742) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.433\u001b[0m \u001b[33m -0.465\u001b[0m,  83% removed (alpha=  33.8386, nnz=126/742) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.574\u001b[0m \u001b[33m -0.621\u001b[0m,  88% removed (alpha=  87.3326, nnz= 89/742) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.870\u001b[0m \u001b[33m -0.979\u001b[0m,  91% removed (alpha= 225.3934, nnz= 68/742) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -1.255\u001b[0m \u001b[33m -1.428\u001b[0m,  94% removed (alpha= 581.7091, nnz= 45/742) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -1.964\u001b[0m \u001b[33m -2.169\u001b[0m,  96% removed (alpha=1501.3107, nnz= 31/742) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -3.288\u001b[0m \u001b[33m -3.519\u001b[0m,  97% removed (alpha=3874.6751, nnz= 23/742) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.169\u001b[0m \u001b[32m -0.226\u001b[0m,  32% removed (alpha=   0.3648, nnz=502/742) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.164\u001b[0m \u001b[32m -0.221\u001b[0m,  39% removed (alpha=   0.4504, nnz=454/742) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.161\u001b[0m \u001b[32m -0.219\u001b[0m,  46% removed (alpha=   0.5560, nnz=398/742) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.163\u001b[0m \u001b[32m -0.220\u001b[0m,  45% removed (alpha=   0.6864, nnz=405/742) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.166\u001b[0m \u001b[32m -0.222\u001b[0m,  46% removed (alpha=   0.8474, nnz=403/742) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.172\u001b[0m \u001b[32m -0.226\u001b[0m,  47% removed (alpha=   1.0462, nnz=393/742) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.179\u001b[0m \u001b[32m -0.232\u001b[0m,  52% removed (alpha=   1.2915, nnz=359/742) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.184\u001b[0m \u001b[31m -0.236\u001b[0m,  52% removed (alpha=   1.5945, nnz=357/742) \u001b[31mover  \u001b[0m [0.2955 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.178\u001b[0m \u001b[32m -0.231\u001b[0m,  50% removed (alpha=   1.1382, nnz=373/742) \u001b[32mfit ok\u001b[0m [1.0462 1.5945]\n",
      " -0.172  -0.212 -> \u001b[32m -0.179\u001b[0m \u001b[32m -0.231\u001b[0m,  50% removed (alpha=   1.2383, nnz=369/742) \u001b[32mfit ok\u001b[0m [1.0462 1.5945]\n",
      " -0.172  -0.212 -> \u001b[32m -0.180\u001b[0m \u001b[32m -0.232\u001b[0m,  51% removed (alpha=   1.3471, nnz=364/742) \u001b[32mfit ok\u001b[0m [1.0462 1.5945]\n",
      " -0.172  -0.212 -> \u001b[32m -0.181\u001b[0m \u001b[31m -0.233\u001b[0m,  51% removed (alpha=   1.4656, nnz=363/742) \u001b[31mover  \u001b[0m [1.0462 1.5945]\n",
      "METRICS orig   tr -0.172\u001b[0m, va -0.212\u001b[0m \u001b[1m[te -0.186]\u001b[0m,  ntrees 100,  nnodes  9586,  nleafs  4843,  nnz  4837\n",
      "METRICS prev   \u001b[32mtr -0.175\u001b[0m, \u001b[32mva -0.231\u001b[0m \u001b[1m\u001b[32m[te -0.189]\u001b[0m,  ntrees  31,  nnodes  1453,  nleafs   742,  nnz   670\n",
      "METRICS now    \u001b[32mtr -0.179\u001b[0m, \u001b[32mva -0.232\u001b[0m \u001b[1m\u001b[32m[te -0.195]\u001b[0m,  ntrees  31,  nnodes  1139,  nleafs   585,  nnz   432\n",
      "\n",
      "Level 6 xxtrain.shape (11478, 585) dense transform time: 0.00s, 1.52s relerr 0.1\n",
      " -0.172  -0.212 -> \u001b[32m -0.153\u001b[0m \u001b[32m -0.213\u001b[0m,   1% removed (alpha=   0.0026, nnz=577/585) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.153\u001b[0m \u001b[32m -0.213\u001b[0m,   2% removed (alpha=   0.0067, nnz=576/585) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.152\u001b[0m \u001b[32m -0.213\u001b[0m,   2% removed (alpha=   0.0172, nnz=574/585) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.152\u001b[0m \u001b[32m -0.213\u001b[0m,   5% removed (alpha=   0.0444, nnz=555/585) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.152\u001b[0m \u001b[32m -0.213\u001b[0m,  20% removed (alpha=   0.1145, nnz=466/585) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.153\u001b[0m \u001b[32m -0.213\u001b[0m,  24% removed (alpha=   0.2955, nnz=443/585) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.155\u001b[0m \u001b[32m -0.215\u001b[0m,  37% removed (alpha=   0.7627, nnz=370/585) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.168\u001b[0m \u001b[32m -0.225\u001b[0m,  44% removed (alpha=   1.9684, nnz=329/585) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[31m -0.197\u001b[0m \u001b[31m -0.250\u001b[0m,  52% removed (alpha=   5.0802, nnz=278/585) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.269\u001b[0m \u001b[33m -0.312\u001b[0m,  59% removed (alpha=  13.1113, nnz=242/585) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.389\u001b[0m \u001b[33m -0.426\u001b[0m,  69% removed (alpha=  33.8386, nnz=181/585) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.582\u001b[0m \u001b[33m -0.623\u001b[0m,  76% removed (alpha=  87.3326, nnz=142/585) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -1.038\u001b[0m \u001b[33m -1.090\u001b[0m,  83% removed (alpha= 225.3934, nnz= 99/585) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -1.766\u001b[0m \u001b[33m -1.849\u001b[0m,  89% removed (alpha= 581.7091, nnz= 66/585) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -2.986\u001b[0m \u001b[33m -3.147\u001b[0m,  93% removed (alpha=1501.3107, nnz= 40/585) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -4.346\u001b[0m \u001b[33m -4.596\u001b[0m,  97% removed (alpha=3874.6751, nnz= 19/585) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.157\u001b[0m \u001b[32m -0.216\u001b[0m,  37% removed (alpha=   0.9416, nnz=370/585) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.172  -0.212 -> \u001b[32m -0.158\u001b[0m \u001b[32m -0.218\u001b[0m,  37% removed (alpha=   1.1624, nnz=371/585) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.172  -0.212 -> \u001b[32m -0.161\u001b[0m \u001b[32m -0.220\u001b[0m,  37% removed (alpha=   1.4350, nnz=368/585) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.172  -0.212 -> \u001b[32m -0.164\u001b[0m \u001b[32m -0.224\u001b[0m,  38% removed (alpha=   1.7716, nnz=362/585) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.172  -0.212 -> \u001b[32m -0.168\u001b[0m \u001b[32m -0.228\u001b[0m,  41% removed (alpha=   2.1871, nnz=347/585) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.172  -0.212 -> \u001b[32m -0.173\u001b[0m \u001b[32m -0.232\u001b[0m,  43% removed (alpha=   2.7001, nnz=335/585) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.172  -0.212 -> \u001b[32m -0.179\u001b[0m \u001b[31m -0.236\u001b[0m,  45% removed (alpha=   3.3333, nnz=319/585) \u001b[31mover  \u001b[0m [0.7627 5.0802]\n",
      " -0.172  -0.212 -> \u001b[32m -0.187\u001b[0m \u001b[31m -0.242\u001b[0m,  46% removed (alpha=   4.1151, nnz=314/585) \u001b[31mover  \u001b[0m [0.7627 5.0802]\n",
      " -0.172  -0.212 -> \u001b[32m -0.172\u001b[0m \u001b[32m -0.230\u001b[0m,  40% removed (alpha=   2.3794, nnz=350/585) \u001b[32mfit ok\u001b[0m [2.1871 3.3333]\n",
      " -0.172  -0.212 -> \u001b[32m -0.173\u001b[0m \u001b[32m -0.231\u001b[0m,  41% removed (alpha=   2.5886, nnz=346/585) \u001b[32mfit ok\u001b[0m [2.1871 3.3333]\n",
      " -0.172  -0.212 -> \u001b[32m -0.175\u001b[0m \u001b[32m -0.233\u001b[0m,  41% removed (alpha=   2.8163, nnz=343/585) \u001b[32mfit ok\u001b[0m [2.1871 3.3333]\n",
      " -0.172  -0.212 -> \u001b[32m -0.177\u001b[0m \u001b[31m -0.235\u001b[0m,  42% removed (alpha=   3.0639, nnz=339/585) \u001b[31mover  \u001b[0m [2.1871 3.3333]\n",
      "METRICS orig   tr -0.172\u001b[0m, va -0.212\u001b[0m \u001b[1m[te -0.186]\u001b[0m,  ntrees 100,  nnodes  9586,  nleafs  4843,  nnz  4837\n",
      "METRICS prev   \u001b[32mtr -0.179\u001b[0m, \u001b[32mva -0.232\u001b[0m \u001b[1m\u001b[32m[te -0.195]\u001b[0m,  ntrees  31,  nnodes  1139,  nleafs   585,  nnz   432\n",
      "METRICS now    \u001b[32mtr -0.168\u001b[0m, \u001b[32mva -0.225\u001b[0m \u001b[1m\u001b[32m[te -0.183]\u001b[0m,  ntrees  30,  nnodes  1090,  nleafs   560,  nnz   328\n",
      "\n",
      "DONE, depth of tree reached 6, 6\n",
      "\n",
      "\n",
      "ROUND 2\n",
      "Level 0 xxtrain.shape (11478, 30) dense transform time: 0.00s, 1.53s relerr 0.1\n",
      " -0.172  -0.212 -> \u001b[32m -0.166\u001b[0m \u001b[32m -0.224\u001b[0m,   0% removed (alpha=   0.0026, nnz=30/30) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.166\u001b[0m \u001b[32m -0.224\u001b[0m,   0% removed (alpha=   0.0067, nnz=30/30) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.166\u001b[0m \u001b[32m -0.224\u001b[0m,   0% removed (alpha=   0.0172, nnz=30/30) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.166\u001b[0m \u001b[32m -0.224\u001b[0m,   0% removed (alpha=   0.0444, nnz=30/30) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.166\u001b[0m \u001b[32m -0.224\u001b[0m,   0% removed (alpha=   0.1145, nnz=30/30) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.166\u001b[0m \u001b[32m -0.224\u001b[0m,   0% removed (alpha=   0.2955, nnz=30/30) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.166\u001b[0m \u001b[32m -0.224\u001b[0m,   0% removed (alpha=   0.7627, nnz=30/30) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.166\u001b[0m \u001b[32m -0.225\u001b[0m,   0% removed (alpha=   1.9684, nnz=30/30) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.168\u001b[0m \u001b[32m -0.226\u001b[0m,   0% removed (alpha=   5.0802, nnz=30/30) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.175\u001b[0m \u001b[32m -0.232\u001b[0m,   3% removed (alpha=  13.1113, nnz=29/30) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[31m -0.202\u001b[0m \u001b[33m -0.256\u001b[0m,   7% removed (alpha=  33.8386, nnz=28/30) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.304\u001b[0m \u001b[33m -0.347\u001b[0m,  17% removed (alpha=  87.3326, nnz=25/30) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.526\u001b[0m \u001b[33m -0.556\u001b[0m,  40% removed (alpha= 225.3934, nnz=18/30) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.817\u001b[0m \u001b[33m -0.838\u001b[0m,  63% removed (alpha= 581.7091, nnz=11/30) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -1.097\u001b[0m \u001b[33m -1.153\u001b[0m,  73% removed (alpha=1501.3107, nnz= 8/30) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -1.369\u001b[0m \u001b[33m -1.456\u001b[0m,  80% removed (alpha=3874.6751, nnz= 6/30) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.169\u001b[0m \u001b[32m -0.227\u001b[0m,   0% removed (alpha=   6.2717, nnz=30/30) \u001b[32mfit ok\u001b[0m [ 5.0802 33.8386]\n",
      " -0.172  -0.212 -> \u001b[32m -0.170\u001b[0m \u001b[32m -0.228\u001b[0m,   0% removed (alpha=   7.7426, nnz=30/30) \u001b[32mfit ok\u001b[0m [ 5.0802 33.8386]\n",
      " -0.172  -0.212 -> \u001b[32m -0.172\u001b[0m \u001b[32m -0.229\u001b[0m,   0% removed (alpha=   9.5586, nnz=30/30) \u001b[32mfit ok\u001b[0m [ 5.0802 33.8386]\n",
      " -0.172  -0.212 -> \u001b[32m -0.174\u001b[0m \u001b[32m -0.231\u001b[0m,   3% removed (alpha=  11.8004, nnz=29/30) \u001b[32mfit ok\u001b[0m [ 5.0802 33.8386]\n",
      " -0.172  -0.212 -> \u001b[32m -0.177\u001b[0m \u001b[31m -0.234\u001b[0m,   3% removed (alpha=  14.5680, nnz=29/30) \u001b[31mover  \u001b[0m [ 5.0802 33.8386]\n",
      " -0.172  -0.212 -> \u001b[32m -0.181\u001b[0m \u001b[31m -0.237\u001b[0m,   3% removed (alpha=  17.9846, nnz=29/30) \u001b[31mover  \u001b[0m [ 5.0802 33.8386]\n",
      " -0.172  -0.212 -> \u001b[32m -0.186\u001b[0m \u001b[31m -0.242\u001b[0m,   7% removed (alpha=  22.2027, nnz=28/30) \u001b[31mover  \u001b[0m [ 5.0802 33.8386]\n",
      " -0.172  -0.212 -> \u001b[31m -0.193\u001b[0m \u001b[31m -0.247\u001b[0m,   7% removed (alpha=  27.4100, nnz=28/30) \u001b[33munder \u001b[0m [ 5.0802 33.8386]\n",
      " -0.172  -0.212 -> \u001b[32m -0.173\u001b[0m \u001b[32m -0.230\u001b[0m,   0% removed (alpha=  10.3990, nnz=30/30) \u001b[32mfit ok\u001b[0m [ 9.5586 14.568 ]\n",
      " -0.172  -0.212 -> \u001b[32m -0.174\u001b[0m \u001b[32m -0.231\u001b[0m,   3% removed (alpha=  11.3134, nnz=29/30) \u001b[32mfit ok\u001b[0m [ 9.5586 14.568 ]\n",
      " -0.172  -0.212 -> \u001b[32m -0.175\u001b[0m \u001b[32m -0.232\u001b[0m,   3% removed (alpha=  12.3082, nnz=29/30) \u001b[32mfit ok\u001b[0m [ 9.5586 14.568 ]\n",
      " -0.172  -0.212 -> \u001b[32m -0.176\u001b[0m \u001b[32m -0.232\u001b[0m,   3% removed (alpha=  13.3905, nnz=29/30) \u001b[32mfit ok\u001b[0m [ 9.5586 14.568 ]\n",
      "METRICS orig   tr -0.172\u001b[0m, va -0.212\u001b[0m \u001b[1m[te -0.186]\u001b[0m,  ntrees 100,  nnodes  9586,  nleafs  4843,  nnz  4837\n",
      "METRICS prev   \u001b[32mtr -0.168\u001b[0m, \u001b[32mva -0.225\u001b[0m \u001b[1m\u001b[32m[te -0.183]\u001b[0m,  ntrees  30,  nnodes  1090,  nleafs   560,  nnz   328\n",
      "METRICS now    \u001b[32mtr -0.175\u001b[0m, \u001b[32mva -0.232\u001b[0m \u001b[1m\u001b[32m[te -0.190]\u001b[0m,  ntrees  29,  nnodes  1079,  nleafs   554,  nnz   326\n",
      "\n",
      "Level 1 xxtrain.shape (11478, 111) dense transform time: 0.00s, 1.57s relerr 0.1\n",
      " -0.172  -0.212 -> \u001b[31m -0.192\u001b[0m \u001b[31m -0.243\u001b[0m,   5% removed (alpha=   0.0026, nnz=105/111) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[31m -0.192\u001b[0m \u001b[31m -0.243\u001b[0m,   5% removed (alpha=   0.0067, nnz=105/111) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[31m -0.192\u001b[0m \u001b[31m -0.243\u001b[0m,   5% removed (alpha=   0.0172, nnz=106/111) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[31m -0.192\u001b[0m \u001b[31m -0.243\u001b[0m,   5% removed (alpha=   0.0444, nnz=106/111) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[31m -0.191\u001b[0m \u001b[31m -0.243\u001b[0m,   5% removed (alpha=   0.1145, nnz=106/111) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[31m -0.191\u001b[0m \u001b[31m -0.242\u001b[0m,   5% removed (alpha=   0.2955, nnz=106/111) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[31m -0.190\u001b[0m \u001b[31m -0.242\u001b[0m,   7% removed (alpha=   0.7627, nnz=103/111) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.188\u001b[0m \u001b[31m -0.240\u001b[0m,  18% removed (alpha=   1.9684, nnz= 91/111) \u001b[31mover  \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[31m -0.196\u001b[0m \u001b[31m -0.247\u001b[0m,  29% removed (alpha=   5.0802, nnz= 79/111) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.221\u001b[0m \u001b[33m -0.268\u001b[0m,  58% removed (alpha=  13.1113, nnz= 47/111) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.306\u001b[0m \u001b[33m -0.342\u001b[0m,  65% removed (alpha=  33.8386, nnz= 39/111) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.411\u001b[0m \u001b[33m -0.440\u001b[0m,  70% removed (alpha=  87.3326, nnz= 33/111) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.650\u001b[0m \u001b[33m -0.669\u001b[0m,  78% removed (alpha= 225.3934, nnz= 24/111) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -1.028\u001b[0m \u001b[33m -1.055\u001b[0m,  87% removed (alpha= 581.7091, nnz= 14/111) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -1.322\u001b[0m \u001b[33m -1.396\u001b[0m,  91% removed (alpha=1501.3107, nnz= 10/111) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -1.755\u001b[0m \u001b[33m -1.845\u001b[0m,  93% removed (alpha=3874.6751, nnz=  8/111) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.175\u001b[0m \u001b[32m -0.231\u001b[0m,  38% removed (alpha=   2.1871, nnz= 69/111) \u001b[32mfit ok\u001b[0m [1.9684 5.0802]\n",
      " -0.172  -0.212 -> \u001b[32m -0.175\u001b[0m \u001b[32m -0.232\u001b[0m,  23% removed (alpha=   2.4301, nnz= 85/111) \u001b[32mfit ok\u001b[0m [1.9684 5.0802]\n",
      " -0.172  -0.212 -> \u001b[32m -0.176\u001b[0m \u001b[32m -0.232\u001b[0m,  21% removed (alpha=   2.7001, nnz= 88/111) \u001b[32mfit ok\u001b[0m [1.9684 5.0802]\n",
      " -0.172  -0.212 -> \u001b[32m -0.177\u001b[0m \u001b[32m -0.233\u001b[0m,  21% removed (alpha=   3.0000, nnz= 88/111) \u001b[32mfit ok\u001b[0m [1.9684 5.0802]\n",
      " -0.172  -0.212 -> \u001b[32m -0.178\u001b[0m \u001b[31m -0.234\u001b[0m,  18% removed (alpha=   3.3333, nnz= 91/111) \u001b[31mover  \u001b[0m [1.9684 5.0802]\n",
      " -0.172  -0.212 -> \u001b[32m -0.179\u001b[0m \u001b[31m -0.234\u001b[0m,  19% removed (alpha=   3.7036, nnz= 90/111) \u001b[31mover  \u001b[0m [1.9684 5.0802]\n",
      " -0.172  -0.212 -> \u001b[32m -0.180\u001b[0m \u001b[31m -0.235\u001b[0m,  27% removed (alpha=   4.1151, nnz= 81/111) \u001b[31mover  \u001b[0m [1.9684 5.0802]\n",
      " -0.172  -0.212 -> \u001b[32m -0.181\u001b[0m \u001b[31m -0.236\u001b[0m,  27% removed (alpha=   4.5723, nnz= 81/111) \u001b[31mover  \u001b[0m [1.9684 5.0802]\n",
      " -0.172  -0.212 -> \u001b[32m -0.176\u001b[0m \u001b[32m -0.232\u001b[0m,  15% removed (alpha=   2.8163, nnz= 94/111) \u001b[32mfit ok\u001b[0m [2.7001 3.3333]\n",
      " -0.172  -0.212 -> \u001b[32m -0.177\u001b[0m \u001b[32m -0.233\u001b[0m,  11% removed (alpha=   2.9375, nnz= 99/111) \u001b[32mfit ok\u001b[0m [2.7001 3.3333]\n",
      " -0.172  -0.212 -> \u001b[32m -0.177\u001b[0m \u001b[32m -0.233\u001b[0m,  10% removed (alpha=   3.0639, nnz=100/111) \u001b[32mfit ok\u001b[0m [2.7001 3.3333]\n",
      " -0.172  -0.212 -> \u001b[32m -0.177\u001b[0m \u001b[32m -0.233\u001b[0m,  10% removed (alpha=   3.1958, nnz=100/111) \u001b[32mfit ok\u001b[0m [2.7001 3.3333]\n",
      "METRICS orig   tr -0.172\u001b[0m, va -0.212\u001b[0m \u001b[1m[te -0.186]\u001b[0m,  ntrees 100,  nnodes  9586,  nleafs  4843,  nnz  4837\n",
      "METRICS prev   \u001b[32mtr -0.175\u001b[0m, \u001b[32mva -0.232\u001b[0m \u001b[1m\u001b[32m[te -0.190]\u001b[0m,  ntrees  29,  nnodes  1079,  nleafs   554,  nnz   326\n",
      "METRICS now    \u001b[32mtr -0.175\u001b[0m, \u001b[32mva -0.231\u001b[0m \u001b[1m\u001b[32m[te -0.188]\u001b[0m,  ntrees  28,  nnodes  1072,  nleafs   550,  nnz   410\n",
      "\n",
      "Level 2 xxtrain.shape (11478, 190) dense transform time: 0.00s, 1.44s relerr 0.1\n",
      " -0.172  -0.212 -> \u001b[31m -0.197\u001b[0m \u001b[31m -0.242\u001b[0m,   1% removed (alpha=   0.0026, nnz=188/190) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[31m -0.196\u001b[0m \u001b[31m -0.242\u001b[0m,   1% removed (alpha=   0.0067, nnz=188/190) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[31m -0.194\u001b[0m \u001b[31m -0.241\u001b[0m,   1% removed (alpha=   0.0172, nnz=188/190) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[31m -0.192\u001b[0m \u001b[31m -0.240\u001b[0m,   1% removed (alpha=   0.0444, nnz=188/190) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[31m -0.190\u001b[0m \u001b[31m -0.238\u001b[0m,   2% removed (alpha=   0.1145, nnz=187/190) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.189\u001b[0m \u001b[31m -0.237\u001b[0m,   2% removed (alpha=   0.2955, nnz=186/190) \u001b[31mover  \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.188\u001b[0m \u001b[31m -0.237\u001b[0m,  13% removed (alpha=   0.7627, nnz=165/190) \u001b[31mover  \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[31m -0.191\u001b[0m \u001b[31m -0.242\u001b[0m,  17% removed (alpha=   1.9684, nnz=157/190) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[31m -0.196\u001b[0m \u001b[31m -0.250\u001b[0m,  46% removed (alpha=   5.0802, nnz=102/190) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.224\u001b[0m \u001b[33m -0.275\u001b[0m,  54% removed (alpha=  13.1113, nnz= 87/190) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.303\u001b[0m \u001b[33m -0.350\u001b[0m,  63% removed (alpha=  33.8386, nnz= 70/190) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.508\u001b[0m \u001b[33m -0.552\u001b[0m,  68% removed (alpha=  87.3326, nnz= 60/190) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.836\u001b[0m \u001b[33m -0.867\u001b[0m,  80% removed (alpha= 225.3934, nnz= 38/190) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -1.218\u001b[0m \u001b[33m -1.281\u001b[0m,  87% removed (alpha= 581.7091, nnz= 24/190) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -1.878\u001b[0m \u001b[33m -2.011\u001b[0m,  93% removed (alpha=1501.3107, nnz= 13/190) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -3.416\u001b[0m \u001b[33m -3.640\u001b[0m,  96% removed (alpha=3874.6751, nnz=  8/190) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.174\u001b[0m \u001b[32m -0.229\u001b[0m,  28% removed (alpha=   0.8474, nnz=137/190) \u001b[32mfit ok\u001b[0m [0.7627 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.174\u001b[0m \u001b[32m -0.229\u001b[0m,  26% removed (alpha=   0.9416, nnz=140/190) \u001b[32mfit ok\u001b[0m [0.7627 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.174\u001b[0m \u001b[32m -0.229\u001b[0m,  27% removed (alpha=   1.0462, nnz=139/190) \u001b[32mfit ok\u001b[0m [0.7627 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.174\u001b[0m \u001b[32m -0.229\u001b[0m,  26% removed (alpha=   1.1624, nnz=140/190) \u001b[32mfit ok\u001b[0m [0.7627 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.174\u001b[0m \u001b[32m -0.230\u001b[0m,  26% removed (alpha=   1.2915, nnz=141/190) \u001b[32mfit ok\u001b[0m [0.7627 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.174\u001b[0m \u001b[32m -0.231\u001b[0m,  26% removed (alpha=   1.4350, nnz=141/190) \u001b[32mfit ok\u001b[0m [0.7627 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.175\u001b[0m \u001b[32m -0.231\u001b[0m,  23% removed (alpha=   1.5945, nnz=147/190) \u001b[32mfit ok\u001b[0m [0.7627 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.176\u001b[0m \u001b[32m -0.232\u001b[0m,  22% removed (alpha=   1.7716, nnz=148/190) \u001b[32mfit ok\u001b[0m [0.7627 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.175\u001b[0m \u001b[32m -0.231\u001b[0m,  14% removed (alpha=   1.6631, nnz=164/190) \u001b[32mfit ok\u001b[0m [1.5945 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.175\u001b[0m \u001b[32m -0.232\u001b[0m,  13% removed (alpha=   1.7347, nnz=166/190) \u001b[32mfit ok\u001b[0m [1.5945 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.176\u001b[0m \u001b[32m -0.232\u001b[0m,  19% removed (alpha=   1.8093, nnz=153/190) \u001b[32mfit ok\u001b[0m [1.5945 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.176\u001b[0m \u001b[32m -0.232\u001b[0m,  22% removed (alpha=   1.8872, nnz=149/190) \u001b[32mfit ok\u001b[0m [1.5945 1.9684]\n",
      "METRICS orig   tr -0.172\u001b[0m, va -0.212\u001b[0m \u001b[1m[te -0.186]\u001b[0m,  ntrees 100,  nnodes  9586,  nleafs  4843,  nnz  4837\n",
      "METRICS prev   \u001b[32mtr -0.175\u001b[0m, \u001b[32mva -0.231\u001b[0m \u001b[1m\u001b[32m[te -0.188]\u001b[0m,  ntrees  28,  nnodes  1072,  nleafs   550,  nnz   410\n",
      "METRICS now    \u001b[32mtr -0.174\u001b[0m, \u001b[32mva -0.229\u001b[0m \u001b[1m\u001b[32m[te -0.188]\u001b[0m,  ntrees  28,  nnodes  1034,  nleafs   531,  nnz   457\n",
      "\n",
      "Level 3 xxtrain.shape (11478, 297) dense transform time: 0.00s, 1.45s relerr 0.1\n",
      " -0.172  -0.212 -> \u001b[33m -0.209\u001b[0m \u001b[31m -0.236\u001b[0m,   1% removed (alpha=   0.0026, nnz=294/297) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.209\u001b[0m \u001b[31m -0.236\u001b[0m,   1% removed (alpha=   0.0067, nnz=294/297) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.208\u001b[0m \u001b[31m -0.236\u001b[0m,   1% removed (alpha=   0.0172, nnz=294/297) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.208\u001b[0m \u001b[31m -0.236\u001b[0m,   1% removed (alpha=   0.0444, nnz=293/297) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.167\u001b[0m \u001b[32m -0.219\u001b[0m,  21% removed (alpha=   0.1145, nnz=235/297) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.166\u001b[0m \u001b[32m -0.221\u001b[0m,  24% removed (alpha=   0.2955, nnz=227/297) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.169\u001b[0m \u001b[32m -0.224\u001b[0m,  26% removed (alpha=   0.7627, nnz=219/297) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.180\u001b[0m \u001b[31m -0.237\u001b[0m,  34% removed (alpha=   1.9684, nnz=196/297) \u001b[31mover  \u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1722.995298114438, tolerance: 1418.6648350597\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.172  -0.212 -> \u001b[33m -0.224\u001b[0m \u001b[33m -0.278\u001b[0m,  43% removed (alpha=   5.0802, nnz=170/297) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.309\u001b[0m \u001b[33m -0.347\u001b[0m,  65% removed (alpha=  13.1113, nnz=105/297) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.432\u001b[0m \u001b[33m -0.467\u001b[0m,  75% removed (alpha=  33.8386, nnz= 75/297) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.615\u001b[0m \u001b[33m -0.648\u001b[0m,  82% removed (alpha=  87.3326, nnz= 52/297) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.864\u001b[0m \u001b[33m -0.928\u001b[0m,  88% removed (alpha= 225.3934, nnz= 36/297) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -1.317\u001b[0m \u001b[33m -1.484\u001b[0m,  91% removed (alpha= 581.7091, nnz= 28/297) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -1.973\u001b[0m \u001b[33m -2.138\u001b[0m,  93% removed (alpha=1501.3107, nnz= 21/297) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -3.106\u001b[0m \u001b[33m -3.289\u001b[0m,  93% removed (alpha=3874.6751, nnz= 20/297) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.172\u001b[0m \u001b[32m -0.222\u001b[0m,  29% removed (alpha=   0.3648, nnz=212/297) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.172\u001b[0m \u001b[32m -0.222\u001b[0m,  26% removed (alpha=   0.4504, nnz=219/297) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.173\u001b[0m \u001b[32m -0.223\u001b[0m,  25% removed (alpha=   0.5560, nnz=222/297) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.173\u001b[0m \u001b[32m -0.224\u001b[0m,  26% removed (alpha=   0.6864, nnz=219/297) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.174\u001b[0m \u001b[32m -0.226\u001b[0m,  28% removed (alpha=   0.8474, nnz=213/297) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.175\u001b[0m \u001b[32m -0.228\u001b[0m,  29% removed (alpha=   1.0462, nnz=210/297) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.176\u001b[0m \u001b[32m -0.231\u001b[0m,  34% removed (alpha=   1.2915, nnz=197/297) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.178\u001b[0m \u001b[32m -0.233\u001b[0m,  35% removed (alpha=   1.5945, nnz=194/297) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.177\u001b[0m \u001b[32m -0.232\u001b[0m,  34% removed (alpha=   1.4051, nnz=197/297) \u001b[32mfit ok\u001b[0m [1.2915 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.177\u001b[0m \u001b[32m -0.233\u001b[0m,  35% removed (alpha=   1.5287, nnz=194/297) \u001b[32mfit ok\u001b[0m [1.2915 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.178\u001b[0m \u001b[31m -0.233\u001b[0m,  35% removed (alpha=   1.6631, nnz=192/297) \u001b[31mover  \u001b[0m [1.2915 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.179\u001b[0m \u001b[31m -0.234\u001b[0m,  35% removed (alpha=   1.8093, nnz=192/297) \u001b[31mover  \u001b[0m [1.2915 1.9684]\n",
      "METRICS orig   tr -0.172\u001b[0m, va -0.212\u001b[0m \u001b[1m[te -0.186]\u001b[0m,  ntrees 100,  nnodes  9586,  nleafs  4843,  nnz  4837\n",
      "METRICS prev   \u001b[32mtr -0.174\u001b[0m, \u001b[32mva -0.229\u001b[0m \u001b[1m\u001b[32m[te -0.188]\u001b[0m,  ntrees  28,  nnodes  1034,  nleafs   531,  nnz   457\n",
      "METRICS now    \u001b[32mtr -0.178\u001b[0m, \u001b[32mva -0.233\u001b[0m \u001b[1m\u001b[32m[te -0.194]\u001b[0m,  ntrees  28,  nnodes   988,  nleafs   508,  nnz   440\n",
      "\n",
      "Level 4 xxtrain.shape (11478, 410) dense transform time: 0.00s, 1.46s relerr 0.1\n",
      " -0.172  -0.212 -> \u001b[31m -0.205\u001b[0m \u001b[31m -0.252\u001b[0m,   1% removed (alpha=   0.0026, nnz=406/410) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[31m -0.204\u001b[0m \u001b[31m -0.251\u001b[0m,   1% removed (alpha=   0.0067, nnz=405/410) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[31m -0.203\u001b[0m \u001b[31m -0.250\u001b[0m,   1% removed (alpha=   0.0172, nnz=404/410) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[31m -0.200\u001b[0m \u001b[31m -0.247\u001b[0m,   4% removed (alpha=   0.0444, nnz=394/410) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[31m -0.200\u001b[0m \u001b[31m -0.247\u001b[0m,   5% removed (alpha=   0.1145, nnz=390/410) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.187\u001b[0m \u001b[31m -0.239\u001b[0m,  27% removed (alpha=   0.2955, nnz=300/410) \u001b[31mover  \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[31m -0.191\u001b[0m \u001b[31m -0.243\u001b[0m,  30% removed (alpha=   0.7627, nnz=286/410) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[31m -0.202\u001b[0m \u001b[33m -0.255\u001b[0m,  38% removed (alpha=   1.9684, nnz=255/410) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2122.8427242411663, tolerance: 1418.6648350597\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3018.80517285897, tolerance: 1418.6648350597\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.172  -0.212 -> \u001b[33m -0.235\u001b[0m \u001b[33m -0.287\u001b[0m,  56% removed (alpha=   5.0802, nnz=180/410) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.326\u001b[0m \u001b[33m -0.370\u001b[0m,  63% removed (alpha=  13.1113, nnz=152/410) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.455\u001b[0m \u001b[33m -0.496\u001b[0m,  74% removed (alpha=  33.8386, nnz=105/410) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.679\u001b[0m \u001b[33m -0.709\u001b[0m,  79% removed (alpha=  87.3326, nnz= 88/410) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.956\u001b[0m \u001b[33m -0.989\u001b[0m,  86% removed (alpha= 225.3934, nnz= 59/410) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -1.480\u001b[0m \u001b[33m -1.522\u001b[0m,  90% removed (alpha= 581.7091, nnz= 40/410) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -2.396\u001b[0m \u001b[33m -2.444\u001b[0m,  95% removed (alpha=1501.3107, nnz= 22/410) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -3.830\u001b[0m \u001b[33m -3.961\u001b[0m,  96% removed (alpha=3874.6751, nnz= 16/410) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[31m -0.196\u001b[0m \u001b[31m -0.247\u001b[0m,  24% removed (alpha=   0.3284, nnz=311/410) \u001b[33munder \u001b[0m [0.2955 0.7627]\n",
      " -0.172  -0.212 -> \u001b[31m -0.196\u001b[0m \u001b[31m -0.247\u001b[0m,  24% removed (alpha=   0.3648, nnz=311/410) \u001b[33munder \u001b[0m [0.2955 0.7627]\n",
      " -0.172  -0.212 -> \u001b[31m -0.195\u001b[0m \u001b[31m -0.247\u001b[0m,  25% removed (alpha=   0.4054, nnz=308/410) \u001b[33munder \u001b[0m [0.2955 0.7627]\n",
      " -0.172  -0.212 -> \u001b[31m -0.195\u001b[0m \u001b[31m -0.246\u001b[0m,  25% removed (alpha=   0.4504, nnz=309/410) \u001b[33munder \u001b[0m [0.2955 0.7627]\n",
      " -0.172  -0.212 -> \u001b[31m -0.195\u001b[0m \u001b[31m -0.246\u001b[0m,  25% removed (alpha=   0.5004, nnz=306/410) \u001b[33munder \u001b[0m [0.2955 0.7627]\n",
      " -0.172  -0.212 -> \u001b[31m -0.195\u001b[0m \u001b[31m -0.247\u001b[0m,  27% removed (alpha=   0.5560, nnz=300/410) \u001b[33munder \u001b[0m [0.2955 0.7627]\n",
      " -0.172  -0.212 -> \u001b[31m -0.196\u001b[0m \u001b[31m -0.247\u001b[0m,  28% removed (alpha=   0.6178, nnz=295/410) \u001b[33munder \u001b[0m [0.2955 0.7627]\n",
      " -0.172  -0.212 -> \u001b[31m -0.196\u001b[0m \u001b[31m -0.247\u001b[0m,  29% removed (alpha=   0.6864, nnz=290/410) \u001b[33munder \u001b[0m [0.2955 0.7627]\n",
      "METRICS orig   tr -0.172\u001b[0m, va -0.212\u001b[0m \u001b[1m[te -0.186]\u001b[0m,  ntrees 100,  nnodes  9586,  nleafs  4843,  nnz  4837\n",
      "METRICS prev   \u001b[32mtr -0.178\u001b[0m, \u001b[32mva -0.233\u001b[0m \u001b[1m\u001b[32m[te -0.194]\u001b[0m,  ntrees  28,  nnodes   988,  nleafs   508,  nnz   440\n",
      "METRICS now    \u001b[32mtr -0.178\u001b[0m, \u001b[32mva -0.233\u001b[0m \u001b[1m\u001b[32m[te -0.194]\u001b[0m,  ntrees  28,  nnodes   988,  nleafs   508,  nnz   440\n",
      "\n",
      "Level 5 xxtrain.shape (11478, 508) dense transform time: 0.00s, 1.46s relerr 0.1\n",
      " -0.172  -0.212 -> \u001b[32m -0.173\u001b[0m \u001b[32m -0.232\u001b[0m,   1% removed (alpha=   0.0026, nnz=504/508) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.173\u001b[0m \u001b[32m -0.232\u001b[0m,   1% removed (alpha=   0.0067, nnz=504/508) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.173\u001b[0m \u001b[32m -0.232\u001b[0m,   1% removed (alpha=   0.0172, nnz=504/508) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.170\u001b[0m \u001b[32m -0.230\u001b[0m,   6% removed (alpha=   0.0444, nnz=479/508) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.170\u001b[0m \u001b[32m -0.230\u001b[0m,   6% removed (alpha=   0.1145, nnz=476/508) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.171\u001b[0m \u001b[32m -0.229\u001b[0m,  20% removed (alpha=   0.2955, nnz=405/508) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.177\u001b[0m \u001b[32m -0.233\u001b[0m,  37% removed (alpha=   0.7627, nnz=319/508) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[31m -0.193\u001b[0m \u001b[31m -0.244\u001b[0m,  44% removed (alpha=   1.9684, nnz=282/508) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1692.6599274567031, tolerance: 1418.6648350597\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.172  -0.212 -> \u001b[33m -0.230\u001b[0m \u001b[33m -0.277\u001b[0m,  54% removed (alpha=   5.0802, nnz=235/508) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.310\u001b[0m \u001b[33m -0.349\u001b[0m,  62% removed (alpha=  13.1113, nnz=194/508) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.432\u001b[0m \u001b[33m -0.461\u001b[0m,  75% removed (alpha=  33.8386, nnz=127/508) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.640\u001b[0m \u001b[33m -0.686\u001b[0m,  81% removed (alpha=  87.3326, nnz= 97/508) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.983\u001b[0m \u001b[33m -1.088\u001b[0m,  89% removed (alpha= 225.3934, nnz= 57/508) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -1.388\u001b[0m \u001b[33m -1.477\u001b[0m,  92% removed (alpha= 581.7091, nnz= 40/508) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -2.531\u001b[0m \u001b[33m -2.623\u001b[0m,  95% removed (alpha=1501.3107, nnz= 27/508) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -3.675\u001b[0m \u001b[33m -3.773\u001b[0m,  96% removed (alpha=3874.6751, nnz= 22/508) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.176\u001b[0m \u001b[32m -0.232\u001b[0m,  18% removed (alpha=   0.3648, nnz=419/508) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.176\u001b[0m \u001b[32m -0.232\u001b[0m,  21% removed (alpha=   0.4504, nnz=402/508) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.173\u001b[0m \u001b[32m -0.230\u001b[0m,  27% removed (alpha=   0.5560, nnz=370/508) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.173\u001b[0m \u001b[32m -0.230\u001b[0m,  32% removed (alpha=   0.6864, nnz=345/508) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.174\u001b[0m \u001b[32m -0.231\u001b[0m,  31% removed (alpha=   0.8474, nnz=353/508) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.175\u001b[0m \u001b[32m -0.232\u001b[0m,  31% removed (alpha=   1.0462, nnz=350/508) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.177\u001b[0m \u001b[31m -0.234\u001b[0m,  32% removed (alpha=   1.2915, nnz=346/508) \u001b[31mover  \u001b[0m [0.2955 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.180\u001b[0m \u001b[31m -0.236\u001b[0m,  32% removed (alpha=   1.5945, nnz=343/508) \u001b[31mover  \u001b[0m [0.2955 1.9684]\n",
      " -0.172  -0.212 -> \u001b[32m -0.175\u001b[0m \u001b[32m -0.232\u001b[0m,  29% removed (alpha=   0.9219, nnz=363/508) \u001b[32mfit ok\u001b[0m [0.8474 1.2915]\n",
      " -0.172  -0.212 -> \u001b[32m -0.176\u001b[0m \u001b[32m -0.232\u001b[0m,  29% removed (alpha=   1.0030, nnz=361/508) \u001b[32mfit ok\u001b[0m [0.8474 1.2915]\n",
      " -0.172  -0.212 -> \u001b[32m -0.176\u001b[0m \u001b[32m -0.232\u001b[0m,  30% removed (alpha=   1.0912, nnz=356/508) \u001b[32mfit ok\u001b[0m [0.8474 1.2915]\n",
      " -0.172  -0.212 -> \u001b[32m -0.177\u001b[0m \u001b[32m -0.233\u001b[0m,  31% removed (alpha=   1.1872, nnz=349/508) \u001b[32mfit ok\u001b[0m [0.8474 1.2915]\n",
      "METRICS orig   tr -0.172\u001b[0m, va -0.212\u001b[0m \u001b[1m[te -0.186]\u001b[0m,  ntrees 100,  nnodes  9586,  nleafs  4843,  nnz  4837\n",
      "METRICS prev   \u001b[32mtr -0.178\u001b[0m, \u001b[32mva -0.233\u001b[0m \u001b[1m\u001b[32m[te -0.194]\u001b[0m,  ntrees  28,  nnodes   988,  nleafs   508,  nnz   440\n",
      "METRICS now    \u001b[32mtr -0.177\u001b[0m, \u001b[32mva -0.233\u001b[0m \u001b[1m\u001b[32m[te -0.192]\u001b[0m,  ntrees  28,  nnodes   952,  nleafs   490,  nnz   356\n",
      "\n",
      "Level 6 xxtrain.shape (11478, 490) dense transform time: 0.00s, 1.41s relerr 0.1\n",
      " -0.172  -0.212 -> \u001b[32m -0.166\u001b[0m \u001b[32m -0.225\u001b[0m,   1% removed (alpha=   0.0026, nnz=486/490) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.166\u001b[0m \u001b[32m -0.225\u001b[0m,   1% removed (alpha=   0.0067, nnz=486/490) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.165\u001b[0m \u001b[32m -0.225\u001b[0m,   1% removed (alpha=   0.0172, nnz=483/490) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.165\u001b[0m \u001b[32m -0.225\u001b[0m,   3% removed (alpha=   0.0444, nnz=477/490) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.165\u001b[0m \u001b[32m -0.225\u001b[0m,  12% removed (alpha=   0.1145, nnz=432/490) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.165\u001b[0m \u001b[32m -0.225\u001b[0m,  16% removed (alpha=   0.2955, nnz=414/490) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.166\u001b[0m \u001b[32m -0.225\u001b[0m,  33% removed (alpha=   0.7627, nnz=326/490) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.173\u001b[0m \u001b[32m -0.230\u001b[0m,  38% removed (alpha=   1.9684, nnz=304/490) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[31m -0.200\u001b[0m \u001b[31m -0.251\u001b[0m,  47% removed (alpha=   5.0802, nnz=258/490) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.272\u001b[0m \u001b[33m -0.313\u001b[0m,  55% removed (alpha=  13.1113, nnz=219/490) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.403\u001b[0m \u001b[33m -0.431\u001b[0m,  67% removed (alpha=  33.8386, nnz=162/490) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -0.601\u001b[0m \u001b[33m -0.625\u001b[0m,  77% removed (alpha=  87.3326, nnz=113/490) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -1.009\u001b[0m \u001b[33m -1.055\u001b[0m,  84% removed (alpha= 225.3934, nnz= 79/490) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -1.793\u001b[0m \u001b[33m -1.877\u001b[0m,  90% removed (alpha= 581.7091, nnz= 47/490) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -2.886\u001b[0m \u001b[33m -3.049\u001b[0m,  92% removed (alpha=1501.3107, nnz= 38/490) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[33m -4.660\u001b[0m \u001b[33m -5.056\u001b[0m,  96% removed (alpha=3874.6751, nnz= 21/490) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.172  -0.212 -> \u001b[32m -0.167\u001b[0m \u001b[32m -0.226\u001b[0m,  31% removed (alpha=   0.9416, nnz=336/490) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.172  -0.212 -> \u001b[32m -0.167\u001b[0m \u001b[32m -0.226\u001b[0m,  32% removed (alpha=   1.1624, nnz=334/490) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.172  -0.212 -> \u001b[32m -0.169\u001b[0m \u001b[32m -0.227\u001b[0m,  36% removed (alpha=   1.4350, nnz=316/490) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.172  -0.212 -> \u001b[32m -0.171\u001b[0m \u001b[32m -0.229\u001b[0m,  35% removed (alpha=   1.7716, nnz=319/490) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.172  -0.212 -> \u001b[32m -0.174\u001b[0m \u001b[32m -0.231\u001b[0m,  37% removed (alpha=   2.1871, nnz=308/490) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.172  -0.212 -> \u001b[32m -0.177\u001b[0m \u001b[31m -0.234\u001b[0m,  38% removed (alpha=   2.7001, nnz=303/490) \u001b[31mover  \u001b[0m [0.7627 5.0802]\n",
      " -0.172  -0.212 -> \u001b[32m -0.183\u001b[0m \u001b[31m -0.238\u001b[0m,  40% removed (alpha=   3.3333, nnz=294/490) \u001b[31mover  \u001b[0m [0.7627 5.0802]\n",
      " -0.172  -0.212 -> \u001b[31m -0.190\u001b[0m \u001b[31m -0.244\u001b[0m,  44% removed (alpha=   4.1151, nnz=276/490) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.172  -0.212 -> \u001b[32m -0.173\u001b[0m \u001b[32m -0.231\u001b[0m,  35% removed (alpha=   1.9274, nnz=320/490) \u001b[32mfit ok\u001b[0m [1.7716 2.7001]\n",
      " -0.172  -0.212 -> \u001b[32m -0.174\u001b[0m \u001b[32m -0.232\u001b[0m,  35% removed (alpha=   2.0969, nnz=318/490) \u001b[32mfit ok\u001b[0m [1.7716 2.7001]\n",
      " -0.172  -0.212 -> \u001b[32m -0.175\u001b[0m \u001b[32m -0.233\u001b[0m,  37% removed (alpha=   2.2812, nnz=310/490) \u001b[32mfit ok\u001b[0m [1.7716 2.7001]\n",
      " -0.172  -0.212 -> \u001b[32m -0.176\u001b[0m \u001b[31m -0.234\u001b[0m,  38% removed (alpha=   2.4818, nnz=305/490) \u001b[31mover  \u001b[0m [1.7716 2.7001]\n",
      "METRICS orig   tr -0.172\u001b[0m, va -0.212\u001b[0m \u001b[1m[te -0.186]\u001b[0m,  ntrees 100,  nnodes  9586,  nleafs  4843,  nnz  4837\n",
      "METRICS prev   \u001b[32mtr -0.177\u001b[0m, \u001b[32mva -0.233\u001b[0m \u001b[1m\u001b[32m[te -0.192]\u001b[0m,  ntrees  28,  nnodes   952,  nleafs   490,  nnz   356\n",
      "METRICS now    \u001b[32mtr -0.173\u001b[0m, \u001b[32mva -0.230\u001b[0m \u001b[1m\u001b[32m[te -0.188]\u001b[0m,  ntrees  28,  nnodes   950,  nleafs   489,  nnz   304\n",
      "\n",
      "DONE, depth of tree reached 6, 6\n",
      "ChaahatRegTarget1 number of non-zero leaves: 4837 -> 304, test accuracy: -18.6% -> -18.8%\n",
      "\n",
      "| XGBOOST's base_score\n",
      "|   base_score diff std      3.7398039406566386e-05 (!) NOT OK\n",
      "|   base_score reported      199.13467\n",
      "|   versus manually detected 199.13464810167508\n",
      "|   abs err                  2.1898324916946876e-05\n",
      "|   rel err                  1.0996741510128234e-07\n",
      "|   base_score OK with relative tolerance 0.001\n",
      "\n",
      "MODEL PERF: mtr -0.130 mte -0.139 mva -0.149\n",
      "\n",
      "\n",
      "ROUND 1\n",
      "Level 0 xxtrain.shape (11478, 100) dense transform time: 0.02s, 4.84s relerr 0.1\n",
      " -0.130  -0.149 -> \u001b[32m -0.089\u001b[0m \u001b[32m -0.115\u001b[0m,   0% removed (alpha=   0.0026, nnz=100/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.089\u001b[0m \u001b[32m -0.115\u001b[0m,   1% removed (alpha=   0.0067, nnz= 99/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.089\u001b[0m \u001b[32m -0.115\u001b[0m,   3% removed (alpha=   0.0172, nnz= 97/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.089\u001b[0m \u001b[32m -0.115\u001b[0m,   6% removed (alpha=   0.0444, nnz= 94/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.091\u001b[0m \u001b[32m -0.117\u001b[0m,  17% removed (alpha=   0.1145, nnz= 83/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.095\u001b[0m \u001b[32m -0.121\u001b[0m,  29% removed (alpha=   0.2955, nnz= 71/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.105\u001b[0m \u001b[32m -0.130\u001b[0m,  43% removed (alpha=   0.7627, nnz= 57/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.125\u001b[0m \u001b[32m -0.147\u001b[0m,  62% removed (alpha=   1.9684, nnz= 38/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[31m -0.150\u001b[0m \u001b[31m -0.172\u001b[0m,  78% removed (alpha=   5.0802, nnz= 22/100) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.182\u001b[0m \u001b[33m -0.208\u001b[0m,  84% removed (alpha=  13.1113, nnz= 16/100) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.220\u001b[0m \u001b[33m -0.249\u001b[0m,  89% removed (alpha=  33.8386, nnz= 11/100) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.275\u001b[0m \u001b[33m -0.301\u001b[0m,  91% removed (alpha=  87.3326, nnz=  9/100) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.389\u001b[0m \u001b[33m -0.407\u001b[0m,  94% removed (alpha= 225.3934, nnz=  6/100) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.540\u001b[0m \u001b[33m -0.546\u001b[0m,  96% removed (alpha= 581.7091, nnz=  4/100) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.718\u001b[0m \u001b[33m -0.721\u001b[0m,  98% removed (alpha=1501.3107, nnz=  2/100) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.778\u001b[0m \u001b[33m -0.782\u001b[0m,  99% removed (alpha=3874.6751, nnz=  1/100) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.109\u001b[0m \u001b[32m -0.134\u001b[0m,  52% removed (alpha=   0.9416, nnz= 48/100) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.130  -0.149 -> \u001b[32m -0.113\u001b[0m \u001b[32m -0.137\u001b[0m,  54% removed (alpha=   1.1624, nnz= 46/100) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.130  -0.149 -> \u001b[32m -0.117\u001b[0m \u001b[32m -0.140\u001b[0m,  57% removed (alpha=   1.4350, nnz= 43/100) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.130  -0.149 -> \u001b[32m -0.122\u001b[0m \u001b[32m -0.145\u001b[0m,  59% removed (alpha=   1.7716, nnz= 41/100) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.130  -0.149 -> \u001b[32m -0.128\u001b[0m \u001b[32m -0.150\u001b[0m,  64% removed (alpha=   2.1871, nnz= 36/100) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.130  -0.149 -> \u001b[32m -0.134\u001b[0m \u001b[32m -0.156\u001b[0m,  70% removed (alpha=   2.7001, nnz= 30/100) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.130  -0.149 -> \u001b[32m -0.140\u001b[0m \u001b[32m -0.162\u001b[0m,  73% removed (alpha=   3.3333, nnz= 27/100) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.130  -0.149 -> \u001b[31m -0.145\u001b[0m \u001b[31m -0.167\u001b[0m,  77% removed (alpha=   4.1151, nnz= 23/100) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.130  -0.149 -> \u001b[32m -0.137\u001b[0m \u001b[32m -0.158\u001b[0m,  70% removed (alpha=   2.9375, nnz= 30/100) \u001b[32mfit ok\u001b[0m [2.7001 4.1151]\n",
      " -0.130  -0.149 -> \u001b[32m -0.139\u001b[0m \u001b[32m -0.161\u001b[0m,  72% removed (alpha=   3.1958, nnz= 28/100) \u001b[32mfit ok\u001b[0m [2.7001 4.1151]\n",
      " -0.130  -0.149 -> \u001b[32m -0.141\u001b[0m \u001b[32m -0.163\u001b[0m,  73% removed (alpha=   3.4768, nnz= 27/100) \u001b[32mfit ok\u001b[0m [2.7001 4.1151]\n",
      " -0.130  -0.149 -> \u001b[31m -0.144\u001b[0m \u001b[31m -0.165\u001b[0m,  75% removed (alpha=   3.7825, nnz= 25/100) \u001b[33munder \u001b[0m [2.7001 4.1151]\n",
      "METRICS orig   tr -0.130\u001b[0m, va -0.149\u001b[0m \u001b[1m[te -0.139]\u001b[0m,  ntrees 100,  nnodes 10400,  nleafs  5250,  nnz  5242\n",
      "METRICS prev   \u001b[32mtr -0.130\u001b[0m, \u001b[32mva -0.149\u001b[0m \u001b[1m\u001b[32m[te -0.139]\u001b[0m,  ntrees 100,  nnodes 10400,  nleafs  5250,  nnz  5242\n",
      "METRICS now    \u001b[32mtr -0.140\u001b[0m, \u001b[32mva -0.162\u001b[0m \u001b[1m\u001b[31m[te -0.154]\u001b[0m,  ntrees  27,  nnodes  2655,  nleafs  1341,  nnz  1340\n",
      "\n",
      "Level 1 xxtrain.shape (11478, 107) dense transform time: 0.01s, 1.46s relerr 0.1\n",
      " -0.130  -0.149 -> \u001b[32m -0.124\u001b[0m \u001b[32m -0.147\u001b[0m,   3% removed (alpha=   0.0026, nnz=104/107) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.123\u001b[0m \u001b[32m -0.147\u001b[0m,   3% removed (alpha=   0.0067, nnz=104/107) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.123\u001b[0m \u001b[32m -0.147\u001b[0m,   3% removed (alpha=   0.0172, nnz=104/107) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.124\u001b[0m \u001b[32m -0.147\u001b[0m,   2% removed (alpha=   0.0444, nnz=105/107) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.124\u001b[0m \u001b[32m -0.147\u001b[0m,   4% removed (alpha=   0.1145, nnz=103/107) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.126\u001b[0m \u001b[32m -0.150\u001b[0m,   6% removed (alpha=   0.2955, nnz=101/107) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.133\u001b[0m \u001b[32m -0.156\u001b[0m,  15% removed (alpha=   0.7627, nnz= 91/107) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.140\u001b[0m \u001b[32m -0.163\u001b[0m,  17% removed (alpha=   1.9684, nnz= 89/107) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.166\u001b[0m \u001b[33m -0.188\u001b[0m,  32% removed (alpha=   5.0802, nnz= 73/107) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.196\u001b[0m \u001b[33m -0.220\u001b[0m,  59% removed (alpha=  13.1113, nnz= 44/107) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.250\u001b[0m \u001b[33m -0.277\u001b[0m,  64% removed (alpha=  33.8386, nnz= 39/107) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.336\u001b[0m \u001b[33m -0.356\u001b[0m,  71% removed (alpha=  87.3326, nnz= 31/107) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.491\u001b[0m \u001b[33m -0.497\u001b[0m,  82% removed (alpha= 225.3934, nnz= 19/107) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.630\u001b[0m \u001b[33m -0.633\u001b[0m,  93% removed (alpha= 581.7091, nnz=  8/107) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.778\u001b[0m \u001b[33m -0.782\u001b[0m,  98% removed (alpha=1501.3107, nnz=  2/107) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.782\u001b[0m \u001b[33m -0.785\u001b[0m,  98% removed (alpha=3874.6751, nnz=  2/107) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.131\u001b[0m \u001b[32m -0.154\u001b[0m,  23% removed (alpha=   0.9416, nnz= 82/107) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.130  -0.149 -> \u001b[32m -0.132\u001b[0m \u001b[32m -0.154\u001b[0m,  21% removed (alpha=   1.1624, nnz= 85/107) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.130  -0.149 -> \u001b[32m -0.134\u001b[0m \u001b[32m -0.157\u001b[0m,  33% removed (alpha=   1.4350, nnz= 72/107) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.130  -0.149 -> \u001b[32m -0.136\u001b[0m \u001b[32m -0.159\u001b[0m,  36% removed (alpha=   1.7716, nnz= 68/107) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.130  -0.149 -> \u001b[32m -0.139\u001b[0m \u001b[32m -0.163\u001b[0m,  36% removed (alpha=   2.1871, nnz= 69/107) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.130  -0.149 -> \u001b[31m -0.144\u001b[0m \u001b[31m -0.167\u001b[0m,  36% removed (alpha=   2.7001, nnz= 69/107) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.130  -0.149 -> \u001b[31m -0.149\u001b[0m \u001b[31m -0.172\u001b[0m,  42% removed (alpha=   3.3333, nnz= 62/107) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.130  -0.149 -> \u001b[31m -0.155\u001b[0m \u001b[31m -0.178\u001b[0m,  44% removed (alpha=   4.1151, nnz= 60/107) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.130  -0.149 -> \u001b[32m -0.138\u001b[0m \u001b[32m -0.161\u001b[0m,  34% removed (alpha=   1.9274, nnz= 71/107) \u001b[32mfit ok\u001b[0m [1.7716 2.7001]\n",
      " -0.130  -0.149 -> \u001b[32m -0.139\u001b[0m \u001b[32m -0.162\u001b[0m,  35% removed (alpha=   2.0969, nnz= 70/107) \u001b[32mfit ok\u001b[0m [1.7716 2.7001]\n",
      " -0.130  -0.149 -> \u001b[32m -0.140\u001b[0m \u001b[32m -0.164\u001b[0m,  35% removed (alpha=   2.2812, nnz= 70/107) \u001b[32mfit ok\u001b[0m [1.7716 2.7001]\n",
      " -0.130  -0.149 -> \u001b[32m -0.142\u001b[0m \u001b[31m -0.165\u001b[0m,  35% removed (alpha=   2.4818, nnz= 70/107) \u001b[31mover  \u001b[0m [1.7716 2.7001]\n",
      "METRICS orig   tr -0.130\u001b[0m, va -0.149\u001b[0m \u001b[1m[te -0.139]\u001b[0m,  ntrees 100,  nnodes 10400,  nleafs  5250,  nnz  5242\n",
      "METRICS prev   \u001b[32mtr -0.140\u001b[0m, \u001b[32mva -0.162\u001b[0m \u001b[1m\u001b[31m[te -0.154]\u001b[0m,  ntrees  27,  nnodes  2655,  nleafs  1341,  nnz  1340\n",
      "METRICS now    \u001b[32mtr -0.136\u001b[0m, \u001b[32mva -0.159\u001b[0m \u001b[1m\u001b[32m[te -0.151]\u001b[0m,  ntrees  26,  nnodes  2174,  nleafs  1100,  nnz  1095\n",
      "\n",
      "Level 2 xxtrain.shape (11478, 178) dense transform time: 0.01s, 1.39s relerr 0.1\n",
      " -0.130  -0.149 -> \u001b[32m -0.134\u001b[0m \u001b[32m -0.153\u001b[0m,   2% removed (alpha=   0.0026, nnz=175/178) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.134\u001b[0m \u001b[32m -0.153\u001b[0m,   2% removed (alpha=   0.0067, nnz=175/178) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.133\u001b[0m \u001b[32m -0.153\u001b[0m,   2% removed (alpha=   0.0172, nnz=174/178) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.133\u001b[0m \u001b[32m -0.153\u001b[0m,   2% removed (alpha=   0.0444, nnz=174/178) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.133\u001b[0m \u001b[32m -0.153\u001b[0m,   3% removed (alpha=   0.1145, nnz=172/178) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.134\u001b[0m \u001b[32m -0.155\u001b[0m,   7% removed (alpha=   0.2955, nnz=165/178) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.140\u001b[0m \u001b[32m -0.162\u001b[0m,  17% removed (alpha=   0.7627, nnz=148/178) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[31m -0.155\u001b[0m \u001b[31m -0.177\u001b[0m,  28% removed (alpha=   1.9684, nnz=128/178) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.186\u001b[0m \u001b[33m -0.206\u001b[0m,  48% removed (alpha=   5.0802, nnz= 93/178) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.216\u001b[0m \u001b[33m -0.237\u001b[0m,  63% removed (alpha=  13.1113, nnz= 66/178) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.295\u001b[0m \u001b[33m -0.321\u001b[0m,  75% removed (alpha=  33.8386, nnz= 45/178) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.395\u001b[0m \u001b[33m -0.416\u001b[0m,  83% removed (alpha=  87.3326, nnz= 31/178) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.542\u001b[0m \u001b[33m -0.551\u001b[0m,  87% removed (alpha= 225.3934, nnz= 24/178) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.689\u001b[0m \u001b[33m -0.696\u001b[0m,  94% removed (alpha= 581.7091, nnz= 10/178) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 487.77802510163565, tolerance: 481.2383887192011\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.130  -0.149 -> \u001b[33m -0.789\u001b[0m \u001b[33m -0.796\u001b[0m,  98% removed (alpha=1501.3107, nnz=  4/178) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.853\u001b[0m \u001b[33m -0.865\u001b[0m,  98% removed (alpha=3874.6751, nnz=  4/178) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.130\u001b[0m \u001b[32m -0.152\u001b[0m,  17% removed (alpha=   0.3648, nnz=148/178) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.130\u001b[0m \u001b[32m -0.153\u001b[0m,  15% removed (alpha=   0.4504, nnz=151/178) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.132\u001b[0m \u001b[32m -0.154\u001b[0m,  16% removed (alpha=   0.5560, nnz=149/178) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.134\u001b[0m \u001b[32m -0.156\u001b[0m,  17% removed (alpha=   0.6864, nnz=147/178) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.136\u001b[0m \u001b[32m -0.159\u001b[0m,  19% removed (alpha=   0.8474, nnz=144/178) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.138\u001b[0m \u001b[32m -0.160\u001b[0m,  28% removed (alpha=   1.0462, nnz=129/178) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.142\u001b[0m \u001b[31m -0.165\u001b[0m,  33% removed (alpha=   1.2915, nnz=119/178) \u001b[31mover  \u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[31m -0.146\u001b[0m \u001b[31m -0.170\u001b[0m,  38% removed (alpha=   1.5945, nnz=110/178) \u001b[33munder \u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.136\u001b[0m \u001b[32m -0.159\u001b[0m,  31% removed (alpha=   0.9219, nnz=122/178) \u001b[32mfit ok\u001b[0m [0.8474 1.2915]\n",
      " -0.130  -0.149 -> \u001b[32m -0.138\u001b[0m \u001b[32m -0.160\u001b[0m,  29% removed (alpha=   1.0030, nnz=126/178) \u001b[32mfit ok\u001b[0m [0.8474 1.2915]\n",
      " -0.130  -0.149 -> \u001b[32m -0.139\u001b[0m \u001b[32m -0.161\u001b[0m,  31% removed (alpha=   1.0912, nnz=123/178) \u001b[32mfit ok\u001b[0m [0.8474 1.2915]\n",
      " -0.130  -0.149 -> \u001b[32m -0.140\u001b[0m \u001b[32m -0.163\u001b[0m,  31% removed (alpha=   1.1872, nnz=122/178) \u001b[32mfit ok\u001b[0m [0.8474 1.2915]\n",
      "METRICS orig   tr -0.130\u001b[0m, va -0.149\u001b[0m \u001b[1m[te -0.139]\u001b[0m,  ntrees 100,  nnodes 10400,  nleafs  5250,  nnz  5242\n",
      "METRICS prev   \u001b[32mtr -0.136\u001b[0m, \u001b[32mva -0.159\u001b[0m \u001b[1m\u001b[32m[te -0.151]\u001b[0m,  ntrees  26,  nnodes  2174,  nleafs  1100,  nnz  1095\n",
      "METRICS now    \u001b[32mtr -0.136\u001b[0m, \u001b[32mva -0.159\u001b[0m \u001b[1m\u001b[32m[te -0.149]\u001b[0m,  ntrees  24,  nnodes  1918,  nleafs   971,  nnz   967\n",
      "\n",
      "Level 3 xxtrain.shape (11478, 299) dense transform time: 0.01s, 1.25s relerr 0.1\n",
      " -0.130  -0.149 -> \u001b[32m -0.121\u001b[0m \u001b[32m -0.140\u001b[0m,   1% removed (alpha=   0.0026, nnz=297/299) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.121\u001b[0m \u001b[32m -0.140\u001b[0m,   1% removed (alpha=   0.0067, nnz=295/299) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.121\u001b[0m \u001b[32m -0.141\u001b[0m,   4% removed (alpha=   0.0172, nnz=287/299) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.122\u001b[0m \u001b[32m -0.142\u001b[0m,   5% removed (alpha=   0.0444, nnz=283/299) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.123\u001b[0m \u001b[32m -0.144\u001b[0m,   7% removed (alpha=   0.1145, nnz=278/299) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.127\u001b[0m \u001b[32m -0.149\u001b[0m,  15% removed (alpha=   0.2955, nnz=255/299) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.139\u001b[0m \u001b[32m -0.162\u001b[0m,  26% removed (alpha=   0.7627, nnz=221/299) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.161\u001b[0m \u001b[33m -0.186\u001b[0m,  37% removed (alpha=   1.9684, nnz=189/299) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 721.6436235427697, tolerance: 481.2383887192011\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.130  -0.149 -> \u001b[33m -0.208\u001b[0m \u001b[33m -0.231\u001b[0m,  53% removed (alpha=   5.0802, nnz=141/299) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.283\u001b[0m \u001b[33m -0.302\u001b[0m,  65% removed (alpha=  13.1113, nnz=106/299) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.399\u001b[0m \u001b[33m -0.403\u001b[0m,  74% removed (alpha=  33.8386, nnz= 79/299) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.511\u001b[0m \u001b[33m -0.506\u001b[0m,  86% removed (alpha=  87.3326, nnz= 42/299) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.619\u001b[0m \u001b[33m -0.614\u001b[0m,  91% removed (alpha= 225.3934, nnz= 27/299) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.816\u001b[0m \u001b[33m -0.823\u001b[0m,  95% removed (alpha= 581.7091, nnz= 15/299) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -1.266\u001b[0m \u001b[33m -1.290\u001b[0m,  97% removed (alpha=1501.3107, nnz= 10/299) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -1.921\u001b[0m \u001b[33m -1.944\u001b[0m,  97% removed (alpha=3874.6751, nnz=  8/299) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.135\u001b[0m \u001b[32m -0.158\u001b[0m,  21% removed (alpha=   0.3648, nnz=235/299) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.131\u001b[0m \u001b[32m -0.154\u001b[0m,  24% removed (alpha=   0.4504, nnz=226/299) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.133\u001b[0m \u001b[32m -0.157\u001b[0m,  25% removed (alpha=   0.5560, nnz=225/299) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.135\u001b[0m \u001b[32m -0.159\u001b[0m,  26% removed (alpha=   0.6864, nnz=220/299) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.138\u001b[0m \u001b[32m -0.162\u001b[0m,  27% removed (alpha=   0.8474, nnz=218/299) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.141\u001b[0m \u001b[31m -0.166\u001b[0m,  27% removed (alpha=   1.0462, nnz=217/299) \u001b[31mover  \u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[31m -0.146\u001b[0m \u001b[31m -0.171\u001b[0m,  31% removed (alpha=   1.2915, nnz=206/299) \u001b[33munder \u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[31m -0.152\u001b[0m \u001b[31m -0.177\u001b[0m,  33% removed (alpha=   1.5945, nnz=200/299) \u001b[33munder \u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.137\u001b[0m \u001b[32m -0.162\u001b[0m,  24% removed (alpha=   0.7468, nnz=227/299) \u001b[32mfit ok\u001b[0m [0.6864 1.0462]\n",
      " -0.130  -0.149 -> \u001b[32m -0.138\u001b[0m \u001b[32m -0.163\u001b[0m,  24% removed (alpha=   0.8125, nnz=228/299) \u001b[32mfit ok\u001b[0m [0.6864 1.0462]\n",
      " -0.130  -0.149 -> \u001b[32m -0.139\u001b[0m \u001b[31m -0.164\u001b[0m,  24% removed (alpha=   0.8839, nnz=228/299) \u001b[31mover  \u001b[0m [0.6864 1.0462]\n",
      " -0.130  -0.149 -> \u001b[32m -0.140\u001b[0m \u001b[31m -0.165\u001b[0m,  25% removed (alpha=   0.9616, nnz=224/299) \u001b[31mover  \u001b[0m [0.6864 1.0462]\n",
      "METRICS orig   tr -0.130\u001b[0m, va -0.149\u001b[0m \u001b[1m[te -0.139]\u001b[0m,  ntrees 100,  nnodes 10400,  nleafs  5250,  nnz  5242\n",
      "METRICS prev   \u001b[32mtr -0.136\u001b[0m, \u001b[32mva -0.159\u001b[0m \u001b[1m\u001b[32m[te -0.149]\u001b[0m,  ntrees  24,  nnodes  1918,  nleafs   971,  nnz   967\n",
      "METRICS now    \u001b[32mtr -0.138\u001b[0m, \u001b[32mva -0.162\u001b[0m \u001b[1m\u001b[32m[te -0.150]\u001b[0m,  ntrees  24,  nnodes  1522,  nleafs   773,  nnz   757\n",
      "\n",
      "Level 4 xxtrain.shape (11478, 453) dense transform time: 0.01s, 1.38s relerr 0.1\n",
      " -0.130  -0.149 -> \u001b[32m -0.122\u001b[0m \u001b[32m -0.142\u001b[0m,   1% removed (alpha=   0.0026, nnz=448/453) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.121\u001b[0m \u001b[32m -0.141\u001b[0m,   1% removed (alpha=   0.0067, nnz=447/453) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.120\u001b[0m \u001b[32m -0.141\u001b[0m,   2% removed (alpha=   0.0172, nnz=444/453) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.119\u001b[0m \u001b[32m -0.142\u001b[0m,   4% removed (alpha=   0.0444, nnz=433/453) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.120\u001b[0m \u001b[32m -0.145\u001b[0m,   9% removed (alpha=   0.1145, nnz=412/453) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.124\u001b[0m \u001b[32m -0.151\u001b[0m,  27% removed (alpha=   0.2955, nnz=329/453) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.134\u001b[0m \u001b[32m -0.158\u001b[0m,  37% removed (alpha=   0.7627, nnz=286/453) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.160\u001b[0m \u001b[33m -0.183\u001b[0m,  47% removed (alpha=   1.9684, nnz=240/453) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 832.9831285573441, tolerance: 481.2383887192011\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.130  -0.149 -> \u001b[33m -0.212\u001b[0m \u001b[33m -0.232\u001b[0m,  58% removed (alpha=   5.0802, nnz=190/453) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.279\u001b[0m \u001b[33m -0.291\u001b[0m,  70% removed (alpha=  13.1113, nnz=137/453) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.357\u001b[0m \u001b[33m -0.367\u001b[0m,  78% removed (alpha=  33.8386, nnz= 98/453) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.562\u001b[0m \u001b[33m -0.597\u001b[0m,  85% removed (alpha=  87.3326, nnz= 67/453) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.815\u001b[0m \u001b[33m -0.890\u001b[0m,  91% removed (alpha= 225.3934, nnz= 42/453) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -1.179\u001b[0m \u001b[33m -1.291\u001b[0m,  94% removed (alpha= 581.7091, nnz= 25/453) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -1.522\u001b[0m \u001b[33m -1.613\u001b[0m,  97% removed (alpha=1501.3107, nnz= 15/453) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -2.402\u001b[0m \u001b[33m -2.467\u001b[0m,  97% removed (alpha=3874.6751, nnz= 13/453) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.129\u001b[0m \u001b[32m -0.154\u001b[0m,  28% removed (alpha=   0.3648, nnz=324/453) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.130\u001b[0m \u001b[32m -0.156\u001b[0m,  30% removed (alpha=   0.4504, nnz=317/453) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.131\u001b[0m \u001b[32m -0.157\u001b[0m,  36% removed (alpha=   0.5560, nnz=291/453) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.134\u001b[0m \u001b[32m -0.159\u001b[0m,  36% removed (alpha=   0.6864, nnz=292/453) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.137\u001b[0m \u001b[32m -0.163\u001b[0m,  37% removed (alpha=   0.8474, nnz=284/453) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.142\u001b[0m \u001b[31m -0.167\u001b[0m,  40% removed (alpha=   1.0462, nnz=270/453) \u001b[31mover  \u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[31m -0.148\u001b[0m \u001b[31m -0.172\u001b[0m,  44% removed (alpha=   1.2915, nnz=252/453) \u001b[33munder \u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[31m -0.152\u001b[0m \u001b[31m -0.175\u001b[0m,  45% removed (alpha=   1.5945, nnz=251/453) \u001b[33munder \u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.138\u001b[0m \u001b[32m -0.162\u001b[0m,  37% removed (alpha=   0.7468, nnz=287/453) \u001b[32mfit ok\u001b[0m [0.6864 1.0462]\n",
      " -0.130  -0.149 -> \u001b[32m -0.139\u001b[0m \u001b[32m -0.163\u001b[0m,  37% removed (alpha=   0.8125, nnz=284/453) \u001b[32mfit ok\u001b[0m [0.6864 1.0462]\n",
      " -0.130  -0.149 -> \u001b[32m -0.140\u001b[0m \u001b[31m -0.164\u001b[0m,  38% removed (alpha=   0.8839, nnz=282/453) \u001b[31mover  \u001b[0m [0.6864 1.0462]\n",
      " -0.130  -0.149 -> \u001b[32m -0.141\u001b[0m \u001b[31m -0.165\u001b[0m,  39% removed (alpha=   0.9616, nnz=277/453) \u001b[31mover  \u001b[0m [0.6864 1.0462]\n",
      "METRICS orig   tr -0.130\u001b[0m, va -0.149\u001b[0m \u001b[1m[te -0.139]\u001b[0m,  ntrees 100,  nnodes 10400,  nleafs  5250,  nnz  5242\n",
      "METRICS prev   \u001b[32mtr -0.138\u001b[0m, \u001b[32mva -0.162\u001b[0m \u001b[1m\u001b[32m[te -0.150]\u001b[0m,  ntrees  24,  nnodes  1522,  nleafs   773,  nnz   757\n",
      "METRICS now    \u001b[32mtr -0.137\u001b[0m, \u001b[32mva -0.163\u001b[0m \u001b[1m\u001b[32m[te -0.148]\u001b[0m,  ntrees  24,  nnodes  1054,  nleafs   539,  nnz   499\n",
      "\n",
      "Level 5 xxtrain.shape (11478, 539) dense transform time: 0.00s, 1.25s relerr 0.1\n",
      " -0.130  -0.149 -> \u001b[32m -0.111\u001b[0m \u001b[32m -0.127\u001b[0m,   1% removed (alpha=   0.0026, nnz=535/539) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.110\u001b[0m \u001b[32m -0.127\u001b[0m,   2% removed (alpha=   0.0067, nnz=530/539) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.110\u001b[0m \u001b[32m -0.127\u001b[0m,   3% removed (alpha=   0.0172, nnz=522/539) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.111\u001b[0m \u001b[32m -0.129\u001b[0m,   8% removed (alpha=   0.0444, nnz=498/539) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.115\u001b[0m \u001b[32m -0.133\u001b[0m,  17% removed (alpha=   0.1145, nnz=445/539) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.121\u001b[0m \u001b[32m -0.142\u001b[0m,  32% removed (alpha=   0.2955, nnz=366/539) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.131\u001b[0m \u001b[32m -0.154\u001b[0m,  41% removed (alpha=   0.7627, nnz=316/539) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.156\u001b[0m \u001b[33m -0.194\u001b[0m,  48% removed (alpha=   1.9684, nnz=281/539) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 865.8381614956043, tolerance: 481.2383887192011\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.130  -0.149 -> \u001b[33m -0.199\u001b[0m \u001b[33m -0.233\u001b[0m,  58% removed (alpha=   5.0802, nnz=227/539) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.269\u001b[0m \u001b[33m -0.295\u001b[0m,  68% removed (alpha=  13.1113, nnz=175/539) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.377\u001b[0m \u001b[33m -0.404\u001b[0m,  79% removed (alpha=  33.8386, nnz=111/539) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.616\u001b[0m \u001b[33m -0.695\u001b[0m,  86% removed (alpha=  87.3326, nnz= 75/539) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.923\u001b[0m \u001b[33m -1.018\u001b[0m,  92% removed (alpha= 225.3934, nnz= 42/539) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -1.346\u001b[0m \u001b[33m -1.436\u001b[0m,  94% removed (alpha= 581.7091, nnz= 33/539) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -2.588\u001b[0m \u001b[33m -2.682\u001b[0m,  95% removed (alpha=1501.3107, nnz= 26/539) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -3.450\u001b[0m \u001b[33m -3.537\u001b[0m,  98% removed (alpha=3874.6751, nnz= 13/539) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.129\u001b[0m \u001b[32m -0.150\u001b[0m,  34% removed (alpha=   0.3648, nnz=356/539) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.131\u001b[0m \u001b[32m -0.152\u001b[0m,  34% removed (alpha=   0.4504, nnz=354/539) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.133\u001b[0m \u001b[32m -0.154\u001b[0m,  35% removed (alpha=   0.5560, nnz=348/539) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.135\u001b[0m \u001b[32m -0.156\u001b[0m,  37% removed (alpha=   0.6864, nnz=341/539) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.137\u001b[0m \u001b[32m -0.159\u001b[0m,  39% removed (alpha=   0.8474, nnz=327/539) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.142\u001b[0m \u001b[32m -0.163\u001b[0m,  42% removed (alpha=   1.0462, nnz=312/539) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[31m -0.145\u001b[0m \u001b[31m -0.166\u001b[0m,  44% removed (alpha=   1.2915, nnz=302/539) \u001b[33munder \u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[31m -0.149\u001b[0m \u001b[31m -0.170\u001b[0m,  45% removed (alpha=   1.5945, nnz=296/539) \u001b[33munder \u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.141\u001b[0m \u001b[32m -0.162\u001b[0m,  41% removed (alpha=   0.9219, nnz=317/539) \u001b[32mfit ok\u001b[0m [0.8474 1.2915]\n",
      " -0.130  -0.149 -> \u001b[32m -0.142\u001b[0m \u001b[32m -0.163\u001b[0m,  42% removed (alpha=   1.0030, nnz=314/539) \u001b[32mfit ok\u001b[0m [0.8474 1.2915]\n",
      " -0.130  -0.149 -> \u001b[32m -0.143\u001b[0m \u001b[32m -0.164\u001b[0m,  42% removed (alpha=   1.0912, nnz=311/539) \u001b[32mfit ok\u001b[0m [0.8474 1.2915]\n",
      " -0.130  -0.149 -> \u001b[31m -0.144\u001b[0m \u001b[31m -0.165\u001b[0m,  42% removed (alpha=   1.1872, nnz=310/539) \u001b[33munder \u001b[0m [0.8474 1.2915]\n",
      "METRICS orig   tr -0.130\u001b[0m, va -0.149\u001b[0m \u001b[1m[te -0.139]\u001b[0m,  ntrees 100,  nnodes 10400,  nleafs  5250,  nnz  5242\n",
      "METRICS prev   \u001b[32mtr -0.137\u001b[0m, \u001b[32mva -0.163\u001b[0m \u001b[1m\u001b[32m[te -0.148]\u001b[0m,  ntrees  24,  nnodes  1054,  nleafs   539,  nnz   499\n",
      "METRICS now    \u001b[32mtr -0.143\u001b[0m, \u001b[32mva -0.164\u001b[0m \u001b[1m\u001b[31m[te -0.154]\u001b[0m,  ntrees  24,  nnodes   828,  nleafs   426,  nnz   346\n",
      "\n",
      "Level 6 xxtrain.shape (11478, 426) dense transform time: 0.00s, 1.19s relerr 0.1\n",
      " -0.130  -0.149 -> \u001b[32m -0.124\u001b[0m \u001b[32m -0.142\u001b[0m,   0% removed (alpha=   0.0026, nnz=424/426) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.124\u001b[0m \u001b[32m -0.142\u001b[0m,   0% removed (alpha=   0.0067, nnz=424/426) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.124\u001b[0m \u001b[32m -0.141\u001b[0m,   2% removed (alpha=   0.0172, nnz=416/426) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.123\u001b[0m \u001b[32m -0.140\u001b[0m,   5% removed (alpha=   0.0444, nnz=404/426) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.123\u001b[0m \u001b[32m -0.141\u001b[0m,  10% removed (alpha=   0.1145, nnz=384/426) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.124\u001b[0m \u001b[32m -0.145\u001b[0m,  17% removed (alpha=   0.2955, nnz=352/426) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.129\u001b[0m \u001b[32m -0.154\u001b[0m,  23% removed (alpha=   0.7627, nnz=329/426) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[31m -0.143\u001b[0m \u001b[31m -0.171\u001b[0m,  32% removed (alpha=   1.9684, nnz=291/426) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.188\u001b[0m \u001b[33m -0.206\u001b[0m,  42% removed (alpha=   5.0802, nnz=249/426) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.281\u001b[0m \u001b[33m -0.292\u001b[0m,  55% removed (alpha=  13.1113, nnz=190/426) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.435\u001b[0m \u001b[33m -0.426\u001b[0m,  72% removed (alpha=  33.8386, nnz=121/426) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.716\u001b[0m \u001b[33m -0.697\u001b[0m,  80% removed (alpha=  87.3326, nnz= 86/426) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.997\u001b[0m \u001b[33m -0.969\u001b[0m,  88% removed (alpha= 225.3934, nnz= 52/426) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -1.576\u001b[0m \u001b[33m -1.552\u001b[0m,  90% removed (alpha= 581.7091, nnz= 43/426) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -2.810\u001b[0m \u001b[33m -2.810\u001b[0m,  94% removed (alpha=1501.3107, nnz= 24/426) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -4.749\u001b[0m \u001b[33m -4.803\u001b[0m,  96% removed (alpha=3874.6751, nnz= 18/426) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.125\u001b[0m \u001b[32m -0.145\u001b[0m,  15% removed (alpha=   0.3648, nnz=360/426) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.125\u001b[0m \u001b[32m -0.146\u001b[0m,  17% removed (alpha=   0.4504, nnz=353/426) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.126\u001b[0m \u001b[32m -0.148\u001b[0m,  19% removed (alpha=   0.5560, nnz=346/426) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.127\u001b[0m \u001b[32m -0.150\u001b[0m,  20% removed (alpha=   0.6864, nnz=342/426) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.128\u001b[0m \u001b[32m -0.152\u001b[0m,  24% removed (alpha=   0.8474, nnz=322/426) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.130\u001b[0m \u001b[32m -0.154\u001b[0m,  23% removed (alpha=   1.0462, nnz=326/426) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.133\u001b[0m \u001b[32m -0.159\u001b[0m,  25% removed (alpha=   1.2915, nnz=318/426) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.136\u001b[0m \u001b[32m -0.163\u001b[0m,  27% removed (alpha=   1.5945, nnz=313/426) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.135\u001b[0m \u001b[32m -0.162\u001b[0m,  24% removed (alpha=   1.4051, nnz=322/426) \u001b[32mfit ok\u001b[0m [1.2915 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.136\u001b[0m \u001b[32m -0.162\u001b[0m,  25% removed (alpha=   1.5287, nnz=319/426) \u001b[32mfit ok\u001b[0m [1.2915 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.137\u001b[0m \u001b[31m -0.164\u001b[0m,  28% removed (alpha=   1.6631, nnz=307/426) \u001b[31mover  \u001b[0m [1.2915 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.139\u001b[0m \u001b[31m -0.166\u001b[0m,  29% removed (alpha=   1.8093, nnz=304/426) \u001b[31mover  \u001b[0m [1.2915 1.9684]\n",
      "METRICS orig   tr -0.130\u001b[0m, va -0.149\u001b[0m \u001b[1m[te -0.139]\u001b[0m,  ntrees 100,  nnodes 10400,  nleafs  5250,  nnz  5242\n",
      "METRICS prev   \u001b[32mtr -0.143\u001b[0m, \u001b[32mva -0.164\u001b[0m \u001b[1m\u001b[31m[te -0.154]\u001b[0m,  ntrees  24,  nnodes   828,  nleafs   426,  nnz   346\n",
      "METRICS now    \u001b[32mtr -0.136\u001b[0m, \u001b[32mva -0.163\u001b[0m \u001b[1m\u001b[32m[te -0.146]\u001b[0m,  ntrees  24,  nnodes   816,  nleafs   420,  nnz   313\n",
      "\n",
      "DONE, depth of tree reached 6, 6\n",
      "\n",
      "\n",
      "ROUND 2\n",
      "Level 0 xxtrain.shape (11478, 24) dense transform time: 0.00s, 1.24s relerr 0.1\n",
      " -0.130  -0.149 -> \u001b[32m -0.135\u001b[0m \u001b[32m -0.162\u001b[0m,   0% removed (alpha=   0.0026, nnz=24/24) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.135\u001b[0m \u001b[32m -0.162\u001b[0m,   0% removed (alpha=   0.0067, nnz=24/24) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.135\u001b[0m \u001b[32m -0.162\u001b[0m,   0% removed (alpha=   0.0172, nnz=24/24) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.135\u001b[0m \u001b[32m -0.162\u001b[0m,   0% removed (alpha=   0.0444, nnz=24/24) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.135\u001b[0m \u001b[32m -0.162\u001b[0m,   0% removed (alpha=   0.1145, nnz=24/24) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.135\u001b[0m \u001b[32m -0.162\u001b[0m,   0% removed (alpha=   0.2955, nnz=24/24) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.136\u001b[0m \u001b[32m -0.162\u001b[0m,   0% removed (alpha=   0.7627, nnz=24/24) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.136\u001b[0m \u001b[32m -0.162\u001b[0m,   0% removed (alpha=   1.9684, nnz=24/24) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.138\u001b[0m \u001b[31m -0.164\u001b[0m,   0% removed (alpha=   5.0802, nnz=24/24) \u001b[31mover  \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[31m -0.149\u001b[0m \u001b[31m -0.174\u001b[0m,   0% removed (alpha=  13.1113, nnz=24/24) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.188\u001b[0m \u001b[33m -0.212\u001b[0m,  12% removed (alpha=  33.8386, nnz=21/24) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.288\u001b[0m \u001b[33m -0.308\u001b[0m,  25% removed (alpha=  87.3326, nnz=18/24) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.470\u001b[0m \u001b[33m -0.480\u001b[0m,  50% removed (alpha= 225.3934, nnz=12/24) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.632\u001b[0m \u001b[33m -0.639\u001b[0m,  67% removed (alpha= 581.7091, nnz= 8/24) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.876\u001b[0m \u001b[33m -0.890\u001b[0m,  79% removed (alpha=1501.3107, nnz= 5/24) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -1.215\u001b[0m \u001b[33m -1.235\u001b[0m,  83% removed (alpha=3874.6751, nnz= 4/24) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.136\u001b[0m \u001b[32m -0.162\u001b[0m,   0% removed (alpha=   0.9416, nnz=24/24) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.130  -0.149 -> \u001b[32m -0.136\u001b[0m \u001b[32m -0.162\u001b[0m,   0% removed (alpha=   1.1624, nnz=24/24) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.130  -0.149 -> \u001b[32m -0.136\u001b[0m \u001b[32m -0.162\u001b[0m,   0% removed (alpha=   1.4350, nnz=24/24) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.130  -0.149 -> \u001b[32m -0.136\u001b[0m \u001b[32m -0.162\u001b[0m,   0% removed (alpha=   1.7716, nnz=24/24) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.130  -0.149 -> \u001b[32m -0.136\u001b[0m \u001b[32m -0.162\u001b[0m,   0% removed (alpha=   2.1871, nnz=24/24) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.130  -0.149 -> \u001b[32m -0.136\u001b[0m \u001b[32m -0.163\u001b[0m,   0% removed (alpha=   2.7001, nnz=24/24) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.130  -0.149 -> \u001b[32m -0.136\u001b[0m \u001b[32m -0.163\u001b[0m,   0% removed (alpha=   3.3333, nnz=24/24) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.130  -0.149 -> \u001b[32m -0.137\u001b[0m \u001b[32m -0.163\u001b[0m,   0% removed (alpha=   4.1151, nnz=24/24) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.130  -0.149 -> \u001b[32m -0.137\u001b[0m \u001b[32m -0.163\u001b[0m,   0% removed (alpha=   3.6264, nnz=24/24) \u001b[32mfit ok\u001b[0m [3.3333 5.0802]\n",
      " -0.130  -0.149 -> \u001b[32m -0.137\u001b[0m \u001b[32m -0.163\u001b[0m,   0% removed (alpha=   3.9453, nnz=24/24) \u001b[32mfit ok\u001b[0m [3.3333 5.0802]\n",
      " -0.130  -0.149 -> \u001b[32m -0.137\u001b[0m \u001b[32m -0.163\u001b[0m,   0% removed (alpha=   4.2922, nnz=24/24) \u001b[32mfit ok\u001b[0m [3.3333 5.0802]\n",
      " -0.130  -0.149 -> \u001b[32m -0.137\u001b[0m \u001b[32m -0.164\u001b[0m,   0% removed (alpha=   4.6696, nnz=24/24) \u001b[32mfit ok\u001b[0m [3.3333 5.0802]\n",
      "METRICS orig   tr -0.130\u001b[0m, va -0.149\u001b[0m \u001b[1m[te -0.139]\u001b[0m,  ntrees 100,  nnodes 10400,  nleafs  5250,  nnz  5242\n",
      "METRICS prev   \u001b[32mtr -0.136\u001b[0m, \u001b[32mva -0.163\u001b[0m \u001b[1m\u001b[32m[te -0.146]\u001b[0m,  ntrees  24,  nnodes   816,  nleafs   420,  nnz   313\n",
      "METRICS now    \u001b[32mtr -0.135\u001b[0m, \u001b[32mva -0.162\u001b[0m \u001b[1m\u001b[32m[te -0.145]\u001b[0m,  ntrees  24,  nnodes   816,  nleafs   420,  nnz   313\n",
      "\n",
      "Level 1 xxtrain.shape (11478, 90) dense transform time: 0.00s, 1.26s relerr 0.1\n",
      " -0.130  -0.149 -> \u001b[32m -0.136\u001b[0m \u001b[32m -0.161\u001b[0m,   3% removed (alpha=   0.0026, nnz=87/90) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.136\u001b[0m \u001b[32m -0.161\u001b[0m,   3% removed (alpha=   0.0067, nnz=87/90) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.136\u001b[0m \u001b[32m -0.161\u001b[0m,   3% removed (alpha=   0.0172, nnz=87/90) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.136\u001b[0m \u001b[32m -0.161\u001b[0m,   2% removed (alpha=   0.0444, nnz=88/90) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.135\u001b[0m \u001b[32m -0.161\u001b[0m,   2% removed (alpha=   0.1145, nnz=88/90) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.135\u001b[0m \u001b[32m -0.161\u001b[0m,   8% removed (alpha=   0.2955, nnz=83/90) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.136\u001b[0m \u001b[32m -0.161\u001b[0m,   9% removed (alpha=   0.7627, nnz=82/90) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.138\u001b[0m \u001b[32m -0.164\u001b[0m,  16% removed (alpha=   1.9684, nnz=76/90) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[31m -0.146\u001b[0m \u001b[31m -0.171\u001b[0m,  18% removed (alpha=   5.0802, nnz=74/90) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.174\u001b[0m \u001b[33m -0.197\u001b[0m,  36% removed (alpha=  13.1113, nnz=58/90) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.240\u001b[0m \u001b[33m -0.258\u001b[0m,  46% removed (alpha=  33.8386, nnz=49/90) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.373\u001b[0m \u001b[33m -0.381\u001b[0m,  66% removed (alpha=  87.3326, nnz=31/90) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.537\u001b[0m \u001b[33m -0.551\u001b[0m,  74% removed (alpha= 225.3934, nnz=23/90) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.740\u001b[0m \u001b[33m -0.764\u001b[0m,  87% removed (alpha= 581.7091, nnz=12/90) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -1.036\u001b[0m \u001b[33m -1.063\u001b[0m,  92% removed (alpha=1501.3107, nnz= 7/90) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -1.203\u001b[0m \u001b[33m -1.234\u001b[0m,  96% removed (alpha=3874.6751, nnz= 4/90) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.136\u001b[0m \u001b[32m -0.162\u001b[0m,  24% removed (alpha=   0.9416, nnz=68/90) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.130  -0.149 -> \u001b[32m -0.136\u001b[0m \u001b[32m -0.162\u001b[0m,  28% removed (alpha=   1.1624, nnz=65/90) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.130  -0.149 -> \u001b[32m -0.137\u001b[0m \u001b[32m -0.162\u001b[0m,  32% removed (alpha=   1.4350, nnz=61/90) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.130  -0.149 -> \u001b[32m -0.137\u001b[0m \u001b[32m -0.163\u001b[0m,  32% removed (alpha=   1.7716, nnz=61/90) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.130  -0.149 -> \u001b[32m -0.139\u001b[0m \u001b[31m -0.164\u001b[0m,  34% removed (alpha=   2.1871, nnz=59/90) \u001b[31mover  \u001b[0m [0.7627 5.0802]\n",
      " -0.130  -0.149 -> \u001b[32m -0.140\u001b[0m \u001b[31m -0.166\u001b[0m,  34% removed (alpha=   2.7001, nnz=59/90) \u001b[31mover  \u001b[0m [0.7627 5.0802]\n",
      " -0.130  -0.149 -> \u001b[32m -0.142\u001b[0m \u001b[31m -0.167\u001b[0m,  34% removed (alpha=   3.3333, nnz=59/90) \u001b[31mover  \u001b[0m [0.7627 5.0802]\n",
      " -0.130  -0.149 -> \u001b[31m -0.144\u001b[0m \u001b[31m -0.169\u001b[0m,  34% removed (alpha=   4.1151, nnz=59/90) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.130  -0.149 -> \u001b[32m -0.137\u001b[0m \u001b[32m -0.163\u001b[0m,  33% removed (alpha=   1.5612, nnz=60/90) \u001b[32mfit ok\u001b[0m [1.435  2.1871]\n",
      " -0.130  -0.149 -> \u001b[32m -0.137\u001b[0m \u001b[32m -0.163\u001b[0m,  32% removed (alpha=   1.6985, nnz=61/90) \u001b[32mfit ok\u001b[0m [1.435  2.1871]\n",
      " -0.130  -0.149 -> \u001b[32m -0.138\u001b[0m \u001b[32m -0.164\u001b[0m,  34% removed (alpha=   1.8478, nnz=59/90) \u001b[32mfit ok\u001b[0m [1.435  2.1871]\n",
      " -0.130  -0.149 -> \u001b[32m -0.138\u001b[0m \u001b[31m -0.164\u001b[0m,  34% removed (alpha=   2.0103, nnz=59/90) \u001b[31mover  \u001b[0m [1.435  2.1871]\n",
      "METRICS orig   tr -0.130\u001b[0m, va -0.149\u001b[0m \u001b[1m[te -0.139]\u001b[0m,  ntrees 100,  nnodes 10400,  nleafs  5250,  nnz  5242\n",
      "METRICS prev   \u001b[32mtr -0.135\u001b[0m, \u001b[32mva -0.162\u001b[0m \u001b[1m\u001b[32m[te -0.145]\u001b[0m,  ntrees  24,  nnodes   816,  nleafs   420,  nnz   313\n",
      "METRICS now    \u001b[32mtr -0.138\u001b[0m, \u001b[32mva -0.164\u001b[0m \u001b[1m\u001b[32m[te -0.148]\u001b[0m,  ntrees  24,  nnodes   814,  nleafs   419,  nnz   336\n",
      "\n",
      "Level 2 xxtrain.shape (11478, 161) dense transform time: 0.00s, 1.27s relerr 0.1\n",
      " -0.130  -0.149 -> \u001b[31m -0.154\u001b[0m \u001b[31m -0.172\u001b[0m,   1% removed (alpha=   0.0026, nnz=159/161) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[31m -0.153\u001b[0m \u001b[31m -0.172\u001b[0m,   1% removed (alpha=   0.0067, nnz=159/161) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[31m -0.151\u001b[0m \u001b[31m -0.171\u001b[0m,   2% removed (alpha=   0.0172, nnz=158/161) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[31m -0.147\u001b[0m \u001b[31m -0.168\u001b[0m,   4% removed (alpha=   0.0444, nnz=154/161) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[31m -0.147\u001b[0m \u001b[31m -0.169\u001b[0m,   4% removed (alpha=   0.1145, nnz=155/161) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[31m -0.147\u001b[0m \u001b[31m -0.169\u001b[0m,   7% removed (alpha=   0.2955, nnz=150/161) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.141\u001b[0m \u001b[31m -0.165\u001b[0m,  19% removed (alpha=   0.7627, nnz=131/161) \u001b[31mover  \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[31m -0.144\u001b[0m \u001b[31m -0.168\u001b[0m,  30% removed (alpha=   1.9684, nnz=112/161) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.167\u001b[0m \u001b[33m -0.189\u001b[0m,  40% removed (alpha=   5.0802, nnz= 96/161) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 523.1346862806563, tolerance: 481.2383887192011\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.130  -0.149 -> \u001b[33m -0.206\u001b[0m \u001b[33m -0.228\u001b[0m,  58% removed (alpha=  13.1113, nnz= 68/161) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.280\u001b[0m \u001b[33m -0.298\u001b[0m,  65% removed (alpha=  33.8386, nnz= 57/161) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.424\u001b[0m \u001b[33m -0.440\u001b[0m,  75% removed (alpha=  87.3326, nnz= 41/161) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.562\u001b[0m \u001b[33m -0.579\u001b[0m,  83% removed (alpha= 225.3934, nnz= 27/161) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.829\u001b[0m \u001b[33m -0.857\u001b[0m,  91% removed (alpha= 581.7091, nnz= 15/161) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -1.098\u001b[0m \u001b[33m -1.139\u001b[0m,  94% removed (alpha=1501.3107, nnz= 10/161) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -1.709\u001b[0m \u001b[33m -1.775\u001b[0m,  96% removed (alpha=3874.6751, nnz=  7/161) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.137\u001b[0m \u001b[32m -0.162\u001b[0m,  32% removed (alpha=   0.8474, nnz=109/161) \u001b[32mfit ok\u001b[0m [0.7627 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.138\u001b[0m \u001b[32m -0.163\u001b[0m,  32% removed (alpha=   0.9416, nnz=109/161) \u001b[32mfit ok\u001b[0m [0.7627 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.138\u001b[0m \u001b[32m -0.163\u001b[0m,  33% removed (alpha=   1.0462, nnz=108/161) \u001b[32mfit ok\u001b[0m [0.7627 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.139\u001b[0m \u001b[32m -0.164\u001b[0m,  33% removed (alpha=   1.1624, nnz=108/161) \u001b[32mfit ok\u001b[0m [0.7627 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.140\u001b[0m \u001b[31m -0.164\u001b[0m,  33% removed (alpha=   1.2915, nnz=108/161) \u001b[31mover  \u001b[0m [0.7627 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.140\u001b[0m \u001b[31m -0.165\u001b[0m,  34% removed (alpha=   1.4350, nnz=107/161) \u001b[31mover  \u001b[0m [0.7627 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.141\u001b[0m \u001b[31m -0.166\u001b[0m,  34% removed (alpha=   1.5945, nnz=106/161) \u001b[31mover  \u001b[0m [0.7627 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.142\u001b[0m \u001b[31m -0.166\u001b[0m,  35% removed (alpha=   1.7716, nnz=104/161) \u001b[31mover  \u001b[0m [0.7627 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.139\u001b[0m \u001b[32m -0.163\u001b[0m,  33% removed (alpha=   1.0912, nnz=108/161) \u001b[32mfit ok\u001b[0m [1.0462 1.2915]\n",
      " -0.130  -0.149 -> \u001b[32m -0.139\u001b[0m \u001b[32m -0.164\u001b[0m,  33% removed (alpha=   1.1382, nnz=108/161) \u001b[32mfit ok\u001b[0m [1.0462 1.2915]\n",
      " -0.130  -0.149 -> \u001b[32m -0.139\u001b[0m \u001b[32m -0.164\u001b[0m,  33% removed (alpha=   1.1872, nnz=108/161) \u001b[32mfit ok\u001b[0m [1.0462 1.2915]\n",
      " -0.130  -0.149 -> \u001b[32m -0.139\u001b[0m \u001b[31m -0.164\u001b[0m,  32% removed (alpha=   1.2383, nnz=109/161) \u001b[31mover  \u001b[0m [1.0462 1.2915]\n",
      "METRICS orig   tr -0.130\u001b[0m, va -0.149\u001b[0m \u001b[1m[te -0.139]\u001b[0m,  ntrees 100,  nnodes 10400,  nleafs  5250,  nnz  5242\n",
      "METRICS prev   \u001b[32mtr -0.138\u001b[0m, \u001b[32mva -0.164\u001b[0m \u001b[1m\u001b[32m[te -0.148]\u001b[0m,  ntrees  24,  nnodes   814,  nleafs   419,  nnz   336\n",
      "METRICS now    \u001b[32mtr -0.138\u001b[0m, \u001b[32mva -0.163\u001b[0m \u001b[1m\u001b[32m[te -0.148]\u001b[0m,  ntrees  24,  nnodes   792,  nleafs   408,  nnz   343\n",
      "\n",
      "Level 3 xxtrain.shape (11478, 251) dense transform time: 0.00s, 1.31s relerr 0.1\n",
      " -0.130  -0.149 -> \u001b[32m -0.134\u001b[0m \u001b[32m -0.160\u001b[0m,   0% removed (alpha=   0.0026, nnz=250/251) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.134\u001b[0m \u001b[32m -0.160\u001b[0m,   1% removed (alpha=   0.0067, nnz=248/251) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.134\u001b[0m \u001b[32m -0.160\u001b[0m,   2% removed (alpha=   0.0172, nnz=247/251) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.134\u001b[0m \u001b[32m -0.160\u001b[0m,   3% removed (alpha=   0.0444, nnz=244/251) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.134\u001b[0m \u001b[32m -0.160\u001b[0m,   4% removed (alpha=   0.1145, nnz=240/251) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.136\u001b[0m \u001b[32m -0.160\u001b[0m,  11% removed (alpha=   0.2955, nnz=223/251) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.142\u001b[0m \u001b[31m -0.166\u001b[0m,  24% removed (alpha=   0.7627, nnz=192/251) \u001b[31mover  \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.156\u001b[0m \u001b[33m -0.181\u001b[0m,  34% removed (alpha=   1.9684, nnz=166/251) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.183\u001b[0m \u001b[33m -0.209\u001b[0m,  44% removed (alpha=   5.0802, nnz=140/251) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.237\u001b[0m \u001b[33m -0.263\u001b[0m,  51% removed (alpha=  13.1113, nnz=124/251) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.346\u001b[0m \u001b[33m -0.379\u001b[0m,  61% removed (alpha=  33.8386, nnz= 97/251) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.518\u001b[0m \u001b[33m -0.567\u001b[0m,  70% removed (alpha=  87.3326, nnz= 75/251) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.760\u001b[0m \u001b[33m -0.840\u001b[0m,  78% removed (alpha= 225.3934, nnz= 54/251) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -1.372\u001b[0m \u001b[33m -1.492\u001b[0m,  85% removed (alpha= 581.7091, nnz= 38/251) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -1.973\u001b[0m \u001b[33m -2.056\u001b[0m,  90% removed (alpha=1501.3107, nnz= 24/251) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -3.304\u001b[0m \u001b[33m -3.329\u001b[0m,  95% removed (alpha=3874.6751, nnz= 12/251) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.136\u001b[0m \u001b[32m -0.162\u001b[0m,   7% removed (alpha=   0.1414, nnz=233/251) \u001b[32mfit ok\u001b[0m [0.1145 0.7627]\n",
      " -0.130  -0.149 -> \u001b[32m -0.135\u001b[0m \u001b[32m -0.161\u001b[0m,  10% removed (alpha=   0.1745, nnz=226/251) \u001b[32mfit ok\u001b[0m [0.1145 0.7627]\n",
      " -0.130  -0.149 -> \u001b[32m -0.135\u001b[0m \u001b[32m -0.160\u001b[0m,  14% removed (alpha=   0.2154, nnz=215/251) \u001b[32mfit ok\u001b[0m [0.1145 0.7627]\n",
      " -0.130  -0.149 -> \u001b[32m -0.136\u001b[0m \u001b[32m -0.161\u001b[0m,  15% removed (alpha=   0.2660, nnz=214/251) \u001b[32mfit ok\u001b[0m [0.1145 0.7627]\n",
      " -0.130  -0.149 -> \u001b[32m -0.136\u001b[0m \u001b[32m -0.161\u001b[0m,  19% removed (alpha=   0.3284, nnz=204/251) \u001b[32mfit ok\u001b[0m [0.1145 0.7627]\n",
      " -0.130  -0.149 -> \u001b[32m -0.137\u001b[0m \u001b[32m -0.162\u001b[0m,  19% removed (alpha=   0.4054, nnz=204/251) \u001b[32mfit ok\u001b[0m [0.1145 0.7627]\n",
      " -0.130  -0.149 -> \u001b[32m -0.138\u001b[0m \u001b[32m -0.163\u001b[0m,  20% removed (alpha=   0.5004, nnz=200/251) \u001b[32mfit ok\u001b[0m [0.1145 0.7627]\n",
      " -0.130  -0.149 -> \u001b[32m -0.139\u001b[0m \u001b[32m -0.164\u001b[0m,  20% removed (alpha=   0.6178, nnz=202/251) \u001b[32mfit ok\u001b[0m [0.1145 0.7627]\n",
      " -0.130  -0.149 -> \u001b[32m -0.138\u001b[0m \u001b[32m -0.163\u001b[0m,  19% removed (alpha=   0.5444, nnz=203/251) \u001b[32mfit ok\u001b[0m [0.5004 0.7627]\n",
      " -0.130  -0.149 -> \u001b[32m -0.139\u001b[0m \u001b[32m -0.164\u001b[0m,  18% removed (alpha=   0.5923, nnz=205/251) \u001b[32mfit ok\u001b[0m [0.5004 0.7627]\n",
      " -0.130  -0.149 -> \u001b[32m -0.139\u001b[0m \u001b[31m -0.164\u001b[0m,  24% removed (alpha=   0.6444, nnz=190/251) \u001b[31mover  \u001b[0m [0.5004 0.7627]\n",
      " -0.130  -0.149 -> \u001b[32m -0.139\u001b[0m \u001b[31m -0.165\u001b[0m,  22% removed (alpha=   0.7011, nnz=196/251) \u001b[31mover  \u001b[0m [0.5004 0.7627]\n",
      "METRICS orig   tr -0.130\u001b[0m, va -0.149\u001b[0m \u001b[1m[te -0.139]\u001b[0m,  ntrees 100,  nnodes 10400,  nleafs  5250,  nnz  5242\n",
      "METRICS prev   \u001b[32mtr -0.138\u001b[0m, \u001b[32mva -0.163\u001b[0m \u001b[1m\u001b[32m[te -0.148]\u001b[0m,  ntrees  24,  nnodes   792,  nleafs   408,  nnz   343\n",
      "METRICS now    \u001b[32mtr -0.138\u001b[0m, \u001b[32mva -0.163\u001b[0m \u001b[1m\u001b[32m[te -0.145]\u001b[0m,  ntrees  24,  nnodes   756,  nleafs   390,  nnz   362\n",
      "\n",
      "Level 4 xxtrain.shape (11478, 330) dense transform time: 0.00s, 1.24s relerr 0.1\n",
      " -0.130  -0.149 -> \u001b[32m -0.136\u001b[0m \u001b[32m -0.160\u001b[0m,   1% removed (alpha=   0.0026, nnz=327/330) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.136\u001b[0m \u001b[32m -0.159\u001b[0m,   1% removed (alpha=   0.0067, nnz=327/330) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.135\u001b[0m \u001b[32m -0.159\u001b[0m,   3% removed (alpha=   0.0172, nnz=321/330) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.135\u001b[0m \u001b[32m -0.159\u001b[0m,   5% removed (alpha=   0.0444, nnz=315/330) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.135\u001b[0m \u001b[32m -0.159\u001b[0m,   8% removed (alpha=   0.1145, nnz=305/330) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.137\u001b[0m \u001b[32m -0.163\u001b[0m,  21% removed (alpha=   0.2955, nnz=262/330) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.143\u001b[0m \u001b[31m -0.170\u001b[0m,  29% removed (alpha=   0.7627, nnz=233/330) \u001b[31mover  \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.159\u001b[0m \u001b[33m -0.183\u001b[0m,  37% removed (alpha=   1.9684, nnz=209/330) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 490.6537789303662, tolerance: 481.2383887192011\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.130  -0.149 -> \u001b[33m -0.190\u001b[0m \u001b[33m -0.207\u001b[0m,  46% removed (alpha=   5.0802, nnz=178/330) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.256\u001b[0m \u001b[33m -0.269\u001b[0m,  58% removed (alpha=  13.1113, nnz=139/330) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.397\u001b[0m \u001b[33m -0.411\u001b[0m,  68% removed (alpha=  33.8386, nnz=104/330) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.591\u001b[0m \u001b[33m -0.610\u001b[0m,  78% removed (alpha=  87.3326, nnz= 72/330) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.833\u001b[0m \u001b[33m -0.865\u001b[0m,  88% removed (alpha= 225.3934, nnz= 40/330) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -1.184\u001b[0m \u001b[33m -1.207\u001b[0m,  91% removed (alpha= 581.7091, nnz= 31/330) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -1.796\u001b[0m \u001b[33m -1.803\u001b[0m,  95% removed (alpha=1501.3107, nnz= 18/330) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -2.934\u001b[0m \u001b[33m -2.957\u001b[0m,  96% removed (alpha=3874.6751, nnz= 12/330) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.138\u001b[0m \u001b[32m -0.162\u001b[0m,  12% removed (alpha=   0.1414, nnz=289/330) \u001b[32mfit ok\u001b[0m [0.1145 0.7627]\n",
      " -0.130  -0.149 -> \u001b[32m -0.138\u001b[0m \u001b[32m -0.162\u001b[0m,  16% removed (alpha=   0.1745, nnz=277/330) \u001b[32mfit ok\u001b[0m [0.1145 0.7627]\n",
      " -0.130  -0.149 -> \u001b[32m -0.138\u001b[0m \u001b[32m -0.162\u001b[0m,  17% removed (alpha=   0.2154, nnz=274/330) \u001b[32mfit ok\u001b[0m [0.1145 0.7627]\n",
      " -0.130  -0.149 -> \u001b[32m -0.138\u001b[0m \u001b[32m -0.163\u001b[0m,  17% removed (alpha=   0.2660, nnz=273/330) \u001b[32mfit ok\u001b[0m [0.1145 0.7627]\n",
      " -0.130  -0.149 -> \u001b[32m -0.139\u001b[0m \u001b[32m -0.163\u001b[0m,  19% removed (alpha=   0.3284, nnz=267/330) \u001b[32mfit ok\u001b[0m [0.1145 0.7627]\n",
      " -0.130  -0.149 -> \u001b[32m -0.139\u001b[0m \u001b[31m -0.164\u001b[0m,  19% removed (alpha=   0.4054, nnz=266/330) \u001b[31mover  \u001b[0m [0.1145 0.7627]\n",
      " -0.130  -0.149 -> \u001b[32m -0.140\u001b[0m \u001b[31m -0.165\u001b[0m,  24% removed (alpha=   0.5004, nnz=251/330) \u001b[31mover  \u001b[0m [0.1145 0.7627]\n",
      " -0.130  -0.149 -> \u001b[32m -0.141\u001b[0m \u001b[31m -0.166\u001b[0m,  26% removed (alpha=   0.6178, nnz=245/330) \u001b[31mover  \u001b[0m [0.1145 0.7627]\n",
      " -0.130  -0.149 -> \u001b[32m -0.138\u001b[0m \u001b[32m -0.164\u001b[0m,  18% removed (alpha=   0.2894, nnz=270/330) \u001b[32mfit ok\u001b[0m [0.266  0.4054]\n",
      " -0.130  -0.149 -> \u001b[32m -0.139\u001b[0m \u001b[32m -0.164\u001b[0m,  18% removed (alpha=   0.3148, nnz=269/330) \u001b[32mfit ok\u001b[0m [0.266  0.4054]\n",
      " -0.130  -0.149 -> \u001b[32m -0.139\u001b[0m \u001b[32m -0.164\u001b[0m,  19% removed (alpha=   0.3425, nnz=267/330) \u001b[32mfit ok\u001b[0m [0.266  0.4054]\n",
      " -0.130  -0.149 -> \u001b[32m -0.139\u001b[0m \u001b[32m -0.164\u001b[0m,  21% removed (alpha=   0.3726, nnz=262/330) \u001b[32mfit ok\u001b[0m [0.266  0.4054]\n",
      "METRICS orig   tr -0.130\u001b[0m, va -0.149\u001b[0m \u001b[1m[te -0.139]\u001b[0m,  ntrees 100,  nnodes 10400,  nleafs  5250,  nnz  5242\n",
      "METRICS prev   \u001b[32mtr -0.138\u001b[0m, \u001b[32mva -0.163\u001b[0m \u001b[1m\u001b[32m[te -0.145]\u001b[0m,  ntrees  24,  nnodes   756,  nleafs   390,  nnz   362\n",
      "METRICS now    \u001b[32mtr -0.137\u001b[0m, \u001b[32mva -0.163\u001b[0m \u001b[1m\u001b[32m[te -0.142]\u001b[0m,  ntrees  24,  nnodes   722,  nleafs   373,  nnz   341\n",
      "\n",
      "Level 5 xxtrain.shape (11478, 373) dense transform time: 0.00s, 1.20s relerr 0.1\n",
      " -0.130  -0.149 -> \u001b[32m -0.136\u001b[0m \u001b[32m -0.149\u001b[0m,   1% removed (alpha=   0.0026, nnz=370/373) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.136\u001b[0m \u001b[32m -0.149\u001b[0m,   1% removed (alpha=   0.0067, nnz=370/373) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.136\u001b[0m \u001b[32m -0.149\u001b[0m,   1% removed (alpha=   0.0172, nnz=368/373) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.135\u001b[0m \u001b[32m -0.148\u001b[0m,   3% removed (alpha=   0.0444, nnz=360/373) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.134\u001b[0m \u001b[32m -0.149\u001b[0m,  10% removed (alpha=   0.1145, nnz=335/373) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.135\u001b[0m \u001b[32m -0.153\u001b[0m,  21% removed (alpha=   0.2955, nnz=296/373) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.140\u001b[0m \u001b[31m -0.165\u001b[0m,  27% removed (alpha=   0.7627, nnz=274/373) \u001b[31mover  \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.156\u001b[0m \u001b[33m -0.184\u001b[0m,  35% removed (alpha=   1.9684, nnz=243/373) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.189\u001b[0m \u001b[33m -0.212\u001b[0m,  45% removed (alpha=   5.0802, nnz=205/373) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 855.0662372152706, tolerance: 481.2383887192011\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.130  -0.149 -> \u001b[33m -0.263\u001b[0m \u001b[33m -0.276\u001b[0m,  57% removed (alpha=  13.1113, nnz=160/373) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.367\u001b[0m \u001b[33m -0.390\u001b[0m,  71% removed (alpha=  33.8386, nnz=107/373) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.522\u001b[0m \u001b[33m -0.540\u001b[0m,  82% removed (alpha=  87.3326, nnz= 69/373) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.835\u001b[0m \u001b[33m -0.828\u001b[0m,  89% removed (alpha= 225.3934, nnz= 41/373) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -1.239\u001b[0m \u001b[33m -1.249\u001b[0m,  92% removed (alpha= 581.7091, nnz= 31/373) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -2.208\u001b[0m \u001b[33m -2.298\u001b[0m,  95% removed (alpha=1501.3107, nnz= 20/373) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -3.618\u001b[0m \u001b[33m -3.741\u001b[0m,  97% removed (alpha=3874.6751, nnz= 11/373) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.139\u001b[0m \u001b[32m -0.154\u001b[0m,  10% removed (alpha=   0.1414, nnz=334/373) \u001b[32mfit ok\u001b[0m [0.1145 0.7627]\n",
      " -0.130  -0.149 -> \u001b[32m -0.136\u001b[0m \u001b[32m -0.152\u001b[0m,  14% removed (alpha=   0.1745, nnz=321/373) \u001b[32mfit ok\u001b[0m [0.1145 0.7627]\n",
      " -0.130  -0.149 -> \u001b[32m -0.136\u001b[0m \u001b[32m -0.152\u001b[0m,  15% removed (alpha=   0.2154, nnz=318/373) \u001b[32mfit ok\u001b[0m [0.1145 0.7627]\n",
      " -0.130  -0.149 -> \u001b[32m -0.135\u001b[0m \u001b[32m -0.153\u001b[0m,  19% removed (alpha=   0.2660, nnz=301/373) \u001b[32mfit ok\u001b[0m [0.1145 0.7627]\n",
      " -0.130  -0.149 -> \u001b[32m -0.135\u001b[0m \u001b[32m -0.155\u001b[0m,  19% removed (alpha=   0.3284, nnz=302/373) \u001b[32mfit ok\u001b[0m [0.1145 0.7627]\n",
      " -0.130  -0.149 -> \u001b[32m -0.136\u001b[0m \u001b[32m -0.156\u001b[0m,  20% removed (alpha=   0.4054, nnz=297/373) \u001b[32mfit ok\u001b[0m [0.1145 0.7627]\n",
      " -0.130  -0.149 -> \u001b[32m -0.136\u001b[0m \u001b[32m -0.158\u001b[0m,  20% removed (alpha=   0.5004, nnz=300/373) \u001b[32mfit ok\u001b[0m [0.1145 0.7627]\n",
      " -0.130  -0.149 -> \u001b[32m -0.138\u001b[0m \u001b[32m -0.162\u001b[0m,  20% removed (alpha=   0.6178, nnz=297/373) \u001b[32mfit ok\u001b[0m [0.1145 0.7627]\n",
      " -0.130  -0.149 -> \u001b[32m -0.138\u001b[0m \u001b[32m -0.161\u001b[0m,  19% removed (alpha=   0.5444, nnz=301/373) \u001b[32mfit ok\u001b[0m [0.5004 0.7627]\n",
      " -0.130  -0.149 -> \u001b[32m -0.138\u001b[0m \u001b[32m -0.162\u001b[0m,  21% removed (alpha=   0.5923, nnz=296/373) \u001b[32mfit ok\u001b[0m [0.5004 0.7627]\n",
      " -0.130  -0.149 -> \u001b[32m -0.138\u001b[0m \u001b[32m -0.162\u001b[0m,  21% removed (alpha=   0.6444, nnz=295/373) \u001b[32mfit ok\u001b[0m [0.5004 0.7627]\n",
      " -0.130  -0.149 -> \u001b[32m -0.139\u001b[0m \u001b[32m -0.163\u001b[0m,  21% removed (alpha=   0.7011, nnz=296/373) \u001b[32mfit ok\u001b[0m [0.5004 0.7627]\n",
      "METRICS orig   tr -0.130\u001b[0m, va -0.149\u001b[0m \u001b[1m[te -0.139]\u001b[0m,  ntrees 100,  nnodes 10400,  nleafs  5250,  nnz  5242\n",
      "METRICS prev   \u001b[32mtr -0.137\u001b[0m, \u001b[32mva -0.163\u001b[0m \u001b[1m\u001b[32m[te -0.142]\u001b[0m,  ntrees  24,  nnodes   722,  nleafs   373,  nnz   341\n",
      "METRICS now    \u001b[32mtr -0.138\u001b[0m, \u001b[32mva -0.162\u001b[0m \u001b[1m\u001b[32m[te -0.147]\u001b[0m,  ntrees  24,  nnodes   702,  nleafs   363,  nnz   307\n",
      "\n",
      "Level 6 xxtrain.shape (11478, 363) dense transform time: 0.00s, 1.22s relerr 0.1\n",
      " -0.130  -0.149 -> \u001b[32m -0.130\u001b[0m \u001b[32m -0.146\u001b[0m,   0% removed (alpha=   0.0026, nnz=362/363) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.130\u001b[0m \u001b[32m -0.146\u001b[0m,   0% removed (alpha=   0.0067, nnz=362/363) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.130\u001b[0m \u001b[32m -0.146\u001b[0m,   1% removed (alpha=   0.0172, nnz=359/363) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.129\u001b[0m \u001b[32m -0.145\u001b[0m,   4% removed (alpha=   0.0444, nnz=350/363) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.128\u001b[0m \u001b[32m -0.146\u001b[0m,  10% removed (alpha=   0.1145, nnz=328/363) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.129\u001b[0m \u001b[32m -0.148\u001b[0m,  16% removed (alpha=   0.2955, nnz=305/363) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.133\u001b[0m \u001b[32m -0.159\u001b[0m,  19% removed (alpha=   0.7627, nnz=295/363) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[31m -0.144\u001b[0m \u001b[31m -0.173\u001b[0m,  28% removed (alpha=   1.9684, nnz=261/363) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.182\u001b[0m \u001b[33m -0.201\u001b[0m,  36% removed (alpha=   5.0802, nnz=233/363) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.261\u001b[0m \u001b[33m -0.282\u001b[0m,  50% removed (alpha=  13.1113, nnz=180/363) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.391\u001b[0m \u001b[33m -0.413\u001b[0m,  68% removed (alpha=  33.8386, nnz=115/363) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.578\u001b[0m \u001b[33m -0.598\u001b[0m,  81% removed (alpha=  87.3326, nnz= 68/363) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -0.923\u001b[0m \u001b[33m -0.936\u001b[0m,  87% removed (alpha= 225.3934, nnz= 46/363) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -1.548\u001b[0m \u001b[33m -1.567\u001b[0m,  89% removed (alpha= 581.7091, nnz= 39/363) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -2.646\u001b[0m \u001b[33m -2.682\u001b[0m,  93% removed (alpha=1501.3107, nnz= 27/363) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[33m -4.613\u001b[0m \u001b[33m -4.689\u001b[0m,  95% removed (alpha=3874.6751, nnz= 17/363) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.130  -0.149 -> \u001b[32m -0.129\u001b[0m \u001b[32m -0.147\u001b[0m,  12% removed (alpha=   0.3648, nnz=320/363) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.130\u001b[0m \u001b[32m -0.148\u001b[0m,  14% removed (alpha=   0.4504, nnz=312/363) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.130\u001b[0m \u001b[32m -0.150\u001b[0m,  15% removed (alpha=   0.5560, nnz=309/363) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.131\u001b[0m \u001b[32m -0.152\u001b[0m,  16% removed (alpha=   0.6864, nnz=304/363) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.133\u001b[0m \u001b[32m -0.157\u001b[0m,  21% removed (alpha=   0.8474, nnz=286/363) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.134\u001b[0m \u001b[32m -0.159\u001b[0m,  21% removed (alpha=   1.0462, nnz=286/363) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.136\u001b[0m \u001b[31m -0.164\u001b[0m,  23% removed (alpha=   1.2915, nnz=280/363) \u001b[31mover  \u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.141\u001b[0m \u001b[31m -0.172\u001b[0m,  25% removed (alpha=   1.5945, nnz=273/363) \u001b[31mover  \u001b[0m [0.2955 1.9684]\n",
      " -0.130  -0.149 -> \u001b[32m -0.135\u001b[0m \u001b[32m -0.164\u001b[0m,  17% removed (alpha=   0.9219, nnz=301/363) \u001b[32mfit ok\u001b[0m [0.8474 1.2915]\n",
      " -0.130  -0.149 -> \u001b[32m -0.136\u001b[0m \u001b[31m -0.165\u001b[0m,  18% removed (alpha=   1.0030, nnz=297/363) \u001b[31mover  \u001b[0m [0.8474 1.2915]\n",
      " -0.130  -0.149 -> \u001b[32m -0.137\u001b[0m \u001b[31m -0.166\u001b[0m,  19% removed (alpha=   1.0912, nnz=293/363) \u001b[31mover  \u001b[0m [0.8474 1.2915]\n",
      " -0.130  -0.149 -> \u001b[32m -0.137\u001b[0m \u001b[31m -0.167\u001b[0m,  20% removed (alpha=   1.1872, nnz=290/363) \u001b[31mover  \u001b[0m [0.8474 1.2915]\n",
      "METRICS orig   tr -0.130\u001b[0m, va -0.149\u001b[0m \u001b[1m[te -0.139]\u001b[0m,  ntrees 100,  nnodes 10400,  nleafs  5250,  nnz  5242\n",
      "METRICS prev   \u001b[32mtr -0.138\u001b[0m, \u001b[32mva -0.162\u001b[0m \u001b[1m\u001b[32m[te -0.147]\u001b[0m,  ntrees  24,  nnodes   702,  nleafs   363,  nnz   307\n",
      "METRICS now    \u001b[32mtr -0.133\u001b[0m, \u001b[32mva -0.157\u001b[0m \u001b[1m\u001b[32m[te -0.142]\u001b[0m,  ntrees  24,  nnodes   702,  nleafs   363,  nnz   286\n",
      "\n",
      "DONE, depth of tree reached 6, 6\n",
      "ChaahatRegTarget2 number of non-zero leaves: 5242 -> 286, test accuracy: -13.9% -> -14.2%\n",
      "\n",
      "| XGBOOST's base_score\n",
      "|   base_score diff std      3.886428325301871e-05 (!) NOT OK\n",
      "|   base_score reported      211.99051\n",
      "|   versus manually detected 211.99051048001118\n",
      "|   abs err                  4.800111810254748e-07\n",
      "|   rel err                  2.2643050437751897e-09\n",
      "|   base_score OK with relative tolerance 0.001\n",
      "\n",
      "MODEL PERF: mtr -0.141 mte -0.154 mva -0.160\n",
      "\n",
      "\n",
      "ROUND 1\n",
      "Level 0 xxtrain.shape (11478, 100) dense transform time: 0.02s, 4.96s relerr 0.1\n",
      " -0.141  -0.160 -> \u001b[32m -0.123\u001b[0m \u001b[32m -0.143\u001b[0m,   0% removed (alpha=   0.0026, nnz=100/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.123\u001b[0m \u001b[32m -0.144\u001b[0m,   1% removed (alpha=   0.0067, nnz= 99/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.124\u001b[0m \u001b[32m -0.144\u001b[0m,   2% removed (alpha=   0.0172, nnz= 98/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.124\u001b[0m \u001b[32m -0.144\u001b[0m,   5% removed (alpha=   0.0444, nnz= 95/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.125\u001b[0m \u001b[32m -0.146\u001b[0m,  14% removed (alpha=   0.1145, nnz= 86/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.128\u001b[0m \u001b[32m -0.150\u001b[0m,  28% removed (alpha=   0.2955, nnz= 72/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.137\u001b[0m \u001b[32m -0.158\u001b[0m,  49% removed (alpha=   0.7627, nnz= 51/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.150\u001b[0m \u001b[32m -0.170\u001b[0m,  66% removed (alpha=   1.9684, nnz= 34/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.170\u001b[0m \u001b[31m -0.188\u001b[0m,  72% removed (alpha=   5.0802, nnz= 28/100) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.204\u001b[0m \u001b[33m -0.219\u001b[0m,  79% removed (alpha=  13.1113, nnz= 21/100) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.254\u001b[0m \u001b[33m -0.267\u001b[0m,  86% removed (alpha=  33.8386, nnz= 14/100) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.321\u001b[0m \u001b[33m -0.333\u001b[0m,  90% removed (alpha=  87.3326, nnz= 10/100) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.446\u001b[0m \u001b[33m -0.469\u001b[0m,  91% removed (alpha= 225.3934, nnz=  9/100) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.598\u001b[0m \u001b[33m -0.621\u001b[0m,  96% removed (alpha= 581.7091, nnz=  4/100) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.732\u001b[0m \u001b[33m -0.764\u001b[0m,  98% removed (alpha=1501.3107, nnz=  2/100) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.880\u001b[0m \u001b[33m -0.922\u001b[0m,  98% removed (alpha=3874.6751, nnz=  2/100) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.140\u001b[0m \u001b[32m -0.161\u001b[0m,  51% removed (alpha=   0.9416, nnz= 49/100) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[32m -0.144\u001b[0m \u001b[32m -0.164\u001b[0m,  56% removed (alpha=   1.1624, nnz= 44/100) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[32m -0.146\u001b[0m \u001b[32m -0.167\u001b[0m,  60% removed (alpha=   1.4350, nnz= 40/100) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[32m -0.149\u001b[0m \u001b[32m -0.169\u001b[0m,  64% removed (alpha=   1.7716, nnz= 36/100) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[32m -0.152\u001b[0m \u001b[32m -0.172\u001b[0m,  66% removed (alpha=   2.1871, nnz= 34/100) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[31m -0.156\u001b[0m \u001b[32m -0.175\u001b[0m,  67% removed (alpha=   2.7001, nnz= 33/100) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[31m -0.160\u001b[0m \u001b[31m -0.179\u001b[0m,  71% removed (alpha=   3.3333, nnz= 29/100) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[31m -0.164\u001b[0m \u001b[31m -0.183\u001b[0m,  72% removed (alpha=   4.1151, nnz= 28/100) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[32m -0.153\u001b[0m \u001b[32m -0.173\u001b[0m,  66% removed (alpha=   2.3794, nnz= 34/100) \u001b[32mfit ok\u001b[0m [2.1871 3.3333]\n",
      " -0.141  -0.160 -> \u001b[32m -0.155\u001b[0m \u001b[32m -0.174\u001b[0m,  67% removed (alpha=   2.5886, nnz= 33/100) \u001b[32mfit ok\u001b[0m [2.1871 3.3333]\n",
      " -0.141  -0.160 -> \u001b[31m -0.156\u001b[0m \u001b[32m -0.176\u001b[0m,  68% removed (alpha=   2.8163, nnz= 32/100) \u001b[32mfit ok\u001b[0m [2.1871 3.3333]\n",
      " -0.141  -0.160 -> \u001b[31m -0.158\u001b[0m \u001b[31m -0.178\u001b[0m,  68% removed (alpha=   3.0639, nnz= 32/100) \u001b[33munder \u001b[0m [2.1871 3.3333]\n",
      "METRICS orig   tr -0.141\u001b[0m, va -0.160\u001b[0m \u001b[1m[te -0.154]\u001b[0m,  ntrees 100,  nnodes 10462,  nleafs  5281,  nnz  5278\n",
      "METRICS prev   \u001b[32mtr -0.141\u001b[0m, \u001b[32mva -0.160\u001b[0m \u001b[1m\u001b[32m[te -0.154]\u001b[0m,  ntrees 100,  nnodes 10462,  nleafs  5281,  nnz  5278\n",
      "METRICS now    \u001b[31mtr -0.156\u001b[0m, \u001b[32mva -0.176\u001b[0m \u001b[1m\u001b[32m[te -0.165]\u001b[0m,  ntrees  32,  nnodes  3222,  nleafs  1627,  nnz  1625\n",
      "\n",
      "Level 1 xxtrain.shape (11478, 128) dense transform time: 0.01s, 1.61s relerr 0.1\n",
      " -0.141  -0.160 -> \u001b[32m -0.131\u001b[0m \u001b[32m -0.152\u001b[0m,   5% removed (alpha=   0.0026, nnz=122/128) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.131\u001b[0m \u001b[32m -0.152\u001b[0m,   2% removed (alpha=   0.0067, nnz=125/128) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.131\u001b[0m \u001b[32m -0.152\u001b[0m,   2% removed (alpha=   0.0172, nnz=126/128) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.131\u001b[0m \u001b[32m -0.152\u001b[0m,   4% removed (alpha=   0.0444, nnz=123/128) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.132\u001b[0m \u001b[32m -0.153\u001b[0m,   5% removed (alpha=   0.1145, nnz=122/128) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.135\u001b[0m \u001b[32m -0.156\u001b[0m,   6% removed (alpha=   0.2955, nnz=120/128) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.142\u001b[0m \u001b[32m -0.161\u001b[0m,  16% removed (alpha=   0.7627, nnz=107/128) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.151\u001b[0m \u001b[32m -0.170\u001b[0m,  26% removed (alpha=   1.9684, nnz= 95/128) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.169\u001b[0m \u001b[31m -0.184\u001b[0m,  43% removed (alpha=   5.0802, nnz= 73/128) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.210\u001b[0m \u001b[33m -0.223\u001b[0m,  58% removed (alpha=  13.1113, nnz= 54/128) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.276\u001b[0m \u001b[33m -0.286\u001b[0m,  70% removed (alpha=  33.8386, nnz= 39/128) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 313.32103494794404, tolerance: 282.42865642705414\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.141  -0.160 -> \u001b[33m -0.367\u001b[0m \u001b[33m -0.373\u001b[0m,  82% removed (alpha=  87.3326, nnz= 23/128) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.541\u001b[0m \u001b[33m -0.560\u001b[0m,  88% removed (alpha= 225.3934, nnz= 15/128) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.687\u001b[0m \u001b[33m -0.715\u001b[0m,  95% removed (alpha= 581.7091, nnz=  7/128) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.804\u001b[0m \u001b[33m -0.833\u001b[0m,  97% removed (alpha=1501.3107, nnz=  4/128) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.971\u001b[0m \u001b[33m -1.018\u001b[0m,  98% removed (alpha=3874.6751, nnz=  2/128) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.144\u001b[0m \u001b[32m -0.163\u001b[0m,  22% removed (alpha=   0.9416, nnz=100/128) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[32m -0.146\u001b[0m \u001b[32m -0.165\u001b[0m,  19% removed (alpha=   1.1624, nnz=104/128) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[32m -0.148\u001b[0m \u001b[32m -0.166\u001b[0m,  23% removed (alpha=   1.4350, nnz= 99/128) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[32m -0.150\u001b[0m \u001b[32m -0.168\u001b[0m,  27% removed (alpha=   1.7716, nnz= 94/128) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[32m -0.152\u001b[0m \u001b[32m -0.170\u001b[0m,  32% removed (alpha=   2.1871, nnz= 87/128) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[32m -0.154\u001b[0m \u001b[32m -0.171\u001b[0m,  42% removed (alpha=   2.7001, nnz= 74/128) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[31m -0.158\u001b[0m \u001b[32m -0.174\u001b[0m,  42% removed (alpha=   3.3333, nnz= 74/128) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[31m -0.163\u001b[0m \u001b[31m -0.179\u001b[0m,  41% removed (alpha=   4.1151, nnz= 76/128) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[31m -0.155\u001b[0m \u001b[32m -0.172\u001b[0m,  42% removed (alpha=   2.9375, nnz= 74/128) \u001b[32mfit ok\u001b[0m [2.7001 4.1151]\n",
      " -0.141  -0.160 -> \u001b[31m -0.157\u001b[0m \u001b[32m -0.174\u001b[0m,  42% removed (alpha=   3.1958, nnz= 74/128) \u001b[32mfit ok\u001b[0m [2.7001 4.1151]\n",
      " -0.141  -0.160 -> \u001b[31m -0.158\u001b[0m \u001b[32m -0.175\u001b[0m,  42% removed (alpha=   3.4768, nnz= 74/128) \u001b[32mfit ok\u001b[0m [2.7001 4.1151]\n",
      " -0.141  -0.160 -> \u001b[31m -0.160\u001b[0m \u001b[31m -0.177\u001b[0m,  42% removed (alpha=   3.7825, nnz= 74/128) \u001b[33munder \u001b[0m [2.7001 4.1151]\n",
      "METRICS orig   tr -0.141\u001b[0m, va -0.160\u001b[0m \u001b[1m[te -0.154]\u001b[0m,  ntrees 100,  nnodes 10462,  nleafs  5281,  nnz  5278\n",
      "METRICS prev   \u001b[31mtr -0.156\u001b[0m, \u001b[32mva -0.176\u001b[0m \u001b[1m\u001b[32m[te -0.165]\u001b[0m,  ntrees  32,  nnodes  3222,  nleafs  1627,  nnz  1625\n",
      "METRICS now    \u001b[32mtr -0.154\u001b[0m, \u001b[32mva -0.171\u001b[0m \u001b[1m\u001b[32m[te -0.165]\u001b[0m,  ntrees  28,  nnodes  2182,  nleafs  1105,  nnz  1101\n",
      "\n",
      "Level 2 xxtrain.shape (11478, 182) dense transform time: 0.01s, 1.50s relerr 0.1\n",
      " -0.141  -0.160 -> \u001b[32m -0.142\u001b[0m \u001b[32m -0.163\u001b[0m,   2% removed (alpha=   0.0026, nnz=178/182) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.141\u001b[0m \u001b[32m -0.163\u001b[0m,   2% removed (alpha=   0.0067, nnz=178/182) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.141\u001b[0m \u001b[32m -0.163\u001b[0m,   3% removed (alpha=   0.0172, nnz=177/182) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.141\u001b[0m \u001b[32m -0.162\u001b[0m,   3% removed (alpha=   0.0444, nnz=177/182) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.140\u001b[0m \u001b[32m -0.162\u001b[0m,   4% removed (alpha=   0.1145, nnz=175/182) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.141\u001b[0m \u001b[32m -0.162\u001b[0m,   5% removed (alpha=   0.2955, nnz=173/182) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.134\u001b[0m \u001b[32m -0.154\u001b[0m,  16% removed (alpha=   0.7627, nnz=152/182) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.154\u001b[0m \u001b[32m -0.172\u001b[0m,  31% removed (alpha=   1.9684, nnz=126/182) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.184\u001b[0m \u001b[33m -0.199\u001b[0m,  49% removed (alpha=   5.0802, nnz= 92/182) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.233\u001b[0m \u001b[33m -0.245\u001b[0m,  60% removed (alpha=  13.1113, nnz= 72/182) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 324.2516943599198, tolerance: 282.42865642705414\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.141  -0.160 -> \u001b[33m -0.319\u001b[0m \u001b[33m -0.325\u001b[0m,  74% removed (alpha=  33.8386, nnz= 47/182) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.450\u001b[0m \u001b[33m -0.457\u001b[0m,  84% removed (alpha=  87.3326, nnz= 30/182) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.566\u001b[0m \u001b[33m -0.584\u001b[0m,  90% removed (alpha= 225.3934, nnz= 18/182) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.730\u001b[0m \u001b[33m -0.751\u001b[0m,  95% removed (alpha= 581.7091, nnz=  9/182) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.900\u001b[0m \u001b[33m -0.919\u001b[0m,  97% removed (alpha=1501.3107, nnz=  5/182) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.995\u001b[0m \u001b[33m -1.028\u001b[0m,  97% removed (alpha=3874.6751, nnz=  5/182) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.137\u001b[0m \u001b[32m -0.156\u001b[0m,  21% removed (alpha=   0.9416, nnz=143/182) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[32m -0.139\u001b[0m \u001b[32m -0.160\u001b[0m,  26% removed (alpha=   1.1624, nnz=135/182) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[32m -0.143\u001b[0m \u001b[32m -0.163\u001b[0m,  30% removed (alpha=   1.4350, nnz=127/182) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[32m -0.148\u001b[0m \u001b[32m -0.168\u001b[0m,  32% removed (alpha=   1.7716, nnz=124/182) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[32m -0.154\u001b[0m \u001b[32m -0.173\u001b[0m,  35% removed (alpha=   2.1871, nnz=118/182) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[31m -0.161\u001b[0m \u001b[31m -0.179\u001b[0m,  42% removed (alpha=   2.7001, nnz=106/182) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[31m -0.168\u001b[0m \u001b[31m -0.186\u001b[0m,  48% removed (alpha=   3.3333, nnz= 95/182) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[33m -0.176\u001b[0m \u001b[33m -0.192\u001b[0m,  50% removed (alpha=   4.1151, nnz= 91/182) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[32m -0.151\u001b[0m \u001b[32m -0.170\u001b[0m,  36% removed (alpha=   1.9274, nnz=117/182) \u001b[32mfit ok\u001b[0m [1.7716 2.7001]\n",
      " -0.141  -0.160 -> \u001b[32m -0.153\u001b[0m \u001b[32m -0.172\u001b[0m,  36% removed (alpha=   2.0969, nnz=117/182) \u001b[32mfit ok\u001b[0m [1.7716 2.7001]\n",
      " -0.141  -0.160 -> \u001b[31m -0.156\u001b[0m \u001b[32m -0.174\u001b[0m,  37% removed (alpha=   2.2812, nnz=114/182) \u001b[32mfit ok\u001b[0m [1.7716 2.7001]\n",
      " -0.141  -0.160 -> \u001b[31m -0.158\u001b[0m \u001b[31m -0.177\u001b[0m,  40% removed (alpha=   2.4818, nnz=110/182) \u001b[33munder \u001b[0m [1.7716 2.7001]\n",
      "METRICS orig   tr -0.141\u001b[0m, va -0.160\u001b[0m \u001b[1m[te -0.154]\u001b[0m,  ntrees 100,  nnodes 10462,  nleafs  5281,  nnz  5278\n",
      "METRICS prev   \u001b[32mtr -0.154\u001b[0m, \u001b[32mva -0.171\u001b[0m \u001b[1m\u001b[32m[te -0.165]\u001b[0m,  ntrees  28,  nnodes  2182,  nleafs  1105,  nnz  1101\n",
      "METRICS now    \u001b[31mtr -0.156\u001b[0m, \u001b[32mva -0.174\u001b[0m \u001b[1m\u001b[31m[te -0.169]\u001b[0m,  ntrees  28,  nnodes  1790,  nleafs   909,  nnz   899\n",
      "\n",
      "Level 3 xxtrain.shape (11478, 294) dense transform time: 0.01s, 1.97s relerr 0.1\n",
      " -0.141  -0.160 -> \u001b[32m -0.133\u001b[0m \u001b[32m -0.151\u001b[0m,   1% removed (alpha=   0.0026, nnz=290/294) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.133\u001b[0m \u001b[32m -0.150\u001b[0m,   1% removed (alpha=   0.0067, nnz=290/294) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.132\u001b[0m \u001b[32m -0.149\u001b[0m,   2% removed (alpha=   0.0172, nnz=289/294) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.132\u001b[0m \u001b[32m -0.150\u001b[0m,   3% removed (alpha=   0.0444, nnz=284/294) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.132\u001b[0m \u001b[32m -0.151\u001b[0m,   9% removed (alpha=   0.1145, nnz=268/294) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.135\u001b[0m \u001b[32m -0.154\u001b[0m,  17% removed (alpha=   0.2955, nnz=245/294) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 352.487379732132, tolerance: 282.42865642705414\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 492.44569457069815, tolerance: 282.42865642705414\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.141  -0.160 -> \u001b[32m -0.146\u001b[0m \u001b[32m -0.164\u001b[0m,  27% removed (alpha=   0.7627, nnz=214/294) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[31m -0.167\u001b[0m \u001b[31m -0.187\u001b[0m,  42% removed (alpha=   1.9684, nnz=171/294) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.205\u001b[0m \u001b[33m -0.222\u001b[0m,  53% removed (alpha=   5.0802, nnz=138/294) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.283\u001b[0m \u001b[33m -0.298\u001b[0m,  62% removed (alpha=  13.1113, nnz=112/294) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.408\u001b[0m \u001b[33m -0.427\u001b[0m,  74% removed (alpha=  33.8386, nnz= 76/294) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.534\u001b[0m \u001b[33m -0.560\u001b[0m,  82% removed (alpha=  87.3326, nnz= 53/294) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.793\u001b[0m \u001b[33m -0.825\u001b[0m,  88% removed (alpha= 225.3934, nnz= 36/294) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -1.245\u001b[0m \u001b[33m -1.260\u001b[0m,  94% removed (alpha= 581.7091, nnz= 19/294) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -1.910\u001b[0m \u001b[33m -1.857\u001b[0m,  96% removed (alpha=1501.3107, nnz= 11/294) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -2.323\u001b[0m \u001b[33m -2.305\u001b[0m,  97% removed (alpha=3874.6751, nnz= 10/294) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.151\u001b[0m \u001b[32m -0.171\u001b[0m,  21% removed (alpha=   0.3648, nnz=232/294) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.141  -0.160 -> \u001b[32m -0.138\u001b[0m \u001b[32m -0.159\u001b[0m,  24% removed (alpha=   0.4504, nnz=222/294) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.141  -0.160 -> \u001b[32m -0.141\u001b[0m \u001b[32m -0.161\u001b[0m,  26% removed (alpha=   0.5560, nnz=219/294) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.141  -0.160 -> \u001b[32m -0.144\u001b[0m \u001b[32m -0.165\u001b[0m,  25% removed (alpha=   0.6864, nnz=220/294) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.141  -0.160 -> \u001b[32m -0.149\u001b[0m \u001b[32m -0.169\u001b[0m,  29% removed (alpha=   0.8474, nnz=209/294) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316.30010979673904, tolerance: 282.42865642705414\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.141  -0.160 -> \u001b[32m -0.151\u001b[0m \u001b[32m -0.170\u001b[0m,  33% removed (alpha=   1.0462, nnz=197/294) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.141  -0.160 -> \u001b[31m -0.155\u001b[0m \u001b[32m -0.173\u001b[0m,  39% removed (alpha=   1.2915, nnz=180/294) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.141  -0.160 -> \u001b[31m -0.159\u001b[0m \u001b[31m -0.177\u001b[0m,  41% removed (alpha=   1.5945, nnz=174/294) \u001b[33munder \u001b[0m [0.2955 1.9684]\n",
      " -0.141  -0.160 -> \u001b[32m -0.153\u001b[0m \u001b[32m -0.171\u001b[0m,  35% removed (alpha=   1.1382, nnz=190/294) \u001b[32mfit ok\u001b[0m [1.0462 1.5945]\n",
      " -0.141  -0.160 -> \u001b[32m -0.154\u001b[0m \u001b[32m -0.172\u001b[0m,  36% removed (alpha=   1.2383, nnz=187/294) \u001b[32mfit ok\u001b[0m [1.0462 1.5945]\n",
      " -0.141  -0.160 -> \u001b[31m -0.156\u001b[0m \u001b[32m -0.174\u001b[0m,  38% removed (alpha=   1.3471, nnz=182/294) \u001b[32mfit ok\u001b[0m [1.0462 1.5945]\n",
      " -0.141  -0.160 -> \u001b[31m -0.157\u001b[0m \u001b[32m -0.175\u001b[0m,  39% removed (alpha=   1.4656, nnz=179/294) \u001b[32mfit ok\u001b[0m [1.0462 1.5945]\n",
      "METRICS orig   tr -0.141\u001b[0m, va -0.160\u001b[0m \u001b[1m[te -0.154]\u001b[0m,  ntrees 100,  nnodes 10462,  nleafs  5281,  nnz  5278\n",
      "METRICS prev   \u001b[31mtr -0.156\u001b[0m, \u001b[32mva -0.174\u001b[0m \u001b[1m\u001b[31m[te -0.169]\u001b[0m,  ntrees  28,  nnodes  1790,  nleafs   909,  nnz   899\n",
      "METRICS now    \u001b[31mtr -0.157\u001b[0m, \u001b[32mva -0.175\u001b[0m \u001b[1m\u001b[31m[te -0.174]\u001b[0m,  ntrees  28,  nnodes  1292,  nleafs   660,  nnz   634\n",
      "\n",
      "Level 4 xxtrain.shape (11478, 407) dense transform time: 0.01s, 2.02s relerr 0.1\n",
      " -0.141  -0.160 -> \u001b[32m -0.143\u001b[0m \u001b[32m -0.159\u001b[0m,   1% removed (alpha=   0.0026, nnz=404/407) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.142\u001b[0m \u001b[32m -0.158\u001b[0m,   1% removed (alpha=   0.0067, nnz=404/407) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.141\u001b[0m \u001b[32m -0.158\u001b[0m,   2% removed (alpha=   0.0172, nnz=400/407) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.140\u001b[0m \u001b[32m -0.157\u001b[0m,   3% removed (alpha=   0.0444, nnz=395/407) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.140\u001b[0m \u001b[32m -0.158\u001b[0m,   9% removed (alpha=   0.1145, nnz=372/407) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.143\u001b[0m \u001b[32m -0.162\u001b[0m,  19% removed (alpha=   0.2955, nnz=330/407) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 353.5599931066522, tolerance: 282.42865642705414\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.141  -0.160 -> \u001b[32m -0.152\u001b[0m \u001b[32m -0.173\u001b[0m,  35% removed (alpha=   0.7627, nnz=266/407) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 457.57965499460823, tolerance: 282.42865642705414\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.141  -0.160 -> \u001b[33m -0.177\u001b[0m \u001b[33m -0.203\u001b[0m,  43% removed (alpha=   1.9684, nnz=233/407) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.219\u001b[0m \u001b[33m -0.247\u001b[0m,  55% removed (alpha=   5.0802, nnz=183/407) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.295\u001b[0m \u001b[33m -0.325\u001b[0m,  66% removed (alpha=  13.1113, nnz=139/407) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.421\u001b[0m \u001b[33m -0.454\u001b[0m,  75% removed (alpha=  33.8386, nnz=101/407) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.631\u001b[0m \u001b[33m -0.634\u001b[0m,  84% removed (alpha=  87.3326, nnz= 67/407) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.941\u001b[0m \u001b[33m -0.903\u001b[0m,  87% removed (alpha= 225.3934, nnz= 51/407) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -1.291\u001b[0m \u001b[33m -1.218\u001b[0m,  93% removed (alpha= 581.7091, nnz= 27/407) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -2.110\u001b[0m \u001b[33m -2.014\u001b[0m,  97% removed (alpha=1501.3107, nnz= 14/407) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -3.543\u001b[0m \u001b[33m -3.425\u001b[0m,  98% removed (alpha=3874.6751, nnz= 10/407) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.143\u001b[0m \u001b[32m -0.161\u001b[0m,  26% removed (alpha=   0.3648, nnz=302/407) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.141  -0.160 -> \u001b[32m -0.145\u001b[0m \u001b[32m -0.163\u001b[0m,  26% removed (alpha=   0.4504, nnz=301/407) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.141  -0.160 -> \u001b[32m -0.148\u001b[0m \u001b[32m -0.167\u001b[0m,  26% removed (alpha=   0.5560, nnz=301/407) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.141  -0.160 -> \u001b[32m -0.151\u001b[0m \u001b[32m -0.170\u001b[0m,  29% removed (alpha=   0.6864, nnz=288/407) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.141  -0.160 -> \u001b[32m -0.154\u001b[0m \u001b[32m -0.174\u001b[0m,  32% removed (alpha=   0.8474, nnz=276/407) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.141  -0.160 -> \u001b[31m -0.158\u001b[0m \u001b[31m -0.179\u001b[0m,  34% removed (alpha=   1.0462, nnz=267/407) \u001b[33munder \u001b[0m [0.2955 1.9684]\n",
      " -0.141  -0.160 -> \u001b[31m -0.164\u001b[0m \u001b[31m -0.185\u001b[0m,  39% removed (alpha=   1.2915, nnz=248/407) \u001b[33munder \u001b[0m [0.2955 1.9684]\n",
      " -0.141  -0.160 -> \u001b[33m -0.170\u001b[0m \u001b[33m -0.192\u001b[0m,  41% removed (alpha=   1.5945, nnz=240/407) \u001b[33munder \u001b[0m [0.2955 1.9684]\n",
      " -0.141  -0.160 -> \u001b[32m -0.154\u001b[0m \u001b[32m -0.173\u001b[0m,  32% removed (alpha=   0.7468, nnz=277/407) \u001b[32mfit ok\u001b[0m [0.6864 1.0462]\n",
      " -0.141  -0.160 -> \u001b[31m -0.155\u001b[0m \u001b[32m -0.174\u001b[0m,  33% removed (alpha=   0.8125, nnz=273/407) \u001b[32mfit ok\u001b[0m [0.6864 1.0462]\n",
      " -0.141  -0.160 -> \u001b[31m -0.156\u001b[0m \u001b[32m -0.176\u001b[0m,  33% removed (alpha=   0.8839, nnz=271/407) \u001b[32mfit ok\u001b[0m [0.6864 1.0462]\n",
      " -0.141  -0.160 -> \u001b[31m -0.157\u001b[0m \u001b[31m -0.177\u001b[0m,  33% removed (alpha=   0.9616, nnz=273/407) \u001b[33munder \u001b[0m [0.6864 1.0462]\n",
      "METRICS orig   tr -0.141\u001b[0m, va -0.160\u001b[0m \u001b[1m[te -0.154]\u001b[0m,  ntrees 100,  nnodes 10462,  nleafs  5281,  nnz  5278\n",
      "METRICS prev   \u001b[31mtr -0.157\u001b[0m, \u001b[32mva -0.175\u001b[0m \u001b[1m\u001b[31m[te -0.174]\u001b[0m,  ntrees  28,  nnodes  1292,  nleafs   660,  nnz   634\n",
      "METRICS now    \u001b[32mtr -0.152\u001b[0m, \u001b[32mva -0.173\u001b[0m \u001b[1m\u001b[32m[te -0.163]\u001b[0m,  ntrees  27,  nnodes  1063,  nleafs   545,  nnz   504\n",
      "\n",
      "Level 5 xxtrain.shape (11478, 545) dense transform time: 0.00s, 1.93s relerr 0.1\n",
      " -0.141  -0.160 -> \u001b[32m -0.130\u001b[0m \u001b[32m -0.150\u001b[0m,   2% removed (alpha=   0.0026, nnz=533/545) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.128\u001b[0m \u001b[32m -0.148\u001b[0m,   4% removed (alpha=   0.0067, nnz=521/545) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.128\u001b[0m \u001b[32m -0.148\u001b[0m,   6% removed (alpha=   0.0172, nnz=514/545) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.128\u001b[0m \u001b[32m -0.148\u001b[0m,   8% removed (alpha=   0.0444, nnz=499/545) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.130\u001b[0m \u001b[32m -0.151\u001b[0m,  18% removed (alpha=   0.1145, nnz=446/545) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.136\u001b[0m \u001b[32m -0.157\u001b[0m,  27% removed (alpha=   0.2955, nnz=396/545) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 356.90487207964327, tolerance: 282.42865642705414\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.141  -0.160 -> \u001b[32m -0.150\u001b[0m \u001b[32m -0.173\u001b[0m,  38% removed (alpha=   0.7627, nnz=337/545) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 410.82870456126705, tolerance: 282.42865642705414\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.141  -0.160 -> \u001b[33m -0.171\u001b[0m \u001b[33m -0.196\u001b[0m,  50% removed (alpha=   1.9684, nnz=275/545) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.210\u001b[0m \u001b[33m -0.228\u001b[0m,  59% removed (alpha=   5.0802, nnz=224/545) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.279\u001b[0m \u001b[33m -0.298\u001b[0m,  68% removed (alpha=  13.1113, nnz=175/545) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.403\u001b[0m \u001b[33m -0.429\u001b[0m,  77% removed (alpha=  33.8386, nnz=127/545) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.600\u001b[0m \u001b[33m -0.627\u001b[0m,  86% removed (alpha=  87.3326, nnz= 78/545) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.870\u001b[0m \u001b[33m -0.875\u001b[0m,  91% removed (alpha= 225.3934, nnz= 51/545) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -1.315\u001b[0m \u001b[33m -1.285\u001b[0m,  95% removed (alpha= 581.7091, nnz= 29/545) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -2.059\u001b[0m \u001b[33m -1.973\u001b[0m,  97% removed (alpha=1501.3107, nnz= 14/545) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -3.338\u001b[0m \u001b[33m -3.237\u001b[0m,  99% removed (alpha=3874.6751, nnz=  8/545) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.138\u001b[0m \u001b[32m -0.160\u001b[0m,  34% removed (alpha=   0.3648, nnz=358/545) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.141  -0.160 -> \u001b[32m -0.140\u001b[0m \u001b[32m -0.163\u001b[0m,  35% removed (alpha=   0.4504, nnz=355/545) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.141  -0.160 -> \u001b[32m -0.142\u001b[0m \u001b[32m -0.166\u001b[0m,  36% removed (alpha=   0.5560, nnz=347/545) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.141  -0.160 -> \u001b[32m -0.146\u001b[0m \u001b[32m -0.170\u001b[0m,  38% removed (alpha=   0.6864, nnz=336/545) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.141  -0.160 -> \u001b[32m -0.152\u001b[0m \u001b[31m -0.177\u001b[0m,  40% removed (alpha=   0.8474, nnz=325/545) \u001b[31mover  \u001b[0m [0.2955 1.9684]\n",
      " -0.141  -0.160 -> \u001b[31m -0.157\u001b[0m \u001b[31m -0.182\u001b[0m,  42% removed (alpha=   1.0462, nnz=316/545) \u001b[33munder \u001b[0m [0.2955 1.9684]\n",
      " -0.141  -0.160 -> \u001b[31m -0.162\u001b[0m \u001b[31m -0.186\u001b[0m,  43% removed (alpha=   1.2915, nnz=308/545) \u001b[33munder \u001b[0m [0.2955 1.9684]\n",
      " -0.141  -0.160 -> \u001b[31m -0.167\u001b[0m \u001b[31m -0.190\u001b[0m,  48% removed (alpha=   1.5945, nnz=284/545) \u001b[33munder \u001b[0m [0.2955 1.9684]\n",
      " -0.141  -0.160 -> \u001b[32m -0.147\u001b[0m \u001b[32m -0.170\u001b[0m,  37% removed (alpha=   0.6049, nnz=345/545) \u001b[32mfit ok\u001b[0m [0.556  0.8474]\n",
      " -0.141  -0.160 -> \u001b[32m -0.148\u001b[0m \u001b[32m -0.171\u001b[0m,  37% removed (alpha=   0.6581, nnz=344/545) \u001b[32mfit ok\u001b[0m [0.556  0.8474]\n",
      " -0.141  -0.160 -> \u001b[32m -0.149\u001b[0m \u001b[32m -0.173\u001b[0m,  37% removed (alpha=   0.7160, nnz=343/545) \u001b[32mfit ok\u001b[0m [0.556  0.8474]\n",
      " -0.141  -0.160 -> \u001b[32m -0.151\u001b[0m \u001b[32m -0.174\u001b[0m,  38% removed (alpha=   0.7789, nnz=339/545) \u001b[32mfit ok\u001b[0m [0.556  0.8474]\n",
      "METRICS orig   tr -0.141\u001b[0m, va -0.160\u001b[0m \u001b[1m[te -0.154]\u001b[0m,  ntrees 100,  nnodes 10462,  nleafs  5281,  nnz  5278\n",
      "METRICS prev   \u001b[32mtr -0.152\u001b[0m, \u001b[32mva -0.173\u001b[0m \u001b[1m\u001b[32m[te -0.163]\u001b[0m,  ntrees  27,  nnodes  1063,  nleafs   545,  nnz   504\n",
      "METRICS now    \u001b[32mtr -0.146\u001b[0m, \u001b[32mva -0.170\u001b[0m \u001b[1m\u001b[32m[te -0.158]\u001b[0m,  ntrees  27,  nnodes   857,  nleafs   442,  nnz   369\n",
      "\n",
      "Level 6 xxtrain.shape (11478, 442) dense transform time: 0.00s, 1.28s relerr 0.1\n",
      " -0.141  -0.160 -> \u001b[32m -0.134\u001b[0m \u001b[32m -0.156\u001b[0m,   1% removed (alpha=   0.0026, nnz=437/442) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.134\u001b[0m \u001b[32m -0.156\u001b[0m,   1% removed (alpha=   0.0067, nnz=437/442) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.134\u001b[0m \u001b[32m -0.156\u001b[0m,   2% removed (alpha=   0.0172, nnz=435/442) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.134\u001b[0m \u001b[32m -0.156\u001b[0m,   3% removed (alpha=   0.0444, nnz=430/442) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.134\u001b[0m \u001b[32m -0.156\u001b[0m,   7% removed (alpha=   0.1145, nnz=409/442) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.135\u001b[0m \u001b[32m -0.160\u001b[0m,  14% removed (alpha=   0.2955, nnz=380/442) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.139\u001b[0m \u001b[32m -0.164\u001b[0m,  26% removed (alpha=   0.7627, nnz=327/442) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 326.82955674299285, tolerance: 282.42865642705414\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.141  -0.160 -> \u001b[32m -0.149\u001b[0m \u001b[32m -0.168\u001b[0m,  32% removed (alpha=   1.9684, nnz=301/442) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.184\u001b[0m \u001b[33m -0.196\u001b[0m,  42% removed (alpha=   5.0802, nnz=257/442) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.257\u001b[0m \u001b[33m -0.267\u001b[0m,  55% removed (alpha=  13.1113, nnz=201/442) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.394\u001b[0m \u001b[33m -0.409\u001b[0m,  67% removed (alpha=  33.8386, nnz=147/442) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.618\u001b[0m \u001b[33m -0.642\u001b[0m,  78% removed (alpha=  87.3326, nnz= 96/442) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.967\u001b[0m \u001b[33m -0.976\u001b[0m,  89% removed (alpha= 225.3934, nnz= 50/442) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -1.571\u001b[0m \u001b[33m -1.566\u001b[0m,  90% removed (alpha= 581.7091, nnz= 42/442) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -2.635\u001b[0m \u001b[33m -2.598\u001b[0m,  95% removed (alpha=1501.3107, nnz= 23/442) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -3.936\u001b[0m \u001b[33m -3.878\u001b[0m,  99% removed (alpha=3874.6751, nnz=  5/442) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.140\u001b[0m \u001b[32m -0.162\u001b[0m,  28% removed (alpha=   0.9416, nnz=318/442) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[32m -0.142\u001b[0m \u001b[32m -0.163\u001b[0m,  30% removed (alpha=   1.1624, nnz=309/442) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[32m -0.144\u001b[0m \u001b[32m -0.164\u001b[0m,  31% removed (alpha=   1.4350, nnz=305/442) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[32m -0.147\u001b[0m \u001b[32m -0.167\u001b[0m,  31% removed (alpha=   1.7716, nnz=304/442) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[32m -0.152\u001b[0m \u001b[32m -0.170\u001b[0m,  33% removed (alpha=   2.1871, nnz=295/442) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[31m -0.159\u001b[0m \u001b[32m -0.176\u001b[0m,  36% removed (alpha=   2.7001, nnz=281/442) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[31m -0.166\u001b[0m \u001b[31m -0.182\u001b[0m,  38% removed (alpha=   3.3333, nnz=272/442) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[33m -0.175\u001b[0m \u001b[31m -0.189\u001b[0m,  41% removed (alpha=   4.1151, nnz=261/442) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[31m -0.157\u001b[0m \u001b[32m -0.173\u001b[0m,  33% removed (alpha=   2.3794, nnz=295/442) \u001b[32mfit ok\u001b[0m [2.1871 3.3333]\n",
      " -0.141  -0.160 -> \u001b[31m -0.159\u001b[0m \u001b[32m -0.174\u001b[0m,  33% removed (alpha=   2.5886, nnz=295/442) \u001b[32mfit ok\u001b[0m [2.1871 3.3333]\n",
      " -0.141  -0.160 -> \u001b[31m -0.161\u001b[0m \u001b[31m -0.176\u001b[0m,  35% removed (alpha=   2.8163, nnz=287/442) \u001b[33munder \u001b[0m [2.1871 3.3333]\n",
      " -0.141  -0.160 -> \u001b[31m -0.163\u001b[0m \u001b[31m -0.178\u001b[0m,  36% removed (alpha=   3.0639, nnz=284/442) \u001b[33munder \u001b[0m [2.1871 3.3333]\n",
      "METRICS orig   tr -0.141\u001b[0m, va -0.160\u001b[0m \u001b[1m[te -0.154]\u001b[0m,  ntrees 100,  nnodes 10462,  nleafs  5281,  nnz  5278\n",
      "METRICS prev   \u001b[32mtr -0.146\u001b[0m, \u001b[32mva -0.170\u001b[0m \u001b[1m\u001b[32m[te -0.158]\u001b[0m,  ntrees  27,  nnodes   857,  nleafs   442,  nnz   369\n",
      "METRICS now    \u001b[31mtr -0.159\u001b[0m, \u001b[32mva -0.176\u001b[0m \u001b[1m\u001b[31m[te -0.173]\u001b[0m,  ntrees  26,  nnodes   822,  nleafs   424,  nnz   281\n",
      "\n",
      "DONE, depth of tree reached 6, 6\n",
      "\n",
      "\n",
      "ROUND 2\n",
      "Level 0 xxtrain.shape (11478, 26) dense transform time: 0.00s, 1.30s relerr 0.1\n",
      " -0.141  -0.160 -> \u001b[31m -0.157\u001b[0m \u001b[32m -0.172\u001b[0m,   0% removed (alpha=   0.0026, nnz=26/26) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[31m -0.157\u001b[0m \u001b[32m -0.172\u001b[0m,   0% removed (alpha=   0.0067, nnz=26/26) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[31m -0.157\u001b[0m \u001b[32m -0.172\u001b[0m,   0% removed (alpha=   0.0172, nnz=26/26) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[31m -0.157\u001b[0m \u001b[32m -0.172\u001b[0m,   0% removed (alpha=   0.0444, nnz=26/26) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[31m -0.157\u001b[0m \u001b[32m -0.172\u001b[0m,   0% removed (alpha=   0.1145, nnz=26/26) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[31m -0.157\u001b[0m \u001b[32m -0.172\u001b[0m,   0% removed (alpha=   0.2955, nnz=26/26) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[31m -0.157\u001b[0m \u001b[32m -0.173\u001b[0m,   0% removed (alpha=   0.7627, nnz=26/26) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[31m -0.160\u001b[0m \u001b[32m -0.175\u001b[0m,   0% removed (alpha=   1.9684, nnz=26/26) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[31m -0.162\u001b[0m \u001b[31m -0.178\u001b[0m,  15% removed (alpha=   5.0802, nnz=22/26) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.171\u001b[0m \u001b[31m -0.188\u001b[0m,  23% removed (alpha=  13.1113, nnz=20/26) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.201\u001b[0m \u001b[33m -0.222\u001b[0m,  23% removed (alpha=  33.8386, nnz=20/26) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.301\u001b[0m \u001b[33m -0.326\u001b[0m,  42% removed (alpha=  87.3326, nnz=15/26) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.418\u001b[0m \u001b[33m -0.443\u001b[0m,  58% removed (alpha= 225.3934, nnz=11/26) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.541\u001b[0m \u001b[33m -0.564\u001b[0m,  69% removed (alpha= 581.7091, nnz= 8/26) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.715\u001b[0m \u001b[33m -0.724\u001b[0m,  73% removed (alpha=1501.3107, nnz= 7/26) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -1.070\u001b[0m \u001b[33m -1.068\u001b[0m,  77% removed (alpha=3874.6751, nnz= 6/26) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[31m -0.157\u001b[0m \u001b[32m -0.173\u001b[0m,   0% removed (alpha=   0.9416, nnz=26/26) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[31m -0.158\u001b[0m \u001b[32m -0.173\u001b[0m,   0% removed (alpha=   1.1624, nnz=26/26) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[31m -0.158\u001b[0m \u001b[32m -0.174\u001b[0m,   0% removed (alpha=   1.4350, nnz=26/26) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[31m -0.159\u001b[0m \u001b[32m -0.174\u001b[0m,   0% removed (alpha=   1.7716, nnz=26/26) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[31m -0.160\u001b[0m \u001b[32m -0.175\u001b[0m,  12% removed (alpha=   2.1871, nnz=23/26) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[31m -0.161\u001b[0m \u001b[32m -0.176\u001b[0m,  12% removed (alpha=   2.7001, nnz=23/26) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[31m -0.161\u001b[0m \u001b[31m -0.176\u001b[0m,  15% removed (alpha=   3.3333, nnz=22/26) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[31m -0.162\u001b[0m \u001b[31m -0.177\u001b[0m,  15% removed (alpha=   4.1151, nnz=22/26) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[31m -0.160\u001b[0m \u001b[32m -0.175\u001b[0m,  12% removed (alpha=   2.3794, nnz=23/26) \u001b[32mfit ok\u001b[0m [2.1871 3.3333]\n",
      " -0.141  -0.160 -> \u001b[31m -0.160\u001b[0m \u001b[32m -0.176\u001b[0m,  12% removed (alpha=   2.5886, nnz=23/26) \u001b[32mfit ok\u001b[0m [2.1871 3.3333]\n",
      " -0.141  -0.160 -> \u001b[31m -0.161\u001b[0m \u001b[31m -0.176\u001b[0m,  15% removed (alpha=   2.8163, nnz=22/26) \u001b[33munder \u001b[0m [2.1871 3.3333]\n",
      " -0.141  -0.160 -> \u001b[31m -0.161\u001b[0m \u001b[31m -0.176\u001b[0m,  15% removed (alpha=   3.0639, nnz=22/26) \u001b[33munder \u001b[0m [2.1871 3.3333]\n",
      "METRICS orig   tr -0.141\u001b[0m, va -0.160\u001b[0m \u001b[1m[te -0.154]\u001b[0m,  ntrees 100,  nnodes 10462,  nleafs  5281,  nnz  5278\n",
      "METRICS prev   \u001b[31mtr -0.159\u001b[0m, \u001b[32mva -0.176\u001b[0m \u001b[1m\u001b[31m[te -0.173]\u001b[0m,  ntrees  26,  nnodes   822,  nleafs   424,  nnz   281\n",
      "METRICS now    \u001b[31mtr -0.160\u001b[0m, \u001b[32mva -0.175\u001b[0m \u001b[1m\u001b[31m[te -0.175]\u001b[0m,  ntrees  23,  nnodes   787,  nleafs   405,  nnz   271\n",
      "\n",
      "Level 1 xxtrain.shape (11478, 82) dense transform time: 0.00s, 1.17s relerr 0.1\n",
      " -0.141  -0.160 -> \u001b[31m -0.158\u001b[0m \u001b[32m -0.174\u001b[0m,   4% removed (alpha=   0.0026, nnz=79/82) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[31m -0.158\u001b[0m \u001b[32m -0.174\u001b[0m,   5% removed (alpha=   0.0067, nnz=78/82) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[31m -0.158\u001b[0m \u001b[32m -0.174\u001b[0m,   5% removed (alpha=   0.0172, nnz=78/82) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[31m -0.158\u001b[0m \u001b[32m -0.174\u001b[0m,   4% removed (alpha=   0.0444, nnz=79/82) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[31m -0.158\u001b[0m \u001b[32m -0.174\u001b[0m,   9% removed (alpha=   0.1145, nnz=75/82) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[31m -0.158\u001b[0m \u001b[32m -0.174\u001b[0m,   5% removed (alpha=   0.2955, nnz=78/82) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[31m -0.159\u001b[0m \u001b[32m -0.175\u001b[0m,   9% removed (alpha=   0.7627, nnz=75/82) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[31m -0.160\u001b[0m \u001b[32m -0.176\u001b[0m,  17% removed (alpha=   1.9684, nnz=68/82) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[31m -0.165\u001b[0m \u001b[31m -0.181\u001b[0m,  13% removed (alpha=   5.0802, nnz=71/82) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.184\u001b[0m \u001b[33m -0.200\u001b[0m,  28% removed (alpha=  13.1113, nnz=59/82) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.250\u001b[0m \u001b[33m -0.266\u001b[0m,  39% removed (alpha=  33.8386, nnz=50/82) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.354\u001b[0m \u001b[33m -0.369\u001b[0m,  60% removed (alpha=  87.3326, nnz=33/82) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.504\u001b[0m \u001b[33m -0.523\u001b[0m,  76% removed (alpha= 225.3934, nnz=20/82) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.639\u001b[0m \u001b[33m -0.662\u001b[0m,  84% removed (alpha= 581.7091, nnz=13/82) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.984\u001b[0m \u001b[33m -1.003\u001b[0m,  85% removed (alpha=1501.3107, nnz=12/82) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -1.835\u001b[0m \u001b[33m -1.827\u001b[0m,  90% removed (alpha=3874.6751, nnz= 8/82) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[31m -0.159\u001b[0m \u001b[32m -0.175\u001b[0m,  12% removed (alpha=   0.9416, nnz=72/82) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[31m -0.159\u001b[0m \u001b[32m -0.175\u001b[0m,   7% removed (alpha=   1.1624, nnz=76/82) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[31m -0.159\u001b[0m \u001b[32m -0.175\u001b[0m,   9% removed (alpha=   1.4350, nnz=75/82) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[31m -0.160\u001b[0m \u001b[32m -0.175\u001b[0m,  15% removed (alpha=   1.7716, nnz=70/82) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[31m -0.160\u001b[0m \u001b[32m -0.176\u001b[0m,  18% removed (alpha=   2.1871, nnz=67/82) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[31m -0.161\u001b[0m \u001b[31m -0.177\u001b[0m,  21% removed (alpha=   2.7001, nnz=65/82) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[31m -0.162\u001b[0m \u001b[31m -0.178\u001b[0m,  18% removed (alpha=   3.3333, nnz=67/82) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[31m -0.164\u001b[0m \u001b[31m -0.179\u001b[0m,  18% removed (alpha=   4.1151, nnz=67/82) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[31m -0.160\u001b[0m \u001b[32m -0.176\u001b[0m,  11% removed (alpha=   1.9274, nnz=73/82) \u001b[32mfit ok\u001b[0m [1.7716 2.7001]\n",
      " -0.141  -0.160 -> \u001b[31m -0.160\u001b[0m \u001b[32m -0.176\u001b[0m,  12% removed (alpha=   2.0969, nnz=72/82) \u001b[32mfit ok\u001b[0m [1.7716 2.7001]\n",
      " -0.141  -0.160 -> \u001b[31m -0.160\u001b[0m \u001b[31m -0.176\u001b[0m,  11% removed (alpha=   2.2812, nnz=73/82) \u001b[33munder \u001b[0m [1.7716 2.7001]\n",
      " -0.141  -0.160 -> \u001b[31m -0.161\u001b[0m \u001b[31m -0.176\u001b[0m,  11% removed (alpha=   2.4818, nnz=73/82) \u001b[33munder \u001b[0m [1.7716 2.7001]\n",
      "METRICS orig   tr -0.141\u001b[0m, va -0.160\u001b[0m \u001b[1m[te -0.154]\u001b[0m,  ntrees 100,  nnodes 10462,  nleafs  5281,  nnz  5278\n",
      "METRICS prev   \u001b[31mtr -0.160\u001b[0m, \u001b[32mva -0.175\u001b[0m \u001b[1m\u001b[31m[te -0.175]\u001b[0m,  ntrees  23,  nnodes   787,  nleafs   405,  nnz   271\n",
      "METRICS now    \u001b[31mtr -0.160\u001b[0m, \u001b[32mva -0.176\u001b[0m \u001b[1m\u001b[31m[te -0.175]\u001b[0m,  ntrees  23,  nnodes   781,  nleafs   402,  nnz   348\n",
      "\n",
      "Level 2 xxtrain.shape (11478, 138) dense transform time: 0.00s, 1.22s relerr 0.1\n",
      " -0.141  -0.160 -> \u001b[32m -0.154\u001b[0m \u001b[32m -0.170\u001b[0m,   4% removed (alpha=   0.0026, nnz=133/138) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.154\u001b[0m \u001b[32m -0.170\u001b[0m,   4% removed (alpha=   0.0067, nnz=133/138) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.154\u001b[0m \u001b[32m -0.170\u001b[0m,   5% removed (alpha=   0.0172, nnz=131/138) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.154\u001b[0m \u001b[32m -0.171\u001b[0m,   5% removed (alpha=   0.0444, nnz=131/138) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.154\u001b[0m \u001b[32m -0.171\u001b[0m,   7% removed (alpha=   0.1145, nnz=128/138) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[31m -0.155\u001b[0m \u001b[32m -0.171\u001b[0m,   9% removed (alpha=   0.2955, nnz=125/138) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[31m -0.156\u001b[0m \u001b[32m -0.173\u001b[0m,  18% removed (alpha=   0.7627, nnz=113/138) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[31m -0.163\u001b[0m \u001b[31m -0.179\u001b[0m,  27% removed (alpha=   1.9684, nnz=101/138) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.179\u001b[0m \u001b[33m -0.195\u001b[0m,  37% removed (alpha=   5.0802, nnz= 87/138) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 315.88214325945717, tolerance: 282.42865642705414\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.141  -0.160 -> \u001b[33m -0.221\u001b[0m \u001b[33m -0.237\u001b[0m,  49% removed (alpha=  13.1113, nnz= 70/138) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.301\u001b[0m \u001b[33m -0.320\u001b[0m,  57% removed (alpha=  33.8386, nnz= 60/138) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.424\u001b[0m \u001b[33m -0.451\u001b[0m,  68% removed (alpha=  87.3326, nnz= 44/138) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.570\u001b[0m \u001b[33m -0.603\u001b[0m,  80% removed (alpha= 225.3934, nnz= 27/138) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.861\u001b[0m \u001b[33m -0.891\u001b[0m,  90% removed (alpha= 581.7091, nnz= 14/138) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -1.199\u001b[0m \u001b[33m -1.219\u001b[0m,  93% removed (alpha=1501.3107, nnz=  9/138) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -1.629\u001b[0m \u001b[33m -1.643\u001b[0m,  95% removed (alpha=3874.6751, nnz=  7/138) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[31m -0.155\u001b[0m \u001b[32m -0.172\u001b[0m,  21% removed (alpha=   0.3648, nnz=109/138) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.141  -0.160 -> \u001b[31m -0.155\u001b[0m \u001b[32m -0.172\u001b[0m,  22% removed (alpha=   0.4504, nnz=107/138) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.141  -0.160 -> \u001b[31m -0.155\u001b[0m \u001b[32m -0.172\u001b[0m,  22% removed (alpha=   0.5560, nnz=107/138) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.141  -0.160 -> \u001b[31m -0.156\u001b[0m \u001b[32m -0.172\u001b[0m,  23% removed (alpha=   0.6864, nnz=106/138) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.141  -0.160 -> \u001b[31m -0.156\u001b[0m \u001b[32m -0.173\u001b[0m,  27% removed (alpha=   0.8474, nnz=101/138) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.141  -0.160 -> \u001b[31m -0.157\u001b[0m \u001b[32m -0.173\u001b[0m,  27% removed (alpha=   1.0462, nnz=101/138) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.141  -0.160 -> \u001b[31m -0.157\u001b[0m \u001b[32m -0.174\u001b[0m,  25% removed (alpha=   1.2915, nnz=103/138) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.141  -0.160 -> \u001b[31m -0.159\u001b[0m \u001b[32m -0.175\u001b[0m,  28% removed (alpha=   1.5945, nnz= 99/138) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.141  -0.160 -> \u001b[31m -0.158\u001b[0m \u001b[32m -0.175\u001b[0m,  24% removed (alpha=   1.4051, nnz=105/138) \u001b[32mfit ok\u001b[0m [1.2915 1.9684]\n",
      " -0.141  -0.160 -> \u001b[31m -0.158\u001b[0m \u001b[32m -0.175\u001b[0m,  24% removed (alpha=   1.5287, nnz=105/138) \u001b[32mfit ok\u001b[0m [1.2915 1.9684]\n",
      " -0.141  -0.160 -> \u001b[31m -0.159\u001b[0m \u001b[32m -0.176\u001b[0m,  30% removed (alpha=   1.6631, nnz= 97/138) \u001b[32mfit ok\u001b[0m [1.2915 1.9684]\n",
      " -0.141  -0.160 -> \u001b[31m -0.160\u001b[0m \u001b[31m -0.176\u001b[0m,  29% removed (alpha=   1.8093, nnz= 98/138) \u001b[33munder \u001b[0m [1.2915 1.9684]\n",
      "METRICS orig   tr -0.141\u001b[0m, va -0.160\u001b[0m \u001b[1m[te -0.154]\u001b[0m,  ntrees 100,  nnodes 10462,  nleafs  5281,  nnz  5278\n",
      "METRICS prev   \u001b[31mtr -0.160\u001b[0m, \u001b[32mva -0.176\u001b[0m \u001b[1m\u001b[31m[te -0.175]\u001b[0m,  ntrees  23,  nnodes   781,  nleafs   402,  nnz   348\n",
      "METRICS now    \u001b[31mtr -0.159\u001b[0m, \u001b[32mva -0.176\u001b[0m \u001b[1m\u001b[31m[te -0.175]\u001b[0m,  ntrees  22,  nnodes   768,  nleafs   395,  nnz   360\n",
      "\n",
      "Level 3 xxtrain.shape (11478, 211) dense transform time: 0.00s, 1.16s relerr 0.1\n",
      " -0.141  -0.160 -> \u001b[33m -0.186\u001b[0m \u001b[31m -0.182\u001b[0m,   1% removed (alpha=   0.0026, nnz=209/211) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.186\u001b[0m \u001b[31m -0.182\u001b[0m,   1% removed (alpha=   0.0067, nnz=209/211) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.184\u001b[0m \u001b[31m -0.181\u001b[0m,   2% removed (alpha=   0.0172, nnz=207/211) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.184\u001b[0m \u001b[31m -0.181\u001b[0m,   2% removed (alpha=   0.0444, nnz=207/211) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.174\u001b[0m \u001b[31m -0.177\u001b[0m,   9% removed (alpha=   0.1145, nnz=191/211) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.173\u001b[0m \u001b[31m -0.177\u001b[0m,  13% removed (alpha=   0.2955, nnz=183/211) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 370.35861996099686, tolerance: 282.42865642705414\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 569.0454040599568, tolerance: 282.42865642705414\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.141  -0.160 -> \u001b[31m -0.158\u001b[0m \u001b[32m -0.173\u001b[0m,  22% removed (alpha=   0.7627, nnz=164/211) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[31m -0.168\u001b[0m \u001b[31m -0.183\u001b[0m,  32% removed (alpha=   1.9684, nnz=144/211) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 385.3614821218412, tolerance: 282.42865642705414\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.141  -0.160 -> \u001b[33m -0.201\u001b[0m \u001b[33m -0.209\u001b[0m,  35% removed (alpha=   5.0802, nnz=137/211) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.281\u001b[0m \u001b[33m -0.285\u001b[0m,  52% removed (alpha=  13.1113, nnz=101/211) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.405\u001b[0m \u001b[33m -0.409\u001b[0m,  71% removed (alpha=  33.8386, nnz= 62/211) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.554\u001b[0m \u001b[33m -0.564\u001b[0m,  78% removed (alpha=  87.3326, nnz= 47/211) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.735\u001b[0m \u001b[33m -0.743\u001b[0m,  85% removed (alpha= 225.3934, nnz= 32/211) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -1.077\u001b[0m \u001b[33m -1.103\u001b[0m,  89% removed (alpha= 581.7091, nnz= 23/211) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -1.536\u001b[0m \u001b[33m -1.552\u001b[0m,  93% removed (alpha=1501.3107, nnz= 14/211) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -2.815\u001b[0m \u001b[33m -2.804\u001b[0m,  94% removed (alpha=3874.6751, nnz= 12/211) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.179\u001b[0m \u001b[31m -0.180\u001b[0m,  15% removed (alpha=   0.3648, nnz=179/211) \u001b[33munder \u001b[0m [0.2955 1.9684]\n",
      " -0.141  -0.160 -> \u001b[33m -0.179\u001b[0m \u001b[31m -0.180\u001b[0m,  17% removed (alpha=   0.4504, nnz=176/211) \u001b[33munder \u001b[0m [0.2955 1.9684]\n",
      " -0.141  -0.160 -> \u001b[33m -0.169\u001b[0m \u001b[31m -0.177\u001b[0m,  22% removed (alpha=   0.5560, nnz=164/211) \u001b[33munder \u001b[0m [0.2955 1.9684]\n",
      " -0.141  -0.160 -> \u001b[31m -0.161\u001b[0m \u001b[32m -0.174\u001b[0m,  24% removed (alpha=   0.6864, nnz=161/211) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.141  -0.160 -> \u001b[31m -0.160\u001b[0m \u001b[32m -0.174\u001b[0m,  23% removed (alpha=   0.8474, nnz=163/211) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.141  -0.160 -> \u001b[31m -0.161\u001b[0m \u001b[32m -0.175\u001b[0m,  25% removed (alpha=   1.0462, nnz=159/211) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.141  -0.160 -> \u001b[31m -0.164\u001b[0m \u001b[31m -0.177\u001b[0m,  25% removed (alpha=   1.2915, nnz=158/211) \u001b[33munder \u001b[0m [0.2955 1.9684]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 424.3649438146608, tolerance: 282.42865642705414\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 324.3366212275722, tolerance: 282.42865642705414\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.141  -0.160 -> \u001b[31m -0.165\u001b[0m \u001b[31m -0.179\u001b[0m,  31% removed (alpha=   1.5945, nnz=146/211) \u001b[33munder \u001b[0m [0.2955 1.9684]\n",
      " -0.141  -0.160 -> \u001b[31m -0.159\u001b[0m \u001b[32m -0.174\u001b[0m,  24% removed (alpha=   0.9219, nnz=160/211) \u001b[32mfit ok\u001b[0m [0.8474 1.2915]\n",
      " -0.141  -0.160 -> \u001b[31m -0.159\u001b[0m \u001b[32m -0.174\u001b[0m,  24% removed (alpha=   1.0030, nnz=161/211) \u001b[32mfit ok\u001b[0m [0.8474 1.2915]\n",
      " -0.141  -0.160 -> \u001b[31m -0.159\u001b[0m \u001b[32m -0.174\u001b[0m,  29% removed (alpha=   1.0912, nnz=150/211) \u001b[32mfit ok\u001b[0m [0.8474 1.2915]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 338.0688978669624, tolerance: 282.42865642705414\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.141  -0.160 -> \u001b[31m -0.160\u001b[0m \u001b[32m -0.175\u001b[0m,  28% removed (alpha=   1.1872, nnz=151/211) \u001b[32mfit ok\u001b[0m [0.8474 1.2915]\n",
      "METRICS orig   tr -0.141\u001b[0m, va -0.160\u001b[0m \u001b[1m[te -0.154]\u001b[0m,  ntrees 100,  nnodes 10462,  nleafs  5281,  nnz  5278\n",
      "METRICS prev   \u001b[31mtr -0.159\u001b[0m, \u001b[32mva -0.176\u001b[0m \u001b[1m\u001b[31m[te -0.175]\u001b[0m,  ntrees  22,  nnodes   768,  nleafs   395,  nnz   360\n",
      "METRICS now    \u001b[31mtr -0.159\u001b[0m, \u001b[32mva -0.174\u001b[0m \u001b[1m\u001b[31m[te -0.172]\u001b[0m,  ntrees  21,  nnodes   743,  nleafs   382,  nnz   349\n",
      "\n",
      "Level 4 xxtrain.shape (11478, 301) dense transform time: 0.00s, 1.13s relerr 0.1\n",
      " -0.141  -0.160 -> \u001b[32m -0.154\u001b[0m \u001b[32m -0.170\u001b[0m,   1% removed (alpha=   0.0026, nnz=299/301) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.154\u001b[0m \u001b[32m -0.170\u001b[0m,   1% removed (alpha=   0.0067, nnz=299/301) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.150\u001b[0m \u001b[32m -0.166\u001b[0m,   2% removed (alpha=   0.0172, nnz=296/301) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.150\u001b[0m \u001b[32m -0.166\u001b[0m,   2% removed (alpha=   0.0444, nnz=294/301) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.149\u001b[0m \u001b[32m -0.166\u001b[0m,   4% removed (alpha=   0.1145, nnz=290/301) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.150\u001b[0m \u001b[32m -0.169\u001b[0m,  13% removed (alpha=   0.2955, nnz=263/301) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[31m -0.155\u001b[0m \u001b[31m -0.177\u001b[0m,  25% removed (alpha=   0.7627, nnz=225/301) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 333.30475167174444, tolerance: 282.42865642705414\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 439.2289739147263, tolerance: 282.42865642705414\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.141  -0.160 -> \u001b[33m -0.170\u001b[0m \u001b[31m -0.191\u001b[0m,  34% removed (alpha=   1.9684, nnz=200/301) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.201\u001b[0m \u001b[33m -0.222\u001b[0m,  45% removed (alpha=   5.0802, nnz=167/301) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.271\u001b[0m \u001b[33m -0.289\u001b[0m,  54% removed (alpha=  13.1113, nnz=138/301) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.378\u001b[0m \u001b[33m -0.401\u001b[0m,  68% removed (alpha=  33.8386, nnz= 97/301) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.548\u001b[0m \u001b[33m -0.569\u001b[0m,  79% removed (alpha=  87.3326, nnz= 64/301) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.831\u001b[0m \u001b[33m -0.833\u001b[0m,  85% removed (alpha= 225.3934, nnz= 46/301) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -1.361\u001b[0m \u001b[33m -1.321\u001b[0m,  91% removed (alpha= 581.7091, nnz= 27/301) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -2.019\u001b[0m \u001b[33m -1.943\u001b[0m,  95% removed (alpha=1501.3107, nnz= 15/301) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -2.954\u001b[0m \u001b[33m -2.899\u001b[0m,  97% removed (alpha=3874.6751, nnz=  8/301) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.149\u001b[0m \u001b[32m -0.166\u001b[0m,   7% removed (alpha=   0.1414, nnz=280/301) \u001b[32mfit ok\u001b[0m [0.1145 0.7627]\n",
      " -0.141  -0.160 -> \u001b[32m -0.148\u001b[0m \u001b[32m -0.166\u001b[0m,   8% removed (alpha=   0.1745, nnz=276/301) \u001b[32mfit ok\u001b[0m [0.1145 0.7627]\n",
      " -0.141  -0.160 -> \u001b[32m -0.149\u001b[0m \u001b[32m -0.167\u001b[0m,  11% removed (alpha=   0.2154, nnz=269/301) \u001b[32mfit ok\u001b[0m [0.1145 0.7627]\n",
      " -0.141  -0.160 -> \u001b[32m -0.149\u001b[0m \u001b[32m -0.168\u001b[0m,  18% removed (alpha=   0.2660, nnz=248/301) \u001b[32mfit ok\u001b[0m [0.1145 0.7627]\n",
      " -0.141  -0.160 -> \u001b[32m -0.150\u001b[0m \u001b[32m -0.169\u001b[0m,  17% removed (alpha=   0.3284, nnz=249/301) \u001b[32mfit ok\u001b[0m [0.1145 0.7627]\n",
      " -0.141  -0.160 -> \u001b[32m -0.151\u001b[0m \u001b[32m -0.170\u001b[0m,  18% removed (alpha=   0.4054, nnz=247/301) \u001b[32mfit ok\u001b[0m [0.1145 0.7627]\n",
      " -0.141  -0.160 -> \u001b[32m -0.152\u001b[0m \u001b[32m -0.171\u001b[0m,  19% removed (alpha=   0.5004, nnz=244/301) \u001b[32mfit ok\u001b[0m [0.1145 0.7627]\n",
      " -0.141  -0.160 -> \u001b[32m -0.153\u001b[0m \u001b[32m -0.173\u001b[0m,  19% removed (alpha=   0.6178, nnz=245/301) \u001b[32mfit ok\u001b[0m [0.1145 0.7627]\n",
      " -0.141  -0.160 -> \u001b[32m -0.152\u001b[0m \u001b[32m -0.172\u001b[0m,  18% removed (alpha=   0.5444, nnz=248/301) \u001b[32mfit ok\u001b[0m [0.5004 0.7627]\n",
      " -0.141  -0.160 -> \u001b[32m -0.153\u001b[0m \u001b[32m -0.172\u001b[0m,  18% removed (alpha=   0.5923, nnz=248/301) \u001b[32mfit ok\u001b[0m [0.5004 0.7627]\n",
      " -0.141  -0.160 -> \u001b[32m -0.153\u001b[0m \u001b[32m -0.173\u001b[0m,  20% removed (alpha=   0.6444, nnz=242/301) \u001b[32mfit ok\u001b[0m [0.5004 0.7627]\n",
      " -0.141  -0.160 -> \u001b[32m -0.154\u001b[0m \u001b[32m -0.174\u001b[0m,  21% removed (alpha=   0.7011, nnz=239/301) \u001b[32mfit ok\u001b[0m [0.5004 0.7627]\n",
      "METRICS orig   tr -0.141\u001b[0m, va -0.160\u001b[0m \u001b[1m[te -0.154]\u001b[0m,  ntrees 100,  nnodes 10462,  nleafs  5281,  nnz  5278\n",
      "METRICS prev   \u001b[31mtr -0.159\u001b[0m, \u001b[32mva -0.174\u001b[0m \u001b[1m\u001b[31m[te -0.172]\u001b[0m,  ntrees  21,  nnodes   743,  nleafs   382,  nnz   349\n",
      "METRICS now    \u001b[32mtr -0.154\u001b[0m, \u001b[32mva -0.174\u001b[0m \u001b[1m\u001b[31m[te -0.169]\u001b[0m,  ntrees  21,  nnodes   715,  nleafs   368,  nnz   333\n",
      "\n",
      "Level 5 xxtrain.shape (11478, 368) dense transform time: 0.00s, 1.11s relerr 0.1\n",
      " -0.141  -0.160 -> \u001b[31m -0.156\u001b[0m \u001b[32m -0.175\u001b[0m,   1% removed (alpha=   0.0026, nnz=366/368) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[31m -0.156\u001b[0m \u001b[32m -0.175\u001b[0m,   1% removed (alpha=   0.0067, nnz=365/368) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[31m -0.155\u001b[0m \u001b[32m -0.174\u001b[0m,   1% removed (alpha=   0.0172, nnz=364/368) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.152\u001b[0m \u001b[32m -0.173\u001b[0m,   2% removed (alpha=   0.0444, nnz=361/368) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.149\u001b[0m \u001b[32m -0.172\u001b[0m,   6% removed (alpha=   0.1145, nnz=346/368) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.148\u001b[0m \u001b[32m -0.174\u001b[0m,  15% removed (alpha=   0.2955, nnz=314/368) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 330.1524332146326, tolerance: 282.42865642705414\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.141  -0.160 -> \u001b[32m -0.154\u001b[0m \u001b[31m -0.179\u001b[0m,  25% removed (alpha=   0.7627, nnz=275/368) \u001b[31mover  \u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 294.013696113892, tolerance: 282.42865642705414\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.141  -0.160 -> \u001b[31m -0.168\u001b[0m \u001b[31m -0.190\u001b[0m,  36% removed (alpha=   1.9684, nnz=236/368) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.203\u001b[0m \u001b[33m -0.219\u001b[0m,  46% removed (alpha=   5.0802, nnz=199/368) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.276\u001b[0m \u001b[33m -0.287\u001b[0m,  58% removed (alpha=  13.1113, nnz=155/368) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.400\u001b[0m \u001b[33m -0.416\u001b[0m,  70% removed (alpha=  33.8386, nnz=111/368) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.609\u001b[0m \u001b[33m -0.632\u001b[0m,  81% removed (alpha=  87.3326, nnz= 69/368) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.820\u001b[0m \u001b[33m -0.824\u001b[0m,  86% removed (alpha= 225.3934, nnz= 50/368) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -1.308\u001b[0m \u001b[33m -1.274\u001b[0m,  93% removed (alpha= 581.7091, nnz= 27/368) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -2.021\u001b[0m \u001b[33m -1.961\u001b[0m,  96% removed (alpha=1501.3107, nnz= 16/368) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -3.124\u001b[0m \u001b[33m -3.071\u001b[0m,  98% removed (alpha=3874.6751, nnz=  9/368) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.147\u001b[0m \u001b[32m -0.171\u001b[0m,   8% removed (alpha=   0.1414, nnz=338/368) \u001b[32mfit ok\u001b[0m [0.1145 0.7627]\n",
      " -0.141  -0.160 -> \u001b[32m -0.146\u001b[0m \u001b[32m -0.172\u001b[0m,  13% removed (alpha=   0.1745, nnz=320/368) \u001b[32mfit ok\u001b[0m [0.1145 0.7627]\n",
      " -0.141  -0.160 -> \u001b[32m -0.146\u001b[0m \u001b[32m -0.172\u001b[0m,  14% removed (alpha=   0.2154, nnz=317/368) \u001b[32mfit ok\u001b[0m [0.1145 0.7627]\n",
      " -0.141  -0.160 -> \u001b[32m -0.147\u001b[0m \u001b[32m -0.172\u001b[0m,  18% removed (alpha=   0.2660, nnz=300/368) \u001b[32mfit ok\u001b[0m [0.1145 0.7627]\n",
      " -0.141  -0.160 -> \u001b[32m -0.147\u001b[0m \u001b[32m -0.172\u001b[0m,  19% removed (alpha=   0.3284, nnz=298/368) \u001b[32mfit ok\u001b[0m [0.1145 0.7627]\n",
      " -0.141  -0.160 -> \u001b[32m -0.148\u001b[0m \u001b[32m -0.173\u001b[0m,  19% removed (alpha=   0.4054, nnz=299/368) \u001b[32mfit ok\u001b[0m [0.1145 0.7627]\n",
      " -0.141  -0.160 -> \u001b[32m -0.149\u001b[0m \u001b[32m -0.175\u001b[0m,  18% removed (alpha=   0.5004, nnz=301/368) \u001b[32mfit ok\u001b[0m [0.1145 0.7627]\n",
      " -0.141  -0.160 -> \u001b[32m -0.151\u001b[0m \u001b[31m -0.177\u001b[0m,  19% removed (alpha=   0.6178, nnz=297/368) \u001b[31mover  \u001b[0m [0.1145 0.7627]\n",
      " -0.141  -0.160 -> \u001b[32m -0.149\u001b[0m \u001b[32m -0.174\u001b[0m,  17% removed (alpha=   0.4410, nnz=307/368) \u001b[32mfit ok\u001b[0m [0.4054 0.6178]\n",
      " -0.141  -0.160 -> \u001b[32m -0.149\u001b[0m \u001b[32m -0.175\u001b[0m,  18% removed (alpha=   0.4798, nnz=303/368) \u001b[32mfit ok\u001b[0m [0.4054 0.6178]\n",
      " -0.141  -0.160 -> \u001b[32m -0.150\u001b[0m \u001b[32m -0.175\u001b[0m,  19% removed (alpha=   0.5220, nnz=297/368) \u001b[32mfit ok\u001b[0m [0.4054 0.6178]\n",
      " -0.141  -0.160 -> \u001b[32m -0.150\u001b[0m \u001b[31m -0.176\u001b[0m,  20% removed (alpha=   0.5679, nnz=293/368) \u001b[31mover  \u001b[0m [0.4054 0.6178]\n",
      "METRICS orig   tr -0.141\u001b[0m, va -0.160\u001b[0m \u001b[1m[te -0.154]\u001b[0m,  ntrees 100,  nnodes 10462,  nleafs  5281,  nnz  5278\n",
      "METRICS prev   \u001b[32mtr -0.154\u001b[0m, \u001b[32mva -0.174\u001b[0m \u001b[1m\u001b[31m[te -0.169]\u001b[0m,  ntrees  21,  nnodes   715,  nleafs   368,  nnz   333\n",
      "METRICS now    \u001b[32mtr -0.150\u001b[0m, \u001b[32mva -0.175\u001b[0m \u001b[1m\u001b[32m[te -0.165]\u001b[0m,  ntrees  21,  nnodes   697,  nleafs   359,  nnz   322\n",
      "\n",
      "Level 6 xxtrain.shape (11478, 359) dense transform time: 0.00s, 1.12s relerr 0.1\n",
      " -0.141  -0.160 -> \u001b[32m -0.146\u001b[0m \u001b[32m -0.170\u001b[0m,   2% removed (alpha=   0.0026, nnz=352/359) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.146\u001b[0m \u001b[32m -0.170\u001b[0m,   2% removed (alpha=   0.0067, nnz=352/359) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.146\u001b[0m \u001b[32m -0.170\u001b[0m,   2% removed (alpha=   0.0172, nnz=352/359) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.146\u001b[0m \u001b[32m -0.169\u001b[0m,   5% removed (alpha=   0.0444, nnz=342/359) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.146\u001b[0m \u001b[32m -0.169\u001b[0m,   6% removed (alpha=   0.1145, nnz=336/359) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.146\u001b[0m \u001b[32m -0.170\u001b[0m,  13% removed (alpha=   0.2955, nnz=314/359) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.149\u001b[0m \u001b[32m -0.175\u001b[0m,  21% removed (alpha=   0.7627, nnz=285/359) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 340.8667410760909, tolerance: 282.42865642705414\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.141  -0.160 -> \u001b[31m -0.155\u001b[0m \u001b[32m -0.175\u001b[0m,  26% removed (alpha=   1.9684, nnz=266/359) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.184\u001b[0m \u001b[33m -0.202\u001b[0m,  38% removed (alpha=   5.0802, nnz=224/359) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.248\u001b[0m \u001b[33m -0.265\u001b[0m,  49% removed (alpha=  13.1113, nnz=182/359) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.390\u001b[0m \u001b[33m -0.413\u001b[0m,  63% removed (alpha=  33.8386, nnz=133/359) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.610\u001b[0m \u001b[33m -0.631\u001b[0m,  77% removed (alpha=  87.3326, nnz= 83/359) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -0.911\u001b[0m \u001b[33m -0.919\u001b[0m,  84% removed (alpha= 225.3934, nnz= 57/359) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -1.567\u001b[0m \u001b[33m -1.558\u001b[0m,  87% removed (alpha= 581.7091, nnz= 45/359) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -2.612\u001b[0m \u001b[33m -2.577\u001b[0m,  93% removed (alpha=1501.3107, nnz= 25/359) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[33m -3.936\u001b[0m \u001b[33m -3.878\u001b[0m,  99% removed (alpha=3874.6751, nnz=  5/359) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.141  -0.160 -> \u001b[32m -0.149\u001b[0m \u001b[32m -0.171\u001b[0m,  24% removed (alpha=   0.9416, nnz=272/359) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[32m -0.150\u001b[0m \u001b[32m -0.172\u001b[0m,  25% removed (alpha=   1.1624, nnz=271/359) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[32m -0.151\u001b[0m \u001b[32m -0.173\u001b[0m,  25% removed (alpha=   1.4350, nnz=270/359) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[32m -0.153\u001b[0m \u001b[32m -0.175\u001b[0m,  26% removed (alpha=   1.7716, nnz=265/359) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[31m -0.156\u001b[0m \u001b[31m -0.178\u001b[0m,  26% removed (alpha=   2.1871, nnz=265/359) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[31m -0.161\u001b[0m \u001b[31m -0.182\u001b[0m,  28% removed (alpha=   2.7001, nnz=259/359) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[31m -0.167\u001b[0m \u001b[31m -0.187\u001b[0m,  30% removed (alpha=   3.3333, nnz=250/359) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[33m -0.175\u001b[0m \u001b[33m -0.193\u001b[0m,  33% removed (alpha=   4.1151, nnz=239/359) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.141  -0.160 -> \u001b[32m -0.153\u001b[0m \u001b[32m -0.174\u001b[0m,  23% removed (alpha=   1.5612, nnz=275/359) \u001b[32mfit ok\u001b[0m [1.435  2.1871]\n",
      " -0.141  -0.160 -> \u001b[32m -0.154\u001b[0m \u001b[32m -0.175\u001b[0m,  24% removed (alpha=   1.6985, nnz=272/359) \u001b[32mfit ok\u001b[0m [1.435  2.1871]\n",
      " -0.141  -0.160 -> \u001b[32m -0.154\u001b[0m \u001b[32m -0.176\u001b[0m,  25% removed (alpha=   1.8478, nnz=271/359) \u001b[32mfit ok\u001b[0m [1.435  2.1871]\n",
      " -0.141  -0.160 -> \u001b[31m -0.156\u001b[0m \u001b[31m -0.177\u001b[0m,  26% removed (alpha=   2.0103, nnz=266/359) \u001b[33munder \u001b[0m [1.435  2.1871]\n",
      "METRICS orig   tr -0.141\u001b[0m, va -0.160\u001b[0m \u001b[1m[te -0.154]\u001b[0m,  ntrees 100,  nnodes 10462,  nleafs  5281,  nnz  5278\n",
      "METRICS prev   \u001b[32mtr -0.150\u001b[0m, \u001b[32mva -0.175\u001b[0m \u001b[1m\u001b[32m[te -0.165]\u001b[0m,  ntrees  21,  nnodes   697,  nleafs   359,  nnz   322\n",
      "METRICS now    \u001b[32mtr -0.153\u001b[0m, \u001b[32mva -0.175\u001b[0m \u001b[1m\u001b[32m[te -0.168]\u001b[0m,  ntrees  21,  nnodes   691,  nleafs   356,  nnz   265\n",
      "\n",
      "DONE, depth of tree reached 6, 6\n",
      "ChaahatRegTarget3 number of non-zero leaves: 5278 -> 265, test accuracy: -15.4% -> -16.8%\n",
      "\n",
      "| XGBOOST's base_score\n",
      "|   base_score diff std      3.696085443200083e-05 (!) NOT OK\n",
      "|   base_score reported      201.63986\n",
      "|   versus manually detected 201.6398636694348\n",
      "|   abs err                  3.6694347897991975e-06\n",
      "|   rel err                  1.8197963387790477e-08\n",
      "|   base_score OK with relative tolerance 0.001\n",
      "\n",
      "MODEL PERF: mtr -0.222 mte -0.234 mva -0.233\n",
      "\n",
      "\n",
      "ROUND 1\n",
      "Level 0 xxtrain.shape (11478, 100) dense transform time: 0.01s, 4.91s relerr 0.1\n",
      " -0.222  -0.233 -> \u001b[32m -0.173\u001b[0m \u001b[32m -0.187\u001b[0m,   0% removed (alpha=   0.0026, nnz=100/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.173\u001b[0m \u001b[32m -0.187\u001b[0m,   0% removed (alpha=   0.0067, nnz=100/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.173\u001b[0m \u001b[32m -0.187\u001b[0m,   1% removed (alpha=   0.0172, nnz= 99/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.173\u001b[0m \u001b[32m -0.187\u001b[0m,   3% removed (alpha=   0.0444, nnz= 97/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.174\u001b[0m \u001b[32m -0.187\u001b[0m,   7% removed (alpha=   0.1145, nnz= 93/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.176\u001b[0m \u001b[32m -0.190\u001b[0m,  17% removed (alpha=   0.2955, nnz= 83/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.183\u001b[0m \u001b[32m -0.197\u001b[0m,  31% removed (alpha=   0.7627, nnz= 69/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.198\u001b[0m \u001b[32m -0.213\u001b[0m,  52% removed (alpha=   1.9684, nnz= 48/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.224\u001b[0m \u001b[32m -0.239\u001b[0m,  67% removed (alpha=   5.0802, nnz= 33/100) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[31m -0.258\u001b[0m \u001b[31m -0.272\u001b[0m,  79% removed (alpha=  13.1113, nnz= 21/100) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.310\u001b[0m \u001b[33m -0.322\u001b[0m,  87% removed (alpha=  33.8386, nnz= 13/100) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.364\u001b[0m \u001b[33m -0.380\u001b[0m,  91% removed (alpha=  87.3326, nnz=  9/100) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.457\u001b[0m \u001b[33m -0.474\u001b[0m,  93% removed (alpha= 225.3934, nnz=  7/100) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.573\u001b[0m \u001b[33m -0.583\u001b[0m,  95% removed (alpha= 581.7091, nnz=  5/100) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.743\u001b[0m \u001b[33m -0.751\u001b[0m,  97% removed (alpha=1501.3107, nnz=  3/100) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.902\u001b[0m \u001b[33m -0.914\u001b[0m,  98% removed (alpha=3874.6751, nnz=  2/100) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.203\u001b[0m \u001b[32m -0.218\u001b[0m,  55% removed (alpha=   2.4301, nnz= 45/100) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.222  -0.233 -> \u001b[32m -0.208\u001b[0m \u001b[32m -0.222\u001b[0m,  59% removed (alpha=   3.0000, nnz= 41/100) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.222  -0.233 -> \u001b[32m -0.214\u001b[0m \u001b[32m -0.229\u001b[0m,  62% removed (alpha=   3.7036, nnz= 38/100) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.222  -0.233 -> \u001b[32m -0.220\u001b[0m \u001b[32m -0.235\u001b[0m,  66% removed (alpha=   4.5723, nnz= 34/100) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.222  -0.233 -> \u001b[32m -0.228\u001b[0m \u001b[32m -0.243\u001b[0m,  68% removed (alpha=   5.6446, nnz= 32/100) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.222  -0.233 -> \u001b[32m -0.235\u001b[0m \u001b[32m -0.250\u001b[0m,  73% removed (alpha=   6.9685, nnz= 27/100) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.222  -0.233 -> \u001b[32m -0.242\u001b[0m \u001b[32m -0.256\u001b[0m,  76% removed (alpha=   8.6028, nnz= 24/100) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.222  -0.233 -> \u001b[31m -0.249\u001b[0m \u001b[31m -0.263\u001b[0m,  77% removed (alpha=  10.6205, nnz= 23/100) \u001b[33munder \u001b[0m [ 1.9684 13.1113]\n",
      " -0.222  -0.233 -> \u001b[32m -0.238\u001b[0m \u001b[32m -0.252\u001b[0m,  74% removed (alpha=   7.5812, nnz= 26/100) \u001b[32mfit ok\u001b[0m [ 6.9685 10.6205]\n",
      " -0.222  -0.233 -> \u001b[32m -0.240\u001b[0m \u001b[32m -0.255\u001b[0m,  76% removed (alpha=   8.2478, nnz= 24/100) \u001b[32mfit ok\u001b[0m [ 6.9685 10.6205]\n",
      " -0.222  -0.233 -> \u001b[32m -0.243\u001b[0m \u001b[31m -0.257\u001b[0m,  76% removed (alpha=   8.9731, nnz= 24/100) \u001b[32mfit ok\u001b[0m [ 6.9685 10.6205]\n",
      " -0.222  -0.233 -> \u001b[31m -0.246\u001b[0m \u001b[31m -0.260\u001b[0m,  77% removed (alpha=   9.7621, nnz= 23/100) \u001b[33munder \u001b[0m [ 6.9685 10.6205]\n",
      "METRICS orig   tr -0.222\u001b[0m, va -0.233\u001b[0m \u001b[1m[te -0.234]\u001b[0m,  ntrees 100,  nnodes 10170,  nleafs  5135,  nnz  5133\n",
      "METRICS prev   \u001b[32mtr -0.222\u001b[0m, \u001b[32mva -0.233\u001b[0m \u001b[1m\u001b[32m[te -0.234]\u001b[0m,  ntrees 100,  nnodes 10170,  nleafs  5135,  nnz  5133\n",
      "METRICS now    \u001b[32mtr -0.242\u001b[0m, \u001b[32mva -0.256\u001b[0m \u001b[1m\u001b[32m[te -0.255]\u001b[0m,  ntrees  24,  nnodes  2420,  nleafs  1222,  nnz  1222\n",
      "\n",
      "Level 1 xxtrain.shape (11478, 96) dense transform time: 0.01s, 1.23s relerr 0.1\n",
      " -0.222  -0.233 -> \u001b[32m -0.187\u001b[0m \u001b[32m -0.203\u001b[0m,   2% removed (alpha=   0.0026, nnz=94/96) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.187\u001b[0m \u001b[32m -0.203\u001b[0m,   2% removed (alpha=   0.0067, nnz=94/96) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.187\u001b[0m \u001b[32m -0.203\u001b[0m,   2% removed (alpha=   0.0172, nnz=94/96) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.187\u001b[0m \u001b[32m -0.203\u001b[0m,   2% removed (alpha=   0.0444, nnz=94/96) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.187\u001b[0m \u001b[32m -0.203\u001b[0m,   2% removed (alpha=   0.1145, nnz=94/96) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.188\u001b[0m \u001b[32m -0.204\u001b[0m,   3% removed (alpha=   0.2955, nnz=93/96) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.193\u001b[0m \u001b[32m -0.208\u001b[0m,   5% removed (alpha=   0.7627, nnz=91/96) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.202\u001b[0m \u001b[32m -0.217\u001b[0m,  15% removed (alpha=   1.9684, nnz=82/96) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.219\u001b[0m \u001b[32m -0.233\u001b[0m,  32% removed (alpha=   5.0802, nnz=65/96) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[31m -0.250\u001b[0m \u001b[31m -0.262\u001b[0m,  53% removed (alpha=  13.1113, nnz=45/96) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 587.398140257653, tolerance: 507.3271805515732\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.222  -0.233 -> \u001b[33m -0.308\u001b[0m \u001b[33m -0.324\u001b[0m,  55% removed (alpha=  33.8386, nnz=43/96) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.403\u001b[0m \u001b[33m -0.422\u001b[0m,  67% removed (alpha=  87.3326, nnz=32/96) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.503\u001b[0m \u001b[33m -0.520\u001b[0m,  83% removed (alpha= 225.3934, nnz=16/96) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.672\u001b[0m \u001b[33m -0.680\u001b[0m,  90% removed (alpha= 581.7091, nnz=10/96) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.886\u001b[0m \u001b[33m -0.896\u001b[0m,  97% removed (alpha=1501.3107, nnz= 3/96) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.998\u001b[0m \u001b[33m -1.008\u001b[0m,  98% removed (alpha=3874.6751, nnz= 2/96) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.204\u001b[0m \u001b[32m -0.219\u001b[0m,  21% removed (alpha=   2.4301, nnz=76/96) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.222  -0.233 -> \u001b[32m -0.207\u001b[0m \u001b[32m -0.222\u001b[0m,  35% removed (alpha=   3.0000, nnz=62/96) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.222  -0.233 -> \u001b[32m -0.211\u001b[0m \u001b[32m -0.225\u001b[0m,  38% removed (alpha=   3.7036, nnz=60/96) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.222  -0.233 -> \u001b[32m -0.215\u001b[0m \u001b[32m -0.229\u001b[0m,  38% removed (alpha=   4.5723, nnz=60/96) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.222  -0.233 -> \u001b[32m -0.220\u001b[0m \u001b[32m -0.234\u001b[0m,  35% removed (alpha=   5.6446, nnz=62/96) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.222  -0.233 -> \u001b[32m -0.226\u001b[0m \u001b[32m -0.239\u001b[0m,  36% removed (alpha=   6.9685, nnz=61/96) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.222  -0.233 -> \u001b[32m -0.234\u001b[0m \u001b[32m -0.247\u001b[0m,  40% removed (alpha=   8.6028, nnz=58/96) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.222  -0.233 -> \u001b[32m -0.241\u001b[0m \u001b[32m -0.254\u001b[0m,  41% removed (alpha=  10.6205, nnz=57/96) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.222  -0.233 -> \u001b[32m -0.237\u001b[0m \u001b[32m -0.250\u001b[0m,  34% removed (alpha=   9.3593, nnz=63/96) \u001b[32mfit ok\u001b[0m [ 8.6028 13.1113]\n",
      " -0.222  -0.233 -> \u001b[32m -0.239\u001b[0m \u001b[32m -0.252\u001b[0m,  32% removed (alpha=  10.1822, nnz=65/96) \u001b[32mfit ok\u001b[0m [ 8.6028 13.1113]\n",
      " -0.222  -0.233 -> \u001b[32m -0.243\u001b[0m \u001b[32m -0.256\u001b[0m,  38% removed (alpha=  11.0776, nnz=60/96) \u001b[32mfit ok\u001b[0m [ 8.6028 13.1113]\n",
      " -0.222  -0.233 -> \u001b[31m -0.246\u001b[0m \u001b[31m -0.259\u001b[0m,  45% removed (alpha=  12.0516, nnz=53/96) \u001b[33munder \u001b[0m [ 8.6028 13.1113]\n",
      "METRICS orig   tr -0.222\u001b[0m, va -0.233\u001b[0m \u001b[1m[te -0.234]\u001b[0m,  ntrees 100,  nnodes 10170,  nleafs  5135,  nnz  5133\n",
      "METRICS prev   \u001b[32mtr -0.242\u001b[0m, \u001b[32mva -0.256\u001b[0m \u001b[1m\u001b[32m[te -0.255]\u001b[0m,  ntrees  24,  nnodes  2420,  nleafs  1222,  nnz  1222\n",
      "METRICS now    \u001b[32mtr -0.241\u001b[0m, \u001b[32mva -0.254\u001b[0m \u001b[1m\u001b[31m[te -0.260]\u001b[0m,  ntrees  21,  nnodes  1531,  nleafs   776,  nnz   770\n",
      "\n",
      "Level 2 xxtrain.shape (11478, 129) dense transform time: 0.00s, 1.13s relerr 0.1\n",
      " -0.222  -0.233 -> \u001b[32m -0.222\u001b[0m \u001b[32m -0.234\u001b[0m,   4% removed (alpha=   0.0026, nnz=124/129) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.222\u001b[0m \u001b[32m -0.234\u001b[0m,   4% removed (alpha=   0.0067, nnz=124/129) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.222\u001b[0m \u001b[32m -0.234\u001b[0m,   4% removed (alpha=   0.0172, nnz=124/129) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.221\u001b[0m \u001b[32m -0.233\u001b[0m,   4% removed (alpha=   0.0444, nnz=124/129) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.221\u001b[0m \u001b[32m -0.233\u001b[0m,   5% removed (alpha=   0.1145, nnz=123/129) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.221\u001b[0m \u001b[32m -0.233\u001b[0m,   8% removed (alpha=   0.2955, nnz=119/129) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.225\u001b[0m \u001b[32m -0.235\u001b[0m,  11% removed (alpha=   0.7627, nnz=115/129) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.225\u001b[0m \u001b[32m -0.237\u001b[0m,  23% removed (alpha=   1.9684, nnz= 99/129) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.241\u001b[0m \u001b[32m -0.252\u001b[0m,  43% removed (alpha=   5.0802, nnz= 74/129) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.285\u001b[0m \u001b[33m -0.298\u001b[0m,  52% removed (alpha=  13.1113, nnz= 62/129) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 618.0560838327283, tolerance: 507.3271805515732\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 557.2602480039352, tolerance: 507.3271805515732\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.222  -0.233 -> \u001b[33m -0.351\u001b[0m \u001b[33m -0.367\u001b[0m,  64% removed (alpha=  33.8386, nnz= 46/129) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.441\u001b[0m \u001b[33m -0.457\u001b[0m,  77% removed (alpha=  87.3326, nnz= 30/129) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.591\u001b[0m \u001b[33m -0.606\u001b[0m,  84% removed (alpha= 225.3934, nnz= 21/129) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.759\u001b[0m \u001b[33m -0.769\u001b[0m,  91% removed (alpha= 581.7091, nnz= 12/129) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.957\u001b[0m \u001b[33m -0.968\u001b[0m,  96% removed (alpha=1501.3107, nnz=  5/129) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -1.044\u001b[0m \u001b[33m -1.063\u001b[0m,  97% removed (alpha=3874.6751, nnz=  4/129) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.227\u001b[0m \u001b[32m -0.238\u001b[0m,  33% removed (alpha=   2.4301, nnz= 87/129) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.222  -0.233 -> \u001b[32m -0.228\u001b[0m \u001b[32m -0.240\u001b[0m,  40% removed (alpha=   3.0000, nnz= 78/129) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.222  -0.233 -> \u001b[32m -0.232\u001b[0m \u001b[32m -0.244\u001b[0m,  40% removed (alpha=   3.7036, nnz= 78/129) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.222  -0.233 -> \u001b[32m -0.238\u001b[0m \u001b[32m -0.249\u001b[0m,  41% removed (alpha=   4.5723, nnz= 76/129) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.222  -0.233 -> \u001b[32m -0.243\u001b[0m \u001b[32m -0.254\u001b[0m,  42% removed (alpha=   5.6446, nnz= 75/129) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.222  -0.233 -> \u001b[31m -0.250\u001b[0m \u001b[31m -0.262\u001b[0m,  41% removed (alpha=   6.9685, nnz= 76/129) \u001b[33munder \u001b[0m [ 1.9684 13.1113]\n",
      " -0.222  -0.233 -> \u001b[31m -0.261\u001b[0m \u001b[31m -0.273\u001b[0m,  42% removed (alpha=   8.6028, nnz= 75/129) \u001b[33munder \u001b[0m [ 1.9684 13.1113]\n",
      " -0.222  -0.233 -> \u001b[33m -0.273\u001b[0m \u001b[33m -0.285\u001b[0m,  48% removed (alpha=  10.6205, nnz= 67/129) \u001b[33munder \u001b[0m [ 1.9684 13.1113]\n",
      " -0.222  -0.233 -> \u001b[32m -0.240\u001b[0m \u001b[32m -0.251\u001b[0m,  39% removed (alpha=   4.9743, nnz= 79/129) \u001b[32mfit ok\u001b[0m [4.5723 6.9685]\n",
      " -0.222  -0.233 -> \u001b[32m -0.242\u001b[0m \u001b[32m -0.253\u001b[0m,  39% removed (alpha=   5.4117, nnz= 79/129) \u001b[32mfit ok\u001b[0m [4.5723 6.9685]\n",
      " -0.222  -0.233 -> \u001b[32m -0.244\u001b[0m \u001b[32m -0.256\u001b[0m,  40% removed (alpha=   5.8876, nnz= 77/129) \u001b[32mfit ok\u001b[0m [4.5723 6.9685]\n",
      " -0.222  -0.233 -> \u001b[31m -0.247\u001b[0m \u001b[31m -0.259\u001b[0m,  39% removed (alpha=   6.4052, nnz= 79/129) \u001b[33munder \u001b[0m [4.5723 6.9685]\n",
      "METRICS orig   tr -0.222\u001b[0m, va -0.233\u001b[0m \u001b[1m[te -0.234]\u001b[0m,  ntrees 100,  nnodes 10170,  nleafs  5135,  nnz  5133\n",
      "METRICS prev   \u001b[32mtr -0.241\u001b[0m, \u001b[32mva -0.254\u001b[0m \u001b[1m\u001b[31m[te -0.260]\u001b[0m,  ntrees  21,  nnodes  1531,  nleafs   776,  nnz   770\n",
      "METRICS now    \u001b[32mtr -0.241\u001b[0m, \u001b[32mva -0.252\u001b[0m \u001b[1m\u001b[32m[te -0.257]\u001b[0m,  ntrees  21,  nnodes  1343,  nleafs   682,  nnz   669\n",
      "\n",
      "Level 3 xxtrain.shape (11478, 216) dense transform time: 0.00s, 1.11s relerr 0.1\n",
      " -0.222  -0.233 -> \u001b[32m -0.231\u001b[0m \u001b[32m -0.238\u001b[0m,   2% removed (alpha=   0.0026, nnz=211/216) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.231\u001b[0m \u001b[32m -0.238\u001b[0m,   3% removed (alpha=   0.0067, nnz=210/216) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.230\u001b[0m \u001b[32m -0.237\u001b[0m,   2% removed (alpha=   0.0172, nnz=211/216) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.229\u001b[0m \u001b[32m -0.236\u001b[0m,   4% removed (alpha=   0.0444, nnz=208/216) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.228\u001b[0m \u001b[32m -0.235\u001b[0m,   4% removed (alpha=   0.1145, nnz=207/216) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.228\u001b[0m \u001b[32m -0.235\u001b[0m,   4% removed (alpha=   0.2955, nnz=207/216) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.221\u001b[0m \u001b[32m -0.232\u001b[0m,  16% removed (alpha=   0.7627, nnz=182/216) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.234\u001b[0m \u001b[32m -0.247\u001b[0m,  33% removed (alpha=   1.9684, nnz=145/216) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 670.1319662156635, tolerance: 507.3271805515732\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.222  -0.233 -> \u001b[31m -0.258\u001b[0m \u001b[31m -0.270\u001b[0m,  46% removed (alpha=   5.0802, nnz=117/216) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.314\u001b[0m \u001b[33m -0.329\u001b[0m,  58% removed (alpha=  13.1113, nnz= 91/216) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.381\u001b[0m \u001b[33m -0.393\u001b[0m,  69% removed (alpha=  33.8386, nnz= 67/216) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.488\u001b[0m \u001b[33m -0.500\u001b[0m,  78% removed (alpha=  87.3326, nnz= 47/216) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.668\u001b[0m \u001b[33m -0.673\u001b[0m,  84% removed (alpha= 225.3934, nnz= 34/216) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.976\u001b[0m \u001b[33m -0.968\u001b[0m,  90% removed (alpha= 581.7091, nnz= 21/216) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -1.543\u001b[0m \u001b[33m -1.503\u001b[0m,  97% removed (alpha=1501.3107, nnz=  7/216) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -1.650\u001b[0m \u001b[33m -1.628\u001b[0m,  97% removed (alpha=3874.6751, nnz=  7/216) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.224\u001b[0m \u001b[32m -0.234\u001b[0m,  23% removed (alpha=   0.9416, nnz=166/216) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.225\u001b[0m \u001b[32m -0.236\u001b[0m,  25% removed (alpha=   1.1624, nnz=161/216) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.227\u001b[0m \u001b[32m -0.237\u001b[0m,  32% removed (alpha=   1.4350, nnz=146/216) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.230\u001b[0m \u001b[32m -0.241\u001b[0m,  32% removed (alpha=   1.7716, nnz=146/216) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.233\u001b[0m \u001b[32m -0.244\u001b[0m,  32% removed (alpha=   2.1871, nnz=147/216) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.237\u001b[0m \u001b[32m -0.249\u001b[0m,  39% removed (alpha=   2.7001, nnz=132/216) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.243\u001b[0m \u001b[32m -0.255\u001b[0m,  42% removed (alpha=   3.3333, nnz=126/216) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[31m -0.249\u001b[0m \u001b[31m -0.262\u001b[0m,  44% removed (alpha=   4.1151, nnz=120/216) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.239\u001b[0m \u001b[32m -0.252\u001b[0m,  38% removed (alpha=   2.9375, nnz=133/216) \u001b[32mfit ok\u001b[0m [2.7001 4.1151]\n",
      " -0.222  -0.233 -> \u001b[32m -0.242\u001b[0m \u001b[32m -0.254\u001b[0m,  42% removed (alpha=   3.1958, nnz=125/216) \u001b[32mfit ok\u001b[0m [2.7001 4.1151]\n",
      " -0.222  -0.233 -> \u001b[32m -0.244\u001b[0m \u001b[32m -0.256\u001b[0m,  42% removed (alpha=   3.4768, nnz=125/216) \u001b[32mfit ok\u001b[0m [2.7001 4.1151]\n",
      " -0.222  -0.233 -> \u001b[31m -0.246\u001b[0m \u001b[31m -0.259\u001b[0m,  42% removed (alpha=   3.7825, nnz=125/216) \u001b[33munder \u001b[0m [2.7001 4.1151]\n",
      "METRICS orig   tr -0.222\u001b[0m, va -0.233\u001b[0m \u001b[1m[te -0.234]\u001b[0m,  ntrees 100,  nnodes 10170,  nleafs  5135,  nnz  5133\n",
      "METRICS prev   \u001b[32mtr -0.241\u001b[0m, \u001b[32mva -0.252\u001b[0m \u001b[1m\u001b[32m[te -0.257]\u001b[0m,  ntrees  21,  nnodes  1343,  nleafs   682,  nnz   669\n",
      "METRICS now    \u001b[32mtr -0.242\u001b[0m, \u001b[32mva -0.254\u001b[0m \u001b[1m\u001b[31m[te -0.261]\u001b[0m,  ntrees  20,  nnodes  1092,  nleafs   556,  nnz   539\n",
      "\n",
      "Level 4 xxtrain.shape (11478, 329) dense transform time: 0.00s, 1.07s relerr 0.1\n",
      " -0.222  -0.233 -> \u001b[31m -0.249\u001b[0m \u001b[31m -0.259\u001b[0m,   1% removed (alpha=   0.0026, nnz=325/329) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[31m -0.248\u001b[0m \u001b[31m -0.259\u001b[0m,   1% removed (alpha=   0.0067, nnz=325/329) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[31m -0.247\u001b[0m \u001b[31m -0.257\u001b[0m,   2% removed (alpha=   0.0172, nnz=324/329) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.244\u001b[0m \u001b[32m -0.255\u001b[0m,   2% removed (alpha=   0.0444, nnz=324/329) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.240\u001b[0m \u001b[32m -0.252\u001b[0m,   4% removed (alpha=   0.1145, nnz=316/329) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.212\u001b[0m \u001b[32m -0.228\u001b[0m,  16% removed (alpha=   0.2955, nnz=278/329) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 581.2320116840771, tolerance: 507.3271805515732\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 828.2989426421625, tolerance: 507.3271805515732\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.222  -0.233 -> \u001b[32m -0.221\u001b[0m \u001b[32m -0.237\u001b[0m,  25% removed (alpha=   0.7627, nnz=248/329) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.239\u001b[0m \u001b[32m -0.253\u001b[0m,  33% removed (alpha=   1.9684, nnz=221/329) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.275\u001b[0m \u001b[33m -0.290\u001b[0m,  47% removed (alpha=   5.0802, nnz=175/329) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.334\u001b[0m \u001b[33m -0.344\u001b[0m,  60% removed (alpha=  13.1113, nnz=133/329) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.464\u001b[0m \u001b[33m -0.459\u001b[0m,  69% removed (alpha=  33.8386, nnz=103/329) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.751\u001b[0m \u001b[33m -0.709\u001b[0m,  77% removed (alpha=  87.3326, nnz= 75/329) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -1.127\u001b[0m \u001b[33m -1.054\u001b[0m,  84% removed (alpha= 225.3934, nnz= 52/329) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -1.584\u001b[0m \u001b[33m -1.518\u001b[0m,  89% removed (alpha= 581.7091, nnz= 37/329) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -2.179\u001b[0m \u001b[33m -2.175\u001b[0m,  94% removed (alpha=1501.3107, nnz= 19/329) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -2.921\u001b[0m \u001b[33m -2.956\u001b[0m,  97% removed (alpha=3874.6751, nnz= 11/329) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 544.6528722547409, tolerance: 507.3271805515732\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.222  -0.233 -> \u001b[32m -0.224\u001b[0m \u001b[32m -0.239\u001b[0m,  26% removed (alpha=   0.9416, nnz=244/329) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 509.5551224939589, tolerance: 507.3271805515732\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.222  -0.233 -> \u001b[32m -0.227\u001b[0m \u001b[32m -0.241\u001b[0m,  31% removed (alpha=   1.1624, nnz=227/329) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.230\u001b[0m \u001b[32m -0.245\u001b[0m,  35% removed (alpha=   1.4350, nnz=214/329) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.234\u001b[0m \u001b[32m -0.249\u001b[0m,  37% removed (alpha=   1.7716, nnz=207/329) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.240\u001b[0m \u001b[32m -0.254\u001b[0m,  37% removed (alpha=   2.1871, nnz=207/329) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[31m -0.247\u001b[0m \u001b[31m -0.262\u001b[0m,  39% removed (alpha=   2.7001, nnz=200/329) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[31m -0.255\u001b[0m \u001b[31m -0.270\u001b[0m,  41% removed (alpha=   3.3333, nnz=195/329) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[31m -0.264\u001b[0m \u001b[31m -0.279\u001b[0m,  43% removed (alpha=   4.1151, nnz=186/329) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.237\u001b[0m \u001b[32m -0.251\u001b[0m,  36% removed (alpha=   1.9274, nnz=211/329) \u001b[32mfit ok\u001b[0m [1.7716 2.7001]\n",
      " -0.222  -0.233 -> \u001b[32m -0.238\u001b[0m \u001b[32m -0.253\u001b[0m,  36% removed (alpha=   2.0969, nnz=212/329) \u001b[32mfit ok\u001b[0m [1.7716 2.7001]\n",
      " -0.222  -0.233 -> \u001b[32m -0.241\u001b[0m \u001b[32m -0.255\u001b[0m,  37% removed (alpha=   2.2812, nnz=207/329) \u001b[32mfit ok\u001b[0m [1.7716 2.7001]\n",
      " -0.222  -0.233 -> \u001b[32m -0.243\u001b[0m \u001b[31m -0.258\u001b[0m,  37% removed (alpha=   2.4818, nnz=208/329) \u001b[32mfit ok\u001b[0m [1.7716 2.7001]\n",
      "METRICS orig   tr -0.222\u001b[0m, va -0.233\u001b[0m \u001b[1m[te -0.234]\u001b[0m,  ntrees 100,  nnodes 10170,  nleafs  5135,  nnz  5133\n",
      "METRICS prev   \u001b[32mtr -0.242\u001b[0m, \u001b[32mva -0.254\u001b[0m \u001b[1m\u001b[31m[te -0.261]\u001b[0m,  ntrees  20,  nnodes  1092,  nleafs   556,  nnz   539\n",
      "METRICS now    \u001b[32mtr -0.234\u001b[0m, \u001b[32mva -0.249\u001b[0m \u001b[1m\u001b[32m[te -0.255]\u001b[0m,  ntrees  20,  nnodes   898,  nleafs   459,  nnz   435\n",
      "\n",
      "Level 5 xxtrain.shape (11478, 459) dense transform time: 0.00s, 1.06s relerr 0.1\n",
      " -0.222  -0.233 -> \u001b[32m -0.213\u001b[0m \u001b[32m -0.232\u001b[0m,   1% removed (alpha=   0.0026, nnz=454/459) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.212\u001b[0m \u001b[32m -0.231\u001b[0m,   1% removed (alpha=   0.0067, nnz=454/459) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.210\u001b[0m \u001b[32m -0.229\u001b[0m,   3% removed (alpha=   0.0172, nnz=446/459) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.208\u001b[0m \u001b[32m -0.227\u001b[0m,   7% removed (alpha=   0.0444, nnz=429/459) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.207\u001b[0m \u001b[32m -0.225\u001b[0m,  12% removed (alpha=   0.1145, nnz=405/459) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.208\u001b[0m \u001b[32m -0.227\u001b[0m,  24% removed (alpha=   0.2955, nnz=349/459) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 625.8196354160171, tolerance: 507.3271805515732\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.222  -0.233 -> \u001b[32m -0.218\u001b[0m \u001b[32m -0.234\u001b[0m,  35% removed (alpha=   0.7627, nnz=298/459) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 964.7725802308646, tolerance: 507.3271805515732\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.222  -0.233 -> \u001b[32m -0.238\u001b[0m \u001b[32m -0.248\u001b[0m,  43% removed (alpha=   1.9684, nnz=263/459) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.277\u001b[0m \u001b[33m -0.282\u001b[0m,  53% removed (alpha=   5.0802, nnz=217/459) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.357\u001b[0m \u001b[33m -0.360\u001b[0m,  59% removed (alpha=  13.1113, nnz=187/459) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.492\u001b[0m \u001b[33m -0.502\u001b[0m,  68% removed (alpha=  33.8386, nnz=148/459) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.743\u001b[0m \u001b[33m -0.744\u001b[0m,  75% removed (alpha=  87.3326, nnz=113/459) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -1.181\u001b[0m \u001b[33m -1.161\u001b[0m,  84% removed (alpha= 225.3934, nnz= 74/459) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -2.077\u001b[0m \u001b[33m -1.976\u001b[0m,  88% removed (alpha= 581.7091, nnz= 54/459) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -3.753\u001b[0m \u001b[33m -3.546\u001b[0m,  92% removed (alpha=1501.3107, nnz= 39/459) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -6.271\u001b[0m \u001b[33m -5.880\u001b[0m,  95% removed (alpha=3874.6751, nnz= 22/459) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 548.5646633864421, tolerance: 507.3271805515732\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.222  -0.233 -> \u001b[32m -0.221\u001b[0m \u001b[32m -0.233\u001b[0m,  35% removed (alpha=   0.9416, nnz=298/459) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.224\u001b[0m \u001b[32m -0.236\u001b[0m,  37% removed (alpha=   1.1624, nnz=290/459) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.228\u001b[0m \u001b[32m -0.238\u001b[0m,  38% removed (alpha=   1.4350, nnz=283/459) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.232\u001b[0m \u001b[32m -0.242\u001b[0m,  39% removed (alpha=   1.7716, nnz=278/459) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.237\u001b[0m \u001b[32m -0.247\u001b[0m,  42% removed (alpha=   2.1871, nnz=268/459) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[31m -0.249\u001b[0m \u001b[31m -0.256\u001b[0m,  46% removed (alpha=   2.7001, nnz=248/459) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[31m -0.256\u001b[0m \u001b[31m -0.263\u001b[0m,  48% removed (alpha=   3.3333, nnz=238/459) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[31m -0.264\u001b[0m \u001b[31m -0.270\u001b[0m,  51% removed (alpha=   4.1151, nnz=227/459) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.242\u001b[0m \u001b[32m -0.249\u001b[0m,  42% removed (alpha=   1.9274, nnz=264/459) \u001b[32mfit ok\u001b[0m [1.7716 2.7001]\n",
      " -0.222  -0.233 -> \u001b[32m -0.243\u001b[0m \u001b[32m -0.250\u001b[0m,  42% removed (alpha=   2.0969, nnz=264/459) \u001b[32mfit ok\u001b[0m [1.7716 2.7001]\n",
      " -0.222  -0.233 -> \u001b[32m -0.245\u001b[0m \u001b[32m -0.252\u001b[0m,  43% removed (alpha=   2.2812, nnz=261/459) \u001b[32mfit ok\u001b[0m [1.7716 2.7001]\n",
      " -0.222  -0.233 -> \u001b[31m -0.247\u001b[0m \u001b[32m -0.254\u001b[0m,  43% removed (alpha=   2.4818, nnz=263/459) \u001b[32mfit ok\u001b[0m [1.7716 2.7001]\n",
      "METRICS orig   tr -0.222\u001b[0m, va -0.233\u001b[0m \u001b[1m[te -0.234]\u001b[0m,  ntrees 100,  nnodes 10170,  nleafs  5135,  nnz  5133\n",
      "METRICS prev   \u001b[32mtr -0.234\u001b[0m, \u001b[32mva -0.249\u001b[0m \u001b[1m\u001b[32m[te -0.255]\u001b[0m,  ntrees  20,  nnodes   898,  nleafs   459,  nnz   435\n",
      "METRICS now    \u001b[32mtr -0.245\u001b[0m, \u001b[32mva -0.252\u001b[0m \u001b[1m\u001b[31m[te -0.267]\u001b[0m,  ntrees  20,  nnodes   712,  nleafs   366,  nnz   308\n",
      "\n",
      "Level 6 xxtrain.shape (11478, 366) dense transform time: 0.00s, 1.02s relerr 0.1\n",
      " -0.222  -0.233 -> \u001b[32m -0.220\u001b[0m \u001b[32m -0.236\u001b[0m,   1% removed (alpha=   0.0026, nnz=362/366) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.220\u001b[0m \u001b[32m -0.236\u001b[0m,   1% removed (alpha=   0.0067, nnz=362/366) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.220\u001b[0m \u001b[32m -0.236\u001b[0m,   1% removed (alpha=   0.0172, nnz=362/366) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.220\u001b[0m \u001b[32m -0.236\u001b[0m,   3% removed (alpha=   0.0444, nnz=356/366) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.220\u001b[0m \u001b[32m -0.236\u001b[0m,   6% removed (alpha=   0.1145, nnz=344/366) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.220\u001b[0m \u001b[32m -0.236\u001b[0m,  17% removed (alpha=   0.2955, nnz=302/366) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.221\u001b[0m \u001b[32m -0.236\u001b[0m,  21% removed (alpha=   0.7627, nnz=289/366) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 565.6137108840882, tolerance: 507.3271805515732\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.222  -0.233 -> \u001b[32m -0.229\u001b[0m \u001b[32m -0.242\u001b[0m,  25% removed (alpha=   1.9684, nnz=274/366) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[31m -0.253\u001b[0m \u001b[31m -0.269\u001b[0m,  30% removed (alpha=   5.0802, nnz=258/366) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.332\u001b[0m \u001b[33m -0.351\u001b[0m,  41% removed (alpha=  13.1113, nnz=215/366) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.491\u001b[0m \u001b[33m -0.524\u001b[0m,  53% removed (alpha=  33.8386, nnz=171/366) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.797\u001b[0m \u001b[33m -0.871\u001b[0m,  66% removed (alpha=  87.3326, nnz=124/366) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -1.292\u001b[0m \u001b[33m -1.377\u001b[0m,  76% removed (alpha= 225.3934, nnz= 88/366) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -2.176\u001b[0m \u001b[33m -2.225\u001b[0m,  84% removed (alpha= 581.7091, nnz= 57/366) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -4.305\u001b[0m \u001b[33m -4.291\u001b[0m,  90% removed (alpha=1501.3107, nnz= 37/366) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -6.934\u001b[0m \u001b[33m -6.944\u001b[0m,  95% removed (alpha=3874.6751, nnz= 17/366) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.223\u001b[0m \u001b[32m -0.237\u001b[0m,  19% removed (alpha=   0.9416, nnz=295/366) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.224\u001b[0m \u001b[32m -0.238\u001b[0m,  20% removed (alpha=   1.1624, nnz=292/366) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.226\u001b[0m \u001b[32m -0.240\u001b[0m,  24% removed (alpha=   1.4350, nnz=279/366) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.228\u001b[0m \u001b[32m -0.242\u001b[0m,  24% removed (alpha=   1.7716, nnz=278/366) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.230\u001b[0m \u001b[32m -0.245\u001b[0m,  25% removed (alpha=   2.1871, nnz=273/366) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.233\u001b[0m \u001b[32m -0.248\u001b[0m,  28% removed (alpha=   2.7001, nnz=265/366) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.237\u001b[0m \u001b[32m -0.252\u001b[0m,  29% removed (alpha=   3.3333, nnz=261/366) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.243\u001b[0m \u001b[31m -0.258\u001b[0m,  29% removed (alpha=   4.1151, nnz=261/366) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.235\u001b[0m \u001b[32m -0.250\u001b[0m,  25% removed (alpha=   2.9375, nnz=273/366) \u001b[32mfit ok\u001b[0m [2.7001 4.1151]\n",
      " -0.222  -0.233 -> \u001b[32m -0.237\u001b[0m \u001b[32m -0.252\u001b[0m,  27% removed (alpha=   3.1958, nnz=269/366) \u001b[32mfit ok\u001b[0m [2.7001 4.1151]\n",
      " -0.222  -0.233 -> \u001b[32m -0.239\u001b[0m \u001b[32m -0.254\u001b[0m,  27% removed (alpha=   3.4768, nnz=266/366) \u001b[32mfit ok\u001b[0m [2.7001 4.1151]\n",
      " -0.222  -0.233 -> \u001b[32m -0.241\u001b[0m \u001b[32m -0.256\u001b[0m,  28% removed (alpha=   3.7825, nnz=265/366) \u001b[32mfit ok\u001b[0m [2.7001 4.1151]\n",
      "METRICS orig   tr -0.222\u001b[0m, va -0.233\u001b[0m \u001b[1m[te -0.234]\u001b[0m,  ntrees 100,  nnodes 10170,  nleafs  5135,  nnz  5133\n",
      "METRICS prev   \u001b[32mtr -0.245\u001b[0m, \u001b[32mva -0.252\u001b[0m \u001b[1m\u001b[31m[te -0.267]\u001b[0m,  ntrees  20,  nnodes   712,  nleafs   366,  nnz   308\n",
      "METRICS now    \u001b[32mtr -0.237\u001b[0m, \u001b[32mva -0.252\u001b[0m \u001b[1m\u001b[31m[te -0.258]\u001b[0m,  ntrees  20,  nnodes   694,  nleafs   357,  nnz   261\n",
      "\n",
      "DONE, depth of tree reached 6, 6\n",
      "\n",
      "\n",
      "ROUND 2\n",
      "Level 0 xxtrain.shape (11478, 20) dense transform time: 0.00s, 1.06s relerr 0.1\n",
      " -0.222  -0.233 -> \u001b[32m -0.237\u001b[0m \u001b[32m -0.252\u001b[0m,   0% removed (alpha=   0.0026, nnz=20/20) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.237\u001b[0m \u001b[32m -0.252\u001b[0m,   0% removed (alpha=   0.0067, nnz=20/20) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.237\u001b[0m \u001b[32m -0.252\u001b[0m,   0% removed (alpha=   0.0172, nnz=20/20) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.237\u001b[0m \u001b[32m -0.252\u001b[0m,   0% removed (alpha=   0.0444, nnz=20/20) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.237\u001b[0m \u001b[32m -0.252\u001b[0m,   0% removed (alpha=   0.1145, nnz=20/20) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.237\u001b[0m \u001b[32m -0.252\u001b[0m,   0% removed (alpha=   0.2955, nnz=20/20) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.237\u001b[0m \u001b[32m -0.252\u001b[0m,   0% removed (alpha=   0.7627, nnz=20/20) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.237\u001b[0m \u001b[32m -0.253\u001b[0m,   5% removed (alpha=   1.9684, nnz=19/20) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.239\u001b[0m \u001b[32m -0.255\u001b[0m,  10% removed (alpha=   5.0802, nnz=18/20) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.242\u001b[0m \u001b[31m -0.258\u001b[0m,  10% removed (alpha=  13.1113, nnz=18/20) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[31m -0.262\u001b[0m \u001b[31m -0.275\u001b[0m,  15% removed (alpha=  33.8386, nnz=17/20) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.296\u001b[0m \u001b[33m -0.306\u001b[0m,  30% removed (alpha=  87.3326, nnz=14/20) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.372\u001b[0m \u001b[33m -0.377\u001b[0m,  45% removed (alpha= 225.3934, nnz=11/20) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.406\u001b[0m \u001b[33m -0.412\u001b[0m,  45% removed (alpha= 581.7091, nnz=11/20) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.545\u001b[0m \u001b[33m -0.553\u001b[0m,  50% removed (alpha=1501.3107, nnz=10/20) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.830\u001b[0m \u001b[33m -0.831\u001b[0m,  50% removed (alpha=3874.6751, nnz=10/20) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.237\u001b[0m \u001b[32m -0.253\u001b[0m,   5% removed (alpha=   2.4301, nnz=19/20) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.222  -0.233 -> \u001b[32m -0.238\u001b[0m \u001b[32m -0.253\u001b[0m,   5% removed (alpha=   3.0000, nnz=19/20) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.222  -0.233 -> \u001b[32m -0.238\u001b[0m \u001b[32m -0.254\u001b[0m,   5% removed (alpha=   3.7036, nnz=19/20) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.222  -0.233 -> \u001b[32m -0.238\u001b[0m \u001b[32m -0.254\u001b[0m,   5% removed (alpha=   4.5723, nnz=19/20) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.222  -0.233 -> \u001b[32m -0.239\u001b[0m \u001b[32m -0.255\u001b[0m,  10% removed (alpha=   5.6446, nnz=18/20) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.222  -0.233 -> \u001b[32m -0.239\u001b[0m \u001b[32m -0.255\u001b[0m,  10% removed (alpha=   6.9685, nnz=18/20) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.222  -0.233 -> \u001b[32m -0.240\u001b[0m \u001b[32m -0.256\u001b[0m,  10% removed (alpha=   8.6028, nnz=18/20) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.222  -0.233 -> \u001b[32m -0.241\u001b[0m \u001b[31m -0.256\u001b[0m,  10% removed (alpha=  10.6205, nnz=18/20) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.222  -0.233 -> \u001b[32m -0.240\u001b[0m \u001b[32m -0.255\u001b[0m,  10% removed (alpha=   7.5812, nnz=18/20) \u001b[32mfit ok\u001b[0m [ 6.9685 10.6205]\n",
      " -0.222  -0.233 -> \u001b[32m -0.240\u001b[0m \u001b[32m -0.255\u001b[0m,  10% removed (alpha=   8.2478, nnz=18/20) \u001b[32mfit ok\u001b[0m [ 6.9685 10.6205]\n",
      " -0.222  -0.233 -> \u001b[32m -0.240\u001b[0m \u001b[32m -0.256\u001b[0m,  10% removed (alpha=   8.9731, nnz=18/20) \u001b[32mfit ok\u001b[0m [ 6.9685 10.6205]\n",
      " -0.222  -0.233 -> \u001b[32m -0.240\u001b[0m \u001b[31m -0.256\u001b[0m,  10% removed (alpha=   9.7621, nnz=18/20) \u001b[32mfit ok\u001b[0m [ 6.9685 10.6205]\n",
      "METRICS orig   tr -0.222\u001b[0m, va -0.233\u001b[0m \u001b[1m[te -0.234]\u001b[0m,  ntrees 100,  nnodes 10170,  nleafs  5135,  nnz  5133\n",
      "METRICS prev   \u001b[32mtr -0.237\u001b[0m, \u001b[32mva -0.252\u001b[0m \u001b[1m\u001b[31m[te -0.258]\u001b[0m,  ntrees  20,  nnodes   694,  nleafs   357,  nnz   261\n",
      "METRICS now    \u001b[32mtr -0.239\u001b[0m, \u001b[32mva -0.255\u001b[0m \u001b[1m\u001b[31m[te -0.259]\u001b[0m,  ntrees  18,  nnodes   686,  nleafs   352,  nnz   258\n",
      "\n",
      "Level 1 xxtrain.shape (11478, 64) dense transform time: 0.00s, 0.95s relerr 0.1\n",
      " -0.222  -0.233 -> \u001b[32m -0.237\u001b[0m \u001b[32m -0.252\u001b[0m,   3% removed (alpha=   0.0026, nnz=62/64) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.237\u001b[0m \u001b[32m -0.252\u001b[0m,   3% removed (alpha=   0.0067, nnz=62/64) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.237\u001b[0m \u001b[32m -0.252\u001b[0m,   3% removed (alpha=   0.0172, nnz=62/64) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.237\u001b[0m \u001b[32m -0.252\u001b[0m,   3% removed (alpha=   0.0444, nnz=62/64) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.237\u001b[0m \u001b[32m -0.252\u001b[0m,   3% removed (alpha=   0.1145, nnz=62/64) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.237\u001b[0m \u001b[32m -0.252\u001b[0m,   5% removed (alpha=   0.2955, nnz=61/64) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.237\u001b[0m \u001b[32m -0.252\u001b[0m,   6% removed (alpha=   0.7627, nnz=60/64) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.237\u001b[0m \u001b[32m -0.252\u001b[0m,  12% removed (alpha=   1.9684, nnz=56/64) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.238\u001b[0m \u001b[32m -0.253\u001b[0m,  38% removed (alpha=   5.0802, nnz=40/64) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[31m -0.246\u001b[0m \u001b[31m -0.261\u001b[0m,  38% removed (alpha=  13.1113, nnz=40/64) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.281\u001b[0m \u001b[33m -0.296\u001b[0m,  44% removed (alpha=  33.8386, nnz=36/64) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.361\u001b[0m \u001b[33m -0.376\u001b[0m,  55% removed (alpha=  87.3326, nnz=29/64) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.454\u001b[0m \u001b[33m -0.468\u001b[0m,  62% removed (alpha= 225.3934, nnz=24/64) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.591\u001b[0m \u001b[33m -0.601\u001b[0m,  70% removed (alpha= 581.7091, nnz=19/64) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.798\u001b[0m \u001b[33m -0.815\u001b[0m,  80% removed (alpha=1501.3107, nnz=13/64) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -1.170\u001b[0m \u001b[33m -1.213\u001b[0m,  81% removed (alpha=3874.6751, nnz=12/64) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.237\u001b[0m \u001b[32m -0.252\u001b[0m,  38% removed (alpha=   2.4301, nnz=40/64) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.222  -0.233 -> \u001b[32m -0.237\u001b[0m \u001b[32m -0.252\u001b[0m,  38% removed (alpha=   3.0000, nnz=40/64) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.222  -0.233 -> \u001b[32m -0.237\u001b[0m \u001b[32m -0.253\u001b[0m,  41% removed (alpha=   3.7036, nnz=38/64) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.222  -0.233 -> \u001b[32m -0.238\u001b[0m \u001b[32m -0.253\u001b[0m,  39% removed (alpha=   4.5723, nnz=39/64) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.222  -0.233 -> \u001b[32m -0.238\u001b[0m \u001b[32m -0.254\u001b[0m,  38% removed (alpha=   5.6446, nnz=40/64) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.222  -0.233 -> \u001b[32m -0.239\u001b[0m \u001b[32m -0.255\u001b[0m,  38% removed (alpha=   6.9685, nnz=40/64) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.222  -0.233 -> \u001b[32m -0.241\u001b[0m \u001b[32m -0.256\u001b[0m,  38% removed (alpha=   8.6028, nnz=40/64) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.222  -0.233 -> \u001b[32m -0.243\u001b[0m \u001b[31m -0.258\u001b[0m,  38% removed (alpha=  10.6205, nnz=40/64) \u001b[32mfit ok\u001b[0m [ 1.9684 13.1113]\n",
      " -0.222  -0.233 -> \u001b[32m -0.240\u001b[0m \u001b[32m -0.255\u001b[0m,  38% removed (alpha=   7.5812, nnz=40/64) \u001b[32mfit ok\u001b[0m [ 6.9685 10.6205]\n",
      " -0.222  -0.233 -> \u001b[32m -0.240\u001b[0m \u001b[32m -0.256\u001b[0m,  38% removed (alpha=   8.2478, nnz=40/64) \u001b[32mfit ok\u001b[0m [ 6.9685 10.6205]\n",
      " -0.222  -0.233 -> \u001b[32m -0.241\u001b[0m \u001b[31m -0.256\u001b[0m,  38% removed (alpha=   8.9731, nnz=40/64) \u001b[32mfit ok\u001b[0m [ 6.9685 10.6205]\n",
      " -0.222  -0.233 -> \u001b[32m -0.242\u001b[0m \u001b[31m -0.257\u001b[0m,  38% removed (alpha=   9.7621, nnz=40/64) \u001b[32mfit ok\u001b[0m [ 6.9685 10.6205]\n",
      "METRICS orig   tr -0.222\u001b[0m, va -0.233\u001b[0m \u001b[1m[te -0.234]\u001b[0m,  ntrees 100,  nnodes 10170,  nleafs  5135,  nnz  5133\n",
      "METRICS prev   \u001b[32mtr -0.239\u001b[0m, \u001b[32mva -0.255\u001b[0m \u001b[1m\u001b[31m[te -0.259]\u001b[0m,  ntrees  18,  nnodes   686,  nleafs   352,  nnz   258\n",
      "METRICS now    \u001b[32mtr -0.237\u001b[0m, \u001b[32mva -0.253\u001b[0m \u001b[1m\u001b[31m[te -0.259]\u001b[0m,  ntrees  18,  nnodes   686,  nleafs   352,  nnz   267\n",
      "\n",
      "Level 2 xxtrain.shape (11478, 113) dense transform time: 0.00s, 0.97s relerr 0.1\n",
      " -0.222  -0.233 -> \u001b[32m -0.244\u001b[0m \u001b[31m -0.258\u001b[0m,   2% removed (alpha=   0.0026, nnz=111/113) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.244\u001b[0m \u001b[31m -0.258\u001b[0m,   2% removed (alpha=   0.0067, nnz=111/113) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.243\u001b[0m \u001b[31m -0.257\u001b[0m,   2% removed (alpha=   0.0172, nnz=111/113) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.240\u001b[0m \u001b[32m -0.254\u001b[0m,   4% removed (alpha=   0.0444, nnz=109/113) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.238\u001b[0m \u001b[32m -0.253\u001b[0m,   4% removed (alpha=   0.1145, nnz=108/113) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.238\u001b[0m \u001b[32m -0.252\u001b[0m,   4% removed (alpha=   0.2955, nnz=109/113) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.238\u001b[0m \u001b[32m -0.252\u001b[0m,   6% removed (alpha=   0.7627, nnz=106/113) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.237\u001b[0m \u001b[32m -0.252\u001b[0m,  27% removed (alpha=   1.9684, nnz= 82/113) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 613.5264911012663, tolerance: 507.3271805515732\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 711.383222578341, tolerance: 507.3271805515732\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.222  -0.233 -> \u001b[32m -0.243\u001b[0m \u001b[31m -0.258\u001b[0m,  31% removed (alpha=   5.0802, nnz= 78/113) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[31m -0.262\u001b[0m \u001b[31m -0.277\u001b[0m,  46% removed (alpha=  13.1113, nnz= 61/113) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.317\u001b[0m \u001b[33m -0.332\u001b[0m,  50% removed (alpha=  33.8386, nnz= 57/113) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.437\u001b[0m \u001b[33m -0.444\u001b[0m,  59% removed (alpha=  87.3326, nnz= 46/113) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.603\u001b[0m \u001b[33m -0.601\u001b[0m,  65% removed (alpha= 225.3934, nnz= 39/113) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.967\u001b[0m \u001b[33m -0.954\u001b[0m,  73% removed (alpha= 581.7091, nnz= 30/113) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -1.617\u001b[0m \u001b[33m -1.583\u001b[0m,  84% removed (alpha=1501.3107, nnz= 18/113) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -2.423\u001b[0m \u001b[33m -2.432\u001b[0m,  86% removed (alpha=3874.6751, nnz= 16/113) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.236\u001b[0m \u001b[32m -0.250\u001b[0m,  18% removed (alpha=   0.9416, nnz= 93/113) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.236\u001b[0m \u001b[32m -0.250\u001b[0m,  17% removed (alpha=   1.1624, nnz= 94/113) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.236\u001b[0m \u001b[32m -0.250\u001b[0m,  17% removed (alpha=   1.4350, nnz= 94/113) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.236\u001b[0m \u001b[32m -0.250\u001b[0m,  19% removed (alpha=   1.7716, nnz= 91/113) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.237\u001b[0m \u001b[32m -0.251\u001b[0m,  19% removed (alpha=   2.1871, nnz= 91/113) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.238\u001b[0m \u001b[32m -0.252\u001b[0m,  21% removed (alpha=   2.7001, nnz= 89/113) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.239\u001b[0m \u001b[32m -0.253\u001b[0m,  29% removed (alpha=   3.3333, nnz= 80/113) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.240\u001b[0m \u001b[32m -0.255\u001b[0m,  30% removed (alpha=   4.1151, nnz= 79/113) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.239\u001b[0m \u001b[32m -0.254\u001b[0m,  28% removed (alpha=   3.6264, nnz= 81/113) \u001b[32mfit ok\u001b[0m [3.3333 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.240\u001b[0m \u001b[32m -0.254\u001b[0m,  29% removed (alpha=   3.9453, nnz= 80/113) \u001b[32mfit ok\u001b[0m [3.3333 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.241\u001b[0m \u001b[32m -0.255\u001b[0m,  31% removed (alpha=   4.2922, nnz= 78/113) \u001b[32mfit ok\u001b[0m [3.3333 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.241\u001b[0m \u001b[32m -0.256\u001b[0m,  34% removed (alpha=   4.6696, nnz= 75/113) \u001b[32mfit ok\u001b[0m [3.3333 5.0802]\n",
      "METRICS orig   tr -0.222\u001b[0m, va -0.233\u001b[0m \u001b[1m[te -0.234]\u001b[0m,  ntrees 100,  nnodes 10170,  nleafs  5135,  nnz  5133\n",
      "METRICS prev   \u001b[32mtr -0.237\u001b[0m, \u001b[32mva -0.253\u001b[0m \u001b[1m\u001b[31m[te -0.259]\u001b[0m,  ntrees  18,  nnodes   686,  nleafs   352,  nnz   267\n",
      "METRICS now    \u001b[32mtr -0.241\u001b[0m, \u001b[32mva -0.256\u001b[0m \u001b[1m\u001b[31m[te -0.261]\u001b[0m,  ntrees  18,  nnodes   672,  nleafs   345,  nnz   285\n",
      "\n",
      "Level 3 xxtrain.shape (11478, 177) dense transform time: 0.00s, 0.96s relerr 0.1\n",
      " -0.222  -0.233 -> \u001b[31m -0.248\u001b[0m \u001b[31m -0.262\u001b[0m,   2% removed (alpha=   0.0026, nnz=174/177) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[31m -0.248\u001b[0m \u001b[31m -0.262\u001b[0m,   1% removed (alpha=   0.0067, nnz=175/177) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[31m -0.247\u001b[0m \u001b[31m -0.261\u001b[0m,   1% removed (alpha=   0.0172, nnz=175/177) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.240\u001b[0m \u001b[32m -0.254\u001b[0m,   3% removed (alpha=   0.0444, nnz=172/177) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.240\u001b[0m \u001b[32m -0.254\u001b[0m,   2% removed (alpha=   0.1145, nnz=173/177) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.237\u001b[0m \u001b[32m -0.251\u001b[0m,   7% removed (alpha=   0.2955, nnz=165/177) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.236\u001b[0m \u001b[32m -0.251\u001b[0m,  18% removed (alpha=   0.7627, nnz=146/177) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 740.0177048490914, tolerance: 507.3271805515732\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 668.3059057806846, tolerance: 507.3271805515732\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.222  -0.233 -> \u001b[32m -0.241\u001b[0m \u001b[32m -0.255\u001b[0m,  28% removed (alpha=   1.9684, nnz=128/177) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[31m -0.255\u001b[0m \u001b[31m -0.269\u001b[0m,  38% removed (alpha=   5.0802, nnz=110/177) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.290\u001b[0m \u001b[33m -0.303\u001b[0m,  44% removed (alpha=  13.1113, nnz= 99/177) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.368\u001b[0m \u001b[33m -0.374\u001b[0m,  54% removed (alpha=  33.8386, nnz= 82/177) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.506\u001b[0m \u001b[33m -0.515\u001b[0m,  62% removed (alpha=  87.3326, nnz= 68/177) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.706\u001b[0m \u001b[33m -0.731\u001b[0m,  71% removed (alpha= 225.3934, nnz= 51/177) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -1.113\u001b[0m \u001b[33m -1.147\u001b[0m,  77% removed (alpha= 581.7091, nnz= 41/177) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -1.997\u001b[0m \u001b[33m -2.065\u001b[0m,  85% removed (alpha=1501.3107, nnz= 26/177) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -3.266\u001b[0m \u001b[33m -3.311\u001b[0m,  92% removed (alpha=3874.6751, nnz= 14/177) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.237\u001b[0m \u001b[32m -0.252\u001b[0m,  21% removed (alpha=   0.9416, nnz=139/177) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.237\u001b[0m \u001b[32m -0.252\u001b[0m,  25% removed (alpha=   1.1624, nnz=133/177) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.238\u001b[0m \u001b[32m -0.252\u001b[0m,  30% removed (alpha=   1.4350, nnz=124/177) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.239\u001b[0m \u001b[32m -0.253\u001b[0m,  30% removed (alpha=   1.7716, nnz=124/177) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.241\u001b[0m \u001b[32m -0.254\u001b[0m,  31% removed (alpha=   2.1871, nnz=123/177) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.242\u001b[0m \u001b[32m -0.256\u001b[0m,  34% removed (alpha=   2.7001, nnz=117/177) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[31m -0.246\u001b[0m \u001b[31m -0.260\u001b[0m,  38% removed (alpha=   3.3333, nnz=110/177) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[31m -0.249\u001b[0m \u001b[31m -0.263\u001b[0m,  40% removed (alpha=   4.1151, nnz=106/177) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.243\u001b[0m \u001b[31m -0.258\u001b[0m,  34% removed (alpha=   2.3794, nnz=116/177) \u001b[32mfit ok\u001b[0m [2.1871 3.3333]\n",
      " -0.222  -0.233 -> \u001b[32m -0.244\u001b[0m \u001b[31m -0.258\u001b[0m,  36% removed (alpha=   2.5886, nnz=113/177) \u001b[32mfit ok\u001b[0m [2.1871 3.3333]\n",
      " -0.222  -0.233 -> \u001b[32m -0.244\u001b[0m \u001b[31m -0.259\u001b[0m,  38% removed (alpha=   2.8163, nnz=110/177) \u001b[32mfit ok\u001b[0m [2.1871 3.3333]\n",
      " -0.222  -0.233 -> \u001b[31m -0.245\u001b[0m \u001b[31m -0.259\u001b[0m,  38% removed (alpha=   3.0639, nnz=109/177) \u001b[33munder \u001b[0m [2.1871 3.3333]\n",
      "METRICS orig   tr -0.222\u001b[0m, va -0.233\u001b[0m \u001b[1m[te -0.234]\u001b[0m,  ntrees 100,  nnodes 10170,  nleafs  5135,  nnz  5133\n",
      "METRICS prev   \u001b[32mtr -0.241\u001b[0m, \u001b[32mva -0.256\u001b[0m \u001b[1m\u001b[31m[te -0.261]\u001b[0m,  ntrees  18,  nnodes   672,  nleafs   345,  nnz   285\n",
      "METRICS now    \u001b[32mtr -0.242\u001b[0m, \u001b[32mva -0.256\u001b[0m \u001b[1m\u001b[31m[te -0.261]\u001b[0m,  ntrees  18,  nnodes   646,  nleafs   332,  nnz   295\n",
      "\n",
      "Level 4 xxtrain.shape (11478, 263) dense transform time: 0.00s, 0.97s relerr 0.1\n",
      " -0.222  -0.233 -> \u001b[31m -0.253\u001b[0m \u001b[31m -0.267\u001b[0m,   1% removed (alpha=   0.0026, nnz=261/263) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[31m -0.252\u001b[0m \u001b[31m -0.267\u001b[0m,   1% removed (alpha=   0.0067, nnz=261/263) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.234\u001b[0m \u001b[32m -0.247\u001b[0m,   2% removed (alpha=   0.0172, nnz=258/263) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.234\u001b[0m \u001b[32m -0.247\u001b[0m,   2% removed (alpha=   0.0444, nnz=259/263) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.234\u001b[0m \u001b[32m -0.247\u001b[0m,   2% removed (alpha=   0.1145, nnz=257/263) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.234\u001b[0m \u001b[32m -0.248\u001b[0m,  17% removed (alpha=   0.2955, nnz=217/263) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 576.656879354123, tolerance: 507.3271805515732\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 830.1717390822157, tolerance: 507.3271805515732\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.222  -0.233 -> \u001b[32m -0.239\u001b[0m \u001b[32m -0.254\u001b[0m,  27% removed (alpha=   0.7627, nnz=193/263) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[31m -0.250\u001b[0m \u001b[31m -0.264\u001b[0m,  31% removed (alpha=   1.9684, nnz=182/263) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.268\u001b[0m \u001b[33m -0.280\u001b[0m,  40% removed (alpha=   5.0802, nnz=157/263) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.331\u001b[0m \u001b[33m -0.338\u001b[0m,  46% removed (alpha=  13.1113, nnz=141/263) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.445\u001b[0m \u001b[33m -0.445\u001b[0m,  59% removed (alpha=  33.8386, nnz=107/263) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.660\u001b[0m \u001b[33m -0.642\u001b[0m,  67% removed (alpha=  87.3326, nnz= 87/263) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -1.159\u001b[0m \u001b[33m -1.084\u001b[0m,  77% removed (alpha= 225.3934, nnz= 60/263) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -1.905\u001b[0m \u001b[33m -1.782\u001b[0m,  87% removed (alpha= 581.7091, nnz= 34/263) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -2.480\u001b[0m \u001b[33m -2.432\u001b[0m,  92% removed (alpha=1501.3107, nnz= 20/263) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -3.392\u001b[0m \u001b[33m -3.374\u001b[0m,  94% removed (alpha=3874.6751, nnz= 17/263) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[31m -0.249\u001b[0m \u001b[31m -0.263\u001b[0m,  12% removed (alpha=   0.3648, nnz=231/263) \u001b[33munder \u001b[0m [0.2955 1.9684]\n",
      " -0.222  -0.233 -> \u001b[31m -0.245\u001b[0m \u001b[31m -0.259\u001b[0m,  13% removed (alpha=   0.4504, nnz=230/263) \u001b[33munder \u001b[0m [0.2955 1.9684]\n",
      " -0.222  -0.233 -> \u001b[32m -0.241\u001b[0m \u001b[32m -0.255\u001b[0m,  16% removed (alpha=   0.5560, nnz=221/263) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.222  -0.233 -> \u001b[32m -0.240\u001b[0m \u001b[32m -0.253\u001b[0m,  19% removed (alpha=   0.6864, nnz=212/263) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.222  -0.233 -> \u001b[32m -0.241\u001b[0m \u001b[32m -0.254\u001b[0m,  19% removed (alpha=   0.8474, nnz=213/263) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.222  -0.233 -> \u001b[32m -0.242\u001b[0m \u001b[32m -0.255\u001b[0m,  22% removed (alpha=   1.0462, nnz=204/263) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.222  -0.233 -> \u001b[32m -0.242\u001b[0m \u001b[32m -0.255\u001b[0m,  26% removed (alpha=   1.2915, nnz=195/263) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.222  -0.233 -> \u001b[32m -0.243\u001b[0m \u001b[31m -0.257\u001b[0m,  32% removed (alpha=   1.5945, nnz=180/263) \u001b[32mfit ok\u001b[0m [0.2955 1.9684]\n",
      " -0.222  -0.233 -> \u001b[32m -0.242\u001b[0m \u001b[32m -0.255\u001b[0m,  27% removed (alpha=   1.1382, nnz=192/263) \u001b[32mfit ok\u001b[0m [1.0462 1.5945]\n",
      " -0.222  -0.233 -> \u001b[32m -0.242\u001b[0m \u001b[32m -0.256\u001b[0m,  27% removed (alpha=   1.2383, nnz=191/263) \u001b[32mfit ok\u001b[0m [1.0462 1.5945]\n",
      " -0.222  -0.233 -> \u001b[32m -0.242\u001b[0m \u001b[32m -0.256\u001b[0m,  28% removed (alpha=   1.3471, nnz=189/263) \u001b[32mfit ok\u001b[0m [1.0462 1.5945]\n",
      " -0.222  -0.233 -> \u001b[32m -0.243\u001b[0m \u001b[31m -0.256\u001b[0m,  29% removed (alpha=   1.4656, nnz=186/263) \u001b[32mfit ok\u001b[0m [1.0462 1.5945]\n",
      "METRICS orig   tr -0.222\u001b[0m, va -0.233\u001b[0m \u001b[1m[te -0.234]\u001b[0m,  ntrees 100,  nnodes 10170,  nleafs  5135,  nnz  5133\n",
      "METRICS prev   \u001b[32mtr -0.242\u001b[0m, \u001b[32mva -0.256\u001b[0m \u001b[1m\u001b[31m[te -0.261]\u001b[0m,  ntrees  18,  nnodes   646,  nleafs   332,  nnz   295\n",
      "METRICS now    \u001b[32mtr -0.242\u001b[0m, \u001b[32mva -0.256\u001b[0m \u001b[1m\u001b[31m[te -0.262]\u001b[0m,  ntrees  18,  nnodes   618,  nleafs   318,  nnz   288\n",
      "\n",
      "Level 5 xxtrain.shape (11478, 318) dense transform time: 0.00s, 1.11s relerr 0.1\n",
      " -0.222  -0.233 -> \u001b[32m -0.239\u001b[0m \u001b[32m -0.254\u001b[0m,   1% removed (alpha=   0.0026, nnz=316/318) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.239\u001b[0m \u001b[32m -0.254\u001b[0m,   1% removed (alpha=   0.0067, nnz=316/318) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.239\u001b[0m \u001b[32m -0.254\u001b[0m,   1% removed (alpha=   0.0172, nnz=316/318) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.239\u001b[0m \u001b[32m -0.253\u001b[0m,   2% removed (alpha=   0.0444, nnz=313/318) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.239\u001b[0m \u001b[32m -0.253\u001b[0m,   2% removed (alpha=   0.1145, nnz=313/318) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.234\u001b[0m \u001b[32m -0.248\u001b[0m,  15% removed (alpha=   0.2955, nnz=271/318) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 626.8581769431254, tolerance: 507.3271805515732\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.222  -0.233 -> \u001b[32m -0.234\u001b[0m \u001b[32m -0.247\u001b[0m,  25% removed (alpha=   0.7627, nnz=238/318) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 938.2171508991819, tolerance: 507.3271805515732\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.222  -0.233 -> \u001b[32m -0.245\u001b[0m \u001b[32m -0.254\u001b[0m,  30% removed (alpha=   1.9684, nnz=222/318) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.274\u001b[0m \u001b[31m -0.277\u001b[0m,  37% removed (alpha=   5.0802, nnz=199/318) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.350\u001b[0m \u001b[33m -0.345\u001b[0m,  43% removed (alpha=  13.1113, nnz=182/318) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.504\u001b[0m \u001b[33m -0.489\u001b[0m,  56% removed (alpha=  33.8386, nnz=139/318) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.750\u001b[0m \u001b[33m -0.746\u001b[0m,  66% removed (alpha=  87.3326, nnz=107/318) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -1.274\u001b[0m \u001b[33m -1.167\u001b[0m,  74% removed (alpha= 225.3934, nnz= 83/318) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -2.067\u001b[0m \u001b[33m -1.871\u001b[0m,  83% removed (alpha= 581.7091, nnz= 55/318) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -3.641\u001b[0m \u001b[33m -3.445\u001b[0m,  89% removed (alpha=1501.3107, nnz= 35/318) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -5.570\u001b[0m \u001b[33m -5.273\u001b[0m,  95% removed (alpha=3874.6751, nnz= 17/318) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.236\u001b[0m \u001b[32m -0.248\u001b[0m,  24% removed (alpha=   0.9416, nnz=241/318) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.237\u001b[0m \u001b[32m -0.248\u001b[0m,  28% removed (alpha=   1.1624, nnz=230/318) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.238\u001b[0m \u001b[32m -0.249\u001b[0m,  27% removed (alpha=   1.4350, nnz=232/318) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.240\u001b[0m \u001b[32m -0.250\u001b[0m,  28% removed (alpha=   1.7716, nnz=229/318) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.243\u001b[0m \u001b[32m -0.253\u001b[0m,  28% removed (alpha=   2.1871, nnz=230/318) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[31m -0.249\u001b[0m \u001b[31m -0.257\u001b[0m,  31% removed (alpha=   2.7001, nnz=219/318) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[31m -0.253\u001b[0m \u001b[31m -0.260\u001b[0m,  32% removed (alpha=   3.3333, nnz=216/318) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[31m -0.261\u001b[0m \u001b[31m -0.266\u001b[0m,  36% removed (alpha=   4.1151, nnz=205/318) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[31m -0.246\u001b[0m \u001b[32m -0.254\u001b[0m,  26% removed (alpha=   1.9274, nnz=234/318) \u001b[32mfit ok\u001b[0m [1.7716 2.7001]\n",
      " -0.222  -0.233 -> \u001b[31m -0.246\u001b[0m \u001b[32m -0.255\u001b[0m,  27% removed (alpha=   2.0969, nnz=232/318) \u001b[32mfit ok\u001b[0m [1.7716 2.7001]\n",
      " -0.222  -0.233 -> \u001b[31m -0.247\u001b[0m \u001b[32m -0.255\u001b[0m,  27% removed (alpha=   2.2812, nnz=231/318) \u001b[32mfit ok\u001b[0m [1.7716 2.7001]\n",
      " -0.222  -0.233 -> \u001b[31m -0.248\u001b[0m \u001b[31m -0.256\u001b[0m,  28% removed (alpha=   2.4818, nnz=228/318) \u001b[33munder \u001b[0m [1.7716 2.7001]\n",
      "METRICS orig   tr -0.222\u001b[0m, va -0.233\u001b[0m \u001b[1m[te -0.234]\u001b[0m,  ntrees 100,  nnodes 10170,  nleafs  5135,  nnz  5133\n",
      "METRICS prev   \u001b[32mtr -0.242\u001b[0m, \u001b[32mva -0.256\u001b[0m \u001b[1m\u001b[31m[te -0.262]\u001b[0m,  ntrees  18,  nnodes   618,  nleafs   318,  nnz   288\n",
      "METRICS now    \u001b[32mtr -0.245\u001b[0m, \u001b[32mva -0.254\u001b[0m \u001b[1m\u001b[31m[te -0.261]\u001b[0m,  ntrees  18,  nnodes   602,  nleafs   310,  nnz   252\n",
      "\n",
      "Level 6 xxtrain.shape (11478, 310) dense transform time: 0.00s, 1.11s relerr 0.1\n",
      " -0.222  -0.233 -> \u001b[32m -0.232\u001b[0m \u001b[32m -0.246\u001b[0m,   1% removed (alpha=   0.0026, nnz=308/310) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.232\u001b[0m \u001b[32m -0.246\u001b[0m,   1% removed (alpha=   0.0067, nnz=308/310) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.232\u001b[0m \u001b[32m -0.246\u001b[0m,   1% removed (alpha=   0.0172, nnz=307/310) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.231\u001b[0m \u001b[32m -0.246\u001b[0m,   2% removed (alpha=   0.0444, nnz=304/310) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.231\u001b[0m \u001b[32m -0.246\u001b[0m,   4% removed (alpha=   0.1145, nnz=298/310) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.231\u001b[0m \u001b[32m -0.245\u001b[0m,  14% removed (alpha=   0.2955, nnz=266/310) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.232\u001b[0m \u001b[32m -0.246\u001b[0m,  18% removed (alpha=   0.7627, nnz=253/310) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.237\u001b[0m \u001b[32m -0.250\u001b[0m,  25% removed (alpha=   1.9684, nnz=231/310) \u001b[32mfit ok\u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[31m -0.261\u001b[0m \u001b[31m -0.275\u001b[0m,  29% removed (alpha=   5.0802, nnz=219/310) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.334\u001b[0m \u001b[33m -0.354\u001b[0m,  39% removed (alpha=  13.1113, nnz=188/310) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.483\u001b[0m \u001b[33m -0.516\u001b[0m,  53% removed (alpha=  33.8386, nnz=146/310) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -0.779\u001b[0m \u001b[33m -0.846\u001b[0m,  63% removed (alpha=  87.3326, nnz=114/310) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -1.253\u001b[0m \u001b[33m -1.299\u001b[0m,  75% removed (alpha= 225.3934, nnz= 79/310) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -2.229\u001b[0m \u001b[33m -2.244\u001b[0m,  82% removed (alpha= 581.7091, nnz= 56/310) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -4.374\u001b[0m \u001b[33m -4.342\u001b[0m,  87% removed (alpha=1501.3107, nnz= 40/310) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[33m -6.546\u001b[0m \u001b[33m -6.524\u001b[0m,  95% removed (alpha=3874.6751, nnz= 17/310) \u001b[33munder \u001b[0m [1.e-03 1.e+04]\n",
      " -0.222  -0.233 -> \u001b[32m -0.233\u001b[0m \u001b[32m -0.247\u001b[0m,  20% removed (alpha=   0.9416, nnz=248/310) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.234\u001b[0m \u001b[32m -0.248\u001b[0m,  22% removed (alpha=   1.1624, nnz=243/310) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.235\u001b[0m \u001b[32m -0.249\u001b[0m,  23% removed (alpha=   1.4350, nnz=239/310) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.237\u001b[0m \u001b[32m -0.251\u001b[0m,  23% removed (alpha=   1.7716, nnz=240/310) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.240\u001b[0m \u001b[32m -0.253\u001b[0m,  25% removed (alpha=   2.1871, nnz=231/310) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurens/venv/veritas/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 585.8732806860298, tolerance: 507.3271805515732\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -0.222  -0.233 -> \u001b[32m -0.242\u001b[0m \u001b[31m -0.256\u001b[0m,  27% removed (alpha=   2.7001, nnz=225/310) \u001b[32mfit ok\u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[31m -0.246\u001b[0m \u001b[31m -0.260\u001b[0m,  28% removed (alpha=   3.3333, nnz=223/310) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[31m -0.250\u001b[0m \u001b[31m -0.265\u001b[0m,  29% removed (alpha=   4.1151, nnz=219/310) \u001b[33munder \u001b[0m [0.7627 5.0802]\n",
      " -0.222  -0.233 -> \u001b[32m -0.239\u001b[0m \u001b[32m -0.253\u001b[0m,  22% removed (alpha=   1.9274, nnz=242/310) \u001b[32mfit ok\u001b[0m [1.7716 2.7001]\n",
      " -0.222  -0.233 -> \u001b[32m -0.240\u001b[0m \u001b[32m -0.253\u001b[0m,  23% removed (alpha=   2.0969, nnz=239/310) \u001b[32mfit ok\u001b[0m [1.7716 2.7001]\n",
      " -0.222  -0.233 -> \u001b[32m -0.241\u001b[0m \u001b[32m -0.254\u001b[0m,  24% removed (alpha=   2.2812, nnz=236/310) \u001b[32mfit ok\u001b[0m [1.7716 2.7001]\n",
      " -0.222  -0.233 -> \u001b[32m -0.242\u001b[0m \u001b[32m -0.255\u001b[0m,  26% removed (alpha=   2.4818, nnz=230/310) \u001b[32mfit ok\u001b[0m [1.7716 2.7001]\n",
      "METRICS orig   tr -0.222\u001b[0m, va -0.233\u001b[0m \u001b[1m[te -0.234]\u001b[0m,  ntrees 100,  nnodes 10170,  nleafs  5135,  nnz  5133\n",
      "METRICS prev   \u001b[32mtr -0.245\u001b[0m, \u001b[32mva -0.254\u001b[0m \u001b[1m\u001b[31m[te -0.261]\u001b[0m,  ntrees  18,  nnodes   602,  nleafs   310,  nnz   252\n",
      "METRICS now    \u001b[32mtr -0.242\u001b[0m, \u001b[32mva -0.255\u001b[0m \u001b[1m\u001b[31m[te -0.258]\u001b[0m,  ntrees  18,  nnodes   602,  nleafs   310,  nnz   230\n",
      "\n",
      "DONE, depth of tree reached 6, 6\n",
      "ChaahatRegTarget4 number of non-zero leaves: 5133 -> 230, test accuracy: -23.4% -> -25.8%\n"
     ]
    }
   ],
   "source": [
    "compressed_models = {}\n",
    "for dname, (d, clf, mtrain, mtest, mvalid, params) in best_models.items():\n",
    "    dtrain, dtest = d.train_and_test_fold(0)\n",
    "    dtrain, dvalid = dtrain.train_and_test_fold(0)\n",
    "    data = tree_compress.Data(\n",
    "        dtrain.X.to_numpy(), dtrain.y.to_numpy(),\n",
    "        dtest.X.to_numpy(), dtest.y.to_numpy(),\n",
    "        dvalid.X.to_numpy(), dvalid.y.to_numpy()\n",
    "    )\n",
    "    \n",
    "    compressed_models[dname] = tree_compress.compress_topdown(\n",
    "        data, clf, relerr=0.10, silent=False, max_rounds=2,\n",
    "        return_compress_object=True\n",
    "    )\n",
    "\n",
    "    before = compressed_models[dname].records[0]\n",
    "    after = compressed_models[dname].records[-1]\n",
    "    print(f\"{dname:10s} number of non-zero leaves: {before.nnz_leafs} -> {after.nnz_leafs},\",\n",
    "          f\"test accuracy: {before.mtest:.3f} -> {after.mtest:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d94fed36-4851-42d2-a321-31d63dec2ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override metrics, because tree_compress used MSE\n",
    "for dname, c, (d, clf, mtrain, mtest, mvalid, params) in zip(compressed_models.keys(),\n",
    "                                                             compressed_models.values(),\n",
    "                                                             best_models.values()):\n",
    "    dtrain, dtest = d.train_and_test_fold(0)\n",
    "    dtrain, dvalid = dtrain.train_and_test_fold(0)\n",
    "    before = c.records[0]\n",
    "    after = c.records[-1]\n",
    "\n",
    "    for rec in [before, after]:\n",
    "        rec.mtrain = dtrain.metric(rec.at)\n",
    "        rec.mtest = dtest.metric(rec.at)\n",
    "        rec.mvalid = dvalid.metric(rec.at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f476a5b7-6a32-4fd7-aeab-b78d5cefa6fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=(10, 10))\n",
    "\n",
    "axs[0].set_prop_cycle(color=['black'])\n",
    "for k in [1, 2]:\n",
    "    axs[k].set_prop_cycle(color=['red', 'green', 'blue'])\n",
    "\n",
    "#for k, (dname, c) in enumerate(compressed_models.items()):\n",
    "for k, dname, c, (d, clf, mtrain, mtest, mvalid, params) in zip(range(len(compressed_models)), compressed_models.keys(), compressed_models.values(), best_models.values()):\n",
    "    before = c.records[0]\n",
    "    after = c.records[-1]\n",
    "\n",
    "    i = k-0.15\n",
    "    j = k+0.15\n",
    "    \n",
    "    lk, = axs[0].plot(k, before.ntrees, \".\")\n",
    "    axs[0].plot(k, after.ntrees, \"v\", color=lk.get_color())\n",
    "    axs[0].plot([k, k], [before.ntrees, after.ntrees],\n",
    "                color=lk.get_color())\n",
    "    \n",
    "    li, = axs[1].plot(i, before.nnodes, \".\")\n",
    "    axs[1].plot(i, after.nnodes, \"v\", color=li.get_color())\n",
    "    axs[1].plot([i, i], [before.nnodes, after.nnodes], \"-\",\n",
    "                color=li.get_color())\n",
    "    \n",
    "    lk, = axs[1].plot(k, before.nleafs, \".\")\n",
    "    axs[1].plot(k, after.nleafs, \"v\", color=lk.get_color())\n",
    "    axs[1].plot([k, k], [before.nleafs, after.nleafs], \"--\",\n",
    "                color=lk.get_color())\n",
    "\n",
    "    lj, = axs[1].plot(j, before.nnz_leafs, \".\")\n",
    "    axs[1].plot(j, after.nnz_leafs, \"v\", color=lj.get_color())\n",
    "    axs[1].plot([j, j], [before.nnz_leafs, after.nnz_leafs], \"-.\",\n",
    "                color=lj.get_color())\n",
    "    \n",
    "    li, = axs[2].plot(i, before.mtrain, \".\")\n",
    "    axs[2].plot(i, after.mtrain, \"^\", color=li.get_color())\n",
    "    axs[2].plot([i, i], [before.mtrain, after.mtrain], \"-\",\n",
    "                color=li.get_color())\n",
    "    \n",
    "    lk, = axs[2].plot(k, before.mvalid, \".\")\n",
    "    axs[2].plot(k, after.mvalid, \"^\", color=lk.get_color())\n",
    "    axs[2].plot([k, k], [before.mvalid, after.mvalid], \"--\",\n",
    "                color=lk.get_color())\n",
    "\n",
    "    lj, = axs[2].plot(j, before.mtest, \".\")\n",
    "    axs[2].plot(j, after.mtest, \"^\", color=lj.get_color())\n",
    "    axs[2].plot([j, j], [before.mtest, after.mtest], \"-.\",\n",
    "                color=lj.get_color())\n",
    "\n",
    "for k in [0, 1, 2]:\n",
    "    axs[k].set_xticks(range(len(compressed_models)))\n",
    "    axs[k].set_xticklabels(list(compressed_models.keys()))\n",
    "    axs[k].grid(visible=True, axis=\"y\", ls=\":\", color=\"gray\", which=\"major\")\n",
    "    axs[k].grid(visible=True, axis=\"y\", ls=\":\", color=\"lightgray\", which=\"minor\")\n",
    "\n",
    "fig.legend([\n",
    "                Line2D([0], [0], c=\"black\", ls=\"\", marker=\".\"),\n",
    "                Line2D([0], [0], c=\"black\", ls=\"\", marker=\"v\")\n",
    "           ], [\n",
    "                \"before (uncompressed)\",\n",
    "                \"after (compressed)\"\n",
    "           ], loc=\"upper center\", ncols=2)\n",
    "\n",
    "axs[1].legend([\n",
    "                Line2D([0], [0], c=\"red\", ls=\"-\"),\n",
    "                Line2D([0], [0], c=\"green\", ls=\"--\"),\n",
    "                Line2D([0], [0], c=\"blue\", ls=\"-.\"),\n",
    "           ], [\n",
    "                \"num. nodes\",\n",
    "                \"num. leaves\",\n",
    "                \"num. non-zero leaves\",\n",
    "           ])\n",
    "\n",
    "axs[2].legend([\n",
    "                Line2D([0], [0], c=\"red\", ls=\"-\"),\n",
    "                Line2D([0], [0], c=\"green\", ls=\"--\"),\n",
    "                Line2D([0], [0], c=\"blue\", ls=\"-.\"),\n",
    "                Line2D([0], [0], c=\"gray\", ls=\"-\"),\n",
    "           ], [\n",
    "                \"train\",\n",
    "                \"validation\",\n",
    "                \"test\",\n",
    "                \"class balance\",\n",
    "           ])\n",
    "\n",
    "axs[0].set_title(\"number of trees in ensemble\")\n",
    "axs[1].set_title(\"number of nodes, leaves, and non-zero leaves\")\n",
    "axs[2].set_title(\"RMSE on training, validation and test sets (higher is worse)\")\n",
    "\n",
    "axs[1].set_yscale(\"log\")\n",
    "#axs[0].yaxis.set_minor_locator(MultipleLocator(5))\n",
    "#axs[2].yaxis.set_minor_locator(MultipleLocator(0.02))\n",
    "\n",
    "fig.subplots_adjust(top=0.94, hspace=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d89beb4-1d23-4789-86ff-5bb803c5d4ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>validation</th>\n",
       "      <th>test</th>\n",
       "      <th>ntrees</th>\n",
       "      <th>nnodes</th>\n",
       "      <th>nleaves</th>\n",
       "      <th>nnz_leaves</th>\n",
       "      <th>nleaf_ratio</th>\n",
       "      <th>nnz_ratio</th>\n",
       "      <th>test_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ChaahatRegTarget0</th>\n",
       "      <th>before</th>\n",
       "      <td>54.4</td>\n",
       "      <td>59.2</td>\n",
       "      <td>58.6</td>\n",
       "      <td>100</td>\n",
       "      <td>8958</td>\n",
       "      <td>4529</td>\n",
       "      <td>4527</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>after</th>\n",
       "      <td>59.5</td>\n",
       "      <td>65.1</td>\n",
       "      <td>64.4</td>\n",
       "      <td>24</td>\n",
       "      <td>752</td>\n",
       "      <td>388</td>\n",
       "      <td>247</td>\n",
       "      <td>11.67</td>\n",
       "      <td>18.33</td>\n",
       "      <td>-0.099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ChaahatRegTarget1</th>\n",
       "      <th>before</th>\n",
       "      <td>17.2</td>\n",
       "      <td>21.2</td>\n",
       "      <td>18.6</td>\n",
       "      <td>100</td>\n",
       "      <td>9586</td>\n",
       "      <td>4843</td>\n",
       "      <td>4837</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>after</th>\n",
       "      <td>17.3</td>\n",
       "      <td>23.0</td>\n",
       "      <td>18.8</td>\n",
       "      <td>28</td>\n",
       "      <td>950</td>\n",
       "      <td>489</td>\n",
       "      <td>304</td>\n",
       "      <td>9.9</td>\n",
       "      <td>15.91</td>\n",
       "      <td>-0.0116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ChaahatRegTarget2</th>\n",
       "      <th>before</th>\n",
       "      <td>13.0</td>\n",
       "      <td>14.9</td>\n",
       "      <td>13.9</td>\n",
       "      <td>100</td>\n",
       "      <td>10400</td>\n",
       "      <td>5250</td>\n",
       "      <td>5242</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>after</th>\n",
       "      <td>13.3</td>\n",
       "      <td>15.7</td>\n",
       "      <td>14.2</td>\n",
       "      <td>24</td>\n",
       "      <td>702</td>\n",
       "      <td>363</td>\n",
       "      <td>286</td>\n",
       "      <td>14.46</td>\n",
       "      <td>18.33</td>\n",
       "      <td>-0.0178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ChaahatRegTarget3</th>\n",
       "      <th>before</th>\n",
       "      <td>14.1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>15.4</td>\n",
       "      <td>100</td>\n",
       "      <td>10462</td>\n",
       "      <td>5281</td>\n",
       "      <td>5278</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>after</th>\n",
       "      <td>15.3</td>\n",
       "      <td>17.5</td>\n",
       "      <td>16.8</td>\n",
       "      <td>21</td>\n",
       "      <td>691</td>\n",
       "      <td>356</td>\n",
       "      <td>265</td>\n",
       "      <td>14.83</td>\n",
       "      <td>19.92</td>\n",
       "      <td>-0.0903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ChaahatRegTarget4</th>\n",
       "      <th>before</th>\n",
       "      <td>22.2</td>\n",
       "      <td>23.3</td>\n",
       "      <td>23.4</td>\n",
       "      <td>100</td>\n",
       "      <td>10170</td>\n",
       "      <td>5135</td>\n",
       "      <td>5133</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>after</th>\n",
       "      <td>24.2</td>\n",
       "      <td>25.5</td>\n",
       "      <td>25.8</td>\n",
       "      <td>18</td>\n",
       "      <td>602</td>\n",
       "      <td>310</td>\n",
       "      <td>230</td>\n",
       "      <td>16.56</td>\n",
       "      <td>22.32</td>\n",
       "      <td>-0.1033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         train validation  test ntrees nnodes nleaves  \\\n",
       "ChaahatRegTarget0 before  54.4       59.2  58.6    100   8958    4529   \n",
       "                  after   59.5       65.1  64.4     24    752     388   \n",
       "ChaahatRegTarget1 before  17.2       21.2  18.6    100   9586    4843   \n",
       "                  after   17.3       23.0  18.8     28    950     489   \n",
       "ChaahatRegTarget2 before  13.0       14.9  13.9    100  10400    5250   \n",
       "                  after   13.3       15.7  14.2     24    702     363   \n",
       "ChaahatRegTarget3 before  14.1       16.0  15.4    100  10462    5281   \n",
       "                  after   15.3       17.5  16.8     21    691     356   \n",
       "ChaahatRegTarget4 before  22.2       23.3  23.4    100  10170    5135   \n",
       "                  after   24.2       25.5  25.8     18    602     310   \n",
       "\n",
       "                         nnz_leaves nleaf_ratio nnz_ratio test_ratio  \n",
       "ChaahatRegTarget0 before       4527           -         -          -  \n",
       "                  after         247       11.67     18.33     -0.099  \n",
       "ChaahatRegTarget1 before       4837           -         -          -  \n",
       "                  after         304         9.9     15.91    -0.0116  \n",
       "ChaahatRegTarget2 before       5242           -         -          -  \n",
       "                  after         286       14.46     18.33    -0.0178  \n",
       "ChaahatRegTarget3 before       5278           -         -          -  \n",
       "                  after         265       14.83     19.92    -0.0903  \n",
       "ChaahatRegTarget4 before       5133           -         -          -  \n",
       "                  after         230       16.56     22.32    -0.1033  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "nleaf_ratio    13.484\n",
       "nnz_ratio      18.962\n",
       "test_ratio    -0.0644\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index_acc = pd.MultiIndex.from_product((compressed_models.keys(), [\"before\", \"after\"]))\n",
    "df = pd.DataFrame(\"-\", columns=[\"train\", \"validation\", \"test\",\n",
    "                                \"ntrees\", \"nnodes\", \"nleaves\", \"nnz_leaves\",\n",
    "                                \"nleaf_ratio\", \"nnz_ratio\",\n",
    "                                \"test_ratio\"], index=index_acc)\n",
    "\n",
    "for k, (dname, c) in enumerate(compressed_models.items()):\n",
    "    before = c.records[0]\n",
    "    after = c.records[-1]\n",
    "\n",
    "    rbefore = df.loc[(dname, \"before\"), :]\n",
    "    rafter = df.loc[(dname, \"after\"), :]\n",
    "\n",
    "    rbefore[\"train\"] = np.round(before.mtrain*100, 1)\n",
    "    rafter[\"train\"] = np.round(after.mtrain*100, 1)\n",
    "    rbefore[\"validation\"] = np.round(before.mvalid*100, 1)\n",
    "    rafter[\"validation\"] = np.round(after.mvalid*100, 1)\n",
    "    rbefore[\"test\"] = np.round(before.mtest*100, 1)\n",
    "    rafter[\"test\"] = np.round(after.mtest*100, 1)\n",
    "\n",
    "    rbefore[\"ntrees\"] = before.ntrees\n",
    "    rafter[ \"ntrees\"] = after.ntrees\n",
    "    rbefore[\"nnodes\"] = before.nnodes\n",
    "    rafter[ \"nnodes\"] = after.nnodes\n",
    "    rbefore[\"nleaves\"] = before.nleafs\n",
    "    rafter[ \"nleaves\"] = after.nleafs\n",
    "    rbefore[\"nnz_leaves\"] = before.nnz_leafs\n",
    "    rafter[ \"nnz_leaves\"] = after.nnz_leafs\n",
    "    rafter[ \"nleaf_ratio\"] = np.round(before.nleafs / after.nleafs, 2)\n",
    "    rafter[ \"nnz_ratio\"] = np.round(before.nnz_leafs / after.nnz_leafs, 2)\n",
    "    rafter[ \"test_ratio\"] = 1.0-np.round(after.mtest / before.mtest, 4)\n",
    "\n",
    "display(df)\n",
    "df_after = df.xs(\"after\", level=1)\n",
    "display(df_after[[\"nleaf_ratio\", \"nnz_ratio\", \"test_ratio\"]].mean(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262af8ba-1fac-4af3-9855-a959fc7ae0fe",
   "metadata": {},
   "source": [
    "# Combine the single-target models into a combined multi-class model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e6dcf46-4cc3-41df-afaa-521cc402b40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_models = {}\n",
    "for dname in dnames:\n",
    "    ntargets = sum(1 for k in compressed_models.keys() if k.startswith(dname))\n",
    "    at_orig = veritas.AddTree(ntargets, veritas.AddTreeType.CLF_SOFTMAX)\n",
    "    at_comp = veritas.AddTree(ntargets, veritas.AddTreeType.CLF_SOFTMAX)\n",
    "    for target in range(ntargets):\n",
    "        key = f\"{dname}RegTarget{target}\"\n",
    "        c = compressed_models[key]\n",
    "        \n",
    "        before = c.records[0]\n",
    "        after = c.records[-1]\n",
    "        \n",
    "        at_orig.add_trees(before.at, target)\n",
    "        at_comp.add_trees(after.at, target)\n",
    "        \n",
    "    dmt = dmts[dname]\n",
    "    dtrain, dtest = dmt.train_and_test_fold(0)\n",
    "    dtrain, dvalid = dtrain.train_and_test_fold(0)\n",
    "\n",
    "    before = tree_compress.CompressRecord(0, 0.0, at_orig)\n",
    "    before.mtrain = dtrain.metric(dtrain.y, np.argmax(at_orig.predict(dtrain.X), axis=1))\n",
    "    before.mtest = dtrain.metric(dtest.y, np.argmax(at_orig.predict(dtest.X), axis=1))\n",
    "    before.mvalid = dtrain.metric(dvalid.y, np.argmax(at_orig.predict(dvalid.X), axis=1))\n",
    "    after = tree_compress.CompressRecord(0, 0.0, at_comp)\n",
    "    after.mtrain = dtrain.metric(dtrain.y, np.argmax(at_comp.predict(dtrain.X), axis=1))\n",
    "    after.mtest = dtrain.metric(dtest.y, np.argmax(at_comp.predict(dtest.X), axis=1))\n",
    "    after.mvalid = dtrain.metric(dvalid.y, np.argmax(at_comp.predict(dvalid.X), axis=1))\n",
    "\n",
    "    combined_models[dname] = [before, after]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34c22dbb-d8d6-474c-92a8-44814d9b83f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1UAAAOeCAYAAADr0DqxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdeXhU5d3/8fdMgJAEhpEtBNkVMLEgZBECWqzsKnWtGxXoD3cQUVHUIiBYoS7QWmNbSws+KnWtOyBIBWQzmABSGUIggZCQhGCWIRAmmZn790eezJMhASYdISF8Xtc1V+4558w59+ecmZAvc859LMYYg4iIiIiIiPxXrPXdARERERERkXOZiioREREREZEgqKgSEREREREJgooqERERERGRIDSp7w6InK88Hg8VFRX13Q0REWlEmjZtSkhISH13Q+S8o6JK5CwzxpCXl0dxcXF9d0VERBohu91Ohw4dsFgs9d0VkfOGiiqRs6yqoGrfvj3h4eH6R09ERH4SxhiOHTvGoUOHAIiKiqrnHomcP1RUiZxFHo/HV1C1adOmvrsjIiKNTFhYGACHDh2iffv2OhVQ5CzRQBUiZ1HVNVTh4eH13BMREWmsqv6N0XW7ImePiiqReqBT/kRE5EzRvzEiZ5+KKhERERERkSCoqBKRgFx11VVMnTo16PV8/PHHXHzxxYSEhPwk6/tv/PznP2fp0qX1sm2p3Zo1a7BYLL5RMVesWEG/fv3wer3127Fz2K5duxg4cCDNmzenX79+Z227zzzzDPfee+9Z215jN3v2bL/j9+STT/LQQw/VX4dEpFYqqkTkrLrvvvu45ZZbOHDgAHPnzj3r2//000/Jz8/n9ttvP+vblsCNGjWKpk2b8vbbb9d3V85Zs2bNIiIigrS0NFavXs2SJUuw2+1ndJt5eXn88Y9/5Le//e0Z3c75bNq0abzxxhtkZGTUd1dEpBoVVSJy1pSWlnLo0CFGjhxJx44dadmy5X+1nvLy8v+6D6+88gq/+c1vsFrP319/58rF6xMmTOCVV16p726c1oEDB0hNTa3xyM7Ortd+7d27lyuuuIKuXbv+pKONejyek36DuGjRIgYNGkTXrl1/su2dLcYY3G53fXfjtNq2bcvIkSP585//XN9dEZFqzt+/KkSkztxuN5MnT6ZVq1a0bduWZ555BmOMb77L5WLatGlceOGFREREMGDAANasWQNUnt5VVURdffXVWCwW37wPP/yQSy+9lNDQULp168bLL7/st91u3boxd+5cxo0bh81m851atH79eq688krCwsLo3LkzU6ZM4ejRoyftf0FBAf/+978ZM2aMb9q+ffuwWCxs27bNN624uNivf1Wnpq1evZr4+HjCw8MZNGgQaWlpfuv/7LPPSEhIoHnz5rRt25Ybb7zRN6+oqIhx48ZxwQUXEB4ezujRo0lPT/fNr/oW4fPPP6d3796Eh4dzyy23cOzYMd544w26devGBRdcwJQpU/B4PDX2zR133EFERAQXXnghSUlJfv2yWCz8+c9/5pe//CURERH87ne/A+CTTz4hNjaW5s2b06NHD5599lnfH5XGGGbPnk2XLl0IDQ2lY8eOTJkyxbfO1157jZ49e9K8eXMiIyO55ZZbfPO8Xi/z5s2je/fuhIWFcdlll/HBBx/49WnZsmX06tWLsLAwfvGLX7Bv374ax2vMmDF899137N27t9bj2RC4XC4SEhKIi4ur8UhISMDlcp2R7a5YsYIrrrgCu91OmzZtuO666/z2k8ViISUlhTlz5mCxWLjqqqv4zW9+Q0lJCRaLBYvFwuzZs30ZTva5hf97b3766afExMQQGhpKVlZWrf165513/D5fUPl+eOGFF7j44osJDQ2lS5cuvvcgwI4dO7j66qsJCwujTZs23HvvvZSWlvrmT5gwgRtuuIHnn3+eyMhI7HY7c+bMwe128/jjj9O6dWs6derE4sWLfa+p+ly/8847DBo0iObNm/Ozn/2MtWvX+pap+lwvX76cuLg4QkNDWb9+/Wnfv0VFRYwdO5Z27doRFhZGz549fdsuLy9n8uTJREVF0bx5c7p27cq8efN8ry0uLubuu++mXbt22Gw2rr76arZv3+63v+bPn09kZCQtW7Zk4sSJHD9+vMZ+HjNmDO+8806tx0BE6okRkbOmrKzM7Ny505SVlQW9rry8PDNq1CjTunVrM2rUKJOXl/cT9PDkhgwZYlq0aGEefvhhs2vXLvPWW2+Z8PBw8/rrr/uWufvuu82gQYPMunXrzJ49e8yLL75oQkNDze7du43L5TJpaWkGMB9++KHJzc01LpfLfPfdd8ZqtZo5c+aYtLQ0s3jxYhMWFmYWL17sW2/Xrl2NzWYzL730ktmzZ4/vERERYRYuXGh2795tNmzYYPr3728mTJhw0gz/+te/TEREhPF4PL5pmZmZBjBbt271TSsqKjKA+frrr40xxnz99dcGMAMGDDBr1qwxP/zwg7nyyivNoEGDfK/5/PPPTUhIiJk5c6bZuXOn2bZtm3n++ed983/5y1+a6Ohos27dOrNt2zYzcuRIc/HFF5vy8nJjjDGLFy82TZs2NcOHDzepqalm7dq1pk2bNmbEiBHm1ltvNT/88IP57LPPTLNmzcw777zjt29atmxp5s2bZ9LS0swrr7xiQkJCzMqVK33LAKZ9+/bmH//4h9m7d6/Zv3+/WbdunbHZbGbJkiVm7969ZuXKlaZbt25m9uzZxhhj3n//fWOz2cyyZcvM/v37zbfffus71lu2bDEhISFm6dKlZt++fSY1NdX88Y9/9G3vueeeM5dccolZsWKF2bt3r1m8eLEJDQ01a9asMcYYk5WVZUJDQ82jjz7qey9FRkYawBQVFfkds8jISL/3QkPj9XpNQkKCsVqtBvA9rFarSUhIMF6v94xs94MPPjAffvihSU9PN1u3bjVjxowxffr08b23c3NzzaWXXmoee+wxk5uba0pKSswf/vAHY7PZTG5ursnNzTVHjhwxxpz6c2vM/703Bw0aZDZs2GB27dpljh49WqNPP/74o7FYLGbz5s1+05944glzwQUXmCVLlpg9e/aYb775xvztb38zxhhTWlpqoqKizE033WR27NhhVq9ebbp3727Gjx/ve/348eNNy5YtzaRJk8yuXbvM3//+dwOYkSNHmt/97ndm9+7dZu7cuaZp06bmwIEDxpj/+1x36tTJfPDBB2bnzp3m7rvvNi1btjSHDx82xvzf57pv375m5cqVZs+ePebHH3887ft30qRJpl+/fmbLli0mMzPTrFq1ynz66afGGGNefPFF07lzZ7Nu3Tqzb98+880335ilS5f6sgwbNsyMGTPGbNmyxezevds89thjpk2bNubHH380xhjz7rvvmtDQULNo0SKza9cu89vf/ta0bNnSXHbZZX771OFwGMBkZmbW+v74Kf+tEZHAqKgSOYt+yn/oRo0aZUJCQgxgQkJCzKhRo36CHp7ckCFDTHR0tN8fidOnTzfR0dHGGGP2799vQkJCTE5Ojt/rhg4dap566iljTM1ixRhj7rzzTjN8+HC/1zz++OMmJibG97xr167mhhtu8Ftm4sSJ5t577/Wb9s033xir1XrS/btw4ULTo0cPv2l1Kaq++uor3zJffPGFAXzbSkxMNGPHjq11u7t37zaA2bBhg2/a4cOHTVhYmHnvvfeMMZV/uAJmz549vmXuu+8+Ex4e7vvj1xhjRo4cae677z6/fXPisb/tttvM6NGjfc8BM3XqVL9lhg4d6lf0GWPMm2++aaKioowxxrz88sumV69evqKvug8//NDYbDbjdDprzDt+/LgJDw83Gzdu9Js+ceJEc8cddxhjjHnqqaf8jq8xle+l2oqq/v37+wq9hmrFihV+BVXVY8WKFWetDwUFBQYwO3bs8E277LLLzKxZs3zPFy9ebFq1auX3ukA+t1XvzW3btp2yD1u3bjWAycrK8k1zOp0mNDTUV0Sd6PXXXzcXXHCBKS0t9U374osvjNVq9f1H0fjx403Xrl39/jOkd+/e5sorr/Q9d7vdJiIiwvzzn/80xvzf53r+/Pm+ZSoqKkynTp3M73//e2PM/32uP/74Y98ygbx/x4wZY37zm9/Umuehhx4yV199da3F9DfffGNsNps5fvy43/SLLrrI/PWvfzXGVP4eefDBB/3mDxgwoEZRVVJSYgBfoXciFVUiZ59O/xM5RyUnJ/tOA/N4PGzZsuWMb3PgwIF+9z9JTEwkPT0dj8fDjh078Hg89OrVixYtWvgea9euPeXpWw6Hg8GDB/tNGzx4sG+9VeLj4/2W2b59O0uWLPHb1siRI/F6vWRmZta6rbKyMpo3b/7fRAegb9++vnZUVBQAhw4dAmDbtm0MHTq01tc5HA6aNGnCgAEDfNPatGlD7969cTgcvmnh4eFcdNFFvueRkZF069aNFi1a+E2r2maVxMTEGs+rrxdq339z5szx23/33HMPubm5HDt2jF/96leUlZXRo0cP7rnnHj766CPfqYHDhw+na9eu9OjRg7vuuou3336bY8eOAbBnzx6OHTvG8OHD/db9P//zP773gcPh8NsXtWWoEhYW5lt3QzVixAgSEhIICQkBICQkhISEBEaMGHHGtpmens4dd9xBjx49sNlsdOvWDeCkp+WdTKCf22bNmvm9/2tTVlYG4PcZczgcuFyuU342LrvsMiIiInzTBg8ejNfr9Tu99tJLL/W7DjIyMpI+ffr4noeEhNCmTZtTfjaaNGlCfHz8KT8bgbx/H3jgAd555x369evHE088wcaNG32vnzBhAtu2baN3795MmTKFlStX+uZt376d0tJS2rRp47fuzMzMOn82wsLCABr8Z0PkfNKkvjsgIv+dyy+/nFWrVuHxeHx/xNWn0tJSQkJCSElJ8f1xWaV6UfDfqv5HV9X27rvvPr/rfKp06dKl1nW0bduWoqIiv2lVf6iZateGnWwgh6ZNm/raVcVl1QX7VX/kBKP6+qu2Udu0/2aY8dr237PPPstNN91UY9nmzZvTuXNn0tLS+Oqrr1i1ahUPPvggL774ImvXrqVly5akpqayZs0aVq5cycyZM5k9ezZbtmzxXQvzxRdfcOGFF/qtNzQ0tM79LiwspF27dnV+3dlksViYO3cuo0aNAir/k2Pu3Lln9AasY8aMoWvXrvztb3+jY8eOeL1efvazn9V5EJdAP7dhYWGnzdO2bVug8pqjqmP2U3wu4Ox9NgJ5/44ePZr9+/ezbNkyVq1axdChQ5k0aRIvvfQSsbGxZGZmsnz5cr766ituvfVWhg0bxgcffEBpaSlRUVF+16tVqeuojIWFhQAN/rMhcj7RN1Ui56glS5YwfPhw2rRpw/Dhw1myZMkZ3+a3337r93zz5s307NmTkJAQ+vfvj8fj4dChQ1x88cV+jw4dOpx0ndHR0WzYsMFv2oYNG+jVq1eNP/Kqi42NZefOnTW2dfHFF9OsWbNaX9O/f3/y8vL8CquqP0pyc3N906oPWhGovn37snr16lrnRUdH43a7/fbfjz/+SFpaGjExMXXe1ok2b95c43l0dPQpXxMbG0taWlqt+6+q0AwLC2PMmDG88sorrFmzhk2bNrFjxw6g8n/9hw0bxgsvvMD333/Pvn37+Pe//+03kMGJ6+3cubNvfyQnJ58yA8Dx48fZu3cv/fv3/6/3zdlS9W0VcMa/pap678yYMYOhQ4cSHR1d4z8LatOsWTO/b3+B//pzW5uLLroIm83Gzp07fdN69uxJWFjYKT8b27dv9xtgZsOGDVitVnr37l2n7dem+vvK7XaTkpJyys9GIO9fqPy9MX78eN566y3+8Ic/8Prrr/vm2Ww2brvtNv72t7/x7rvv8uGHH1JYWEhsbCx5eXk0adKkxrqrCtLo6Ohaf8+e6D//+Q9Nmzbl0ksv/a/3jYj8tPRNlcg5KjIykuXLl5/VbWZlZfHoo49y3333kZqayp/+9CffSH29evVi7NixjBs3jpdffpn+/ftTUFDA6tWr6du3L9dee22t63zsscdISEhg7ty53HbbbWzatIlXX32V11577ZR9mT59OgMHDmTy5MncfffdREREsHPnTlatWsWrr75a62v69+9P27Zt2bBhA9dddx1QWTgMHDiQ+fPn0717dw4dOsSMGTPqvG9mzZrF0KFDueiii7j99ttxu90sW7aM6dOn07NnT66//nruuece/vrXv9KyZUuefPJJLrzwQq6//vo6b+tEGzZs4IUXXuCGG25g1apVvP/++3zxxRenfM3MmTO57rrr6NKlC7fccgtWq5Xt27fzn//8h+eee44lS5bg8XgYMGAA4eHhvPXWW4SFhdG1a1c+//xzMjIy+PnPf84FF1zAsmXL8Hq99O7dm5YtWzJt2jQeeeQRvF4vV1xxBSUlJWzYsAGbzcb48eO5//77efnll3n88ce5++67SUlJqfU/BTZv3kxoaOhJTw1sSCwWC88//zxTpkzh+eefP6PfUl1wwQW0adOG119/naioKLKysnjyySdP+7pu3bpRWlrK6tWrueyyywgPD/+vP7e1sVqtDBs2jPXr13PDDTcAld96Tp8+nSeeeIJmzZoxePBgCgoK+OGHH5g4cSJjx45l1qxZjB8/ntmzZ1NQUMBDDz3EXXfdRWRk5H+7i3ySkpLo2bMn0dHRLFy4kKKiIv7f//t/J10+kPfvzJkziYuL49JLL8XlcvH555/7CrUFCxYQFRVF//79sVqtvP/++3To0AG73c6wYcNITEzkhhtu4IUXXqBXr14cPHiQL774ghtvvJH4+HgefvhhJkyYQHx8PIMHD+btt9/mhx9+oEePHn79/Oabb3wjn4pIA1HfF3WJnE/O5YuHhwwZYh588EFz//33G5vNZi644ALz9NNP+12QXV5ebmbOnGm6detmmjZtaqKiosyNN95ovv/+e2NM7QNVGFM5kllMTIxp2rSp6dKli3nxxRf95nft2tUsXLiwRp+Sk5PN8OHDTYsWLUxERITp27ev+d3vfnfKHE888YS5/fbb/abt3LnTJCYmmrCwMNOvXz+zcuXKWgeqqD6IQtVF+dVH3/rwww9Nv379TLNmzUzbtm3NTTfd5JtXWFho7rrrLtOqVSsTFhZmRo4c6RtdzZjaBxGYNWtWjQvUx48fb66//nq/ffPss8+aX/3qVyY8PNx06NDBbyQ+YyoHqvjoo49q7IsVK1aYQYMGmbCwMGOz2czll1/uG+Hvo48+MgMGDDA2m81ERESYgQMH+gbq+Oabb8yQIUPMBRdcYMLCwkzfvn3Nu+++61uv1+s1f/jDH0zv3r1N06ZNTbt27czIkSPN2rVrfct89tln5uKLLzahoaHmyiuvNP/4xz9q7ON7773Xb1AO+T+rVq0y0dHRJjQ01PTt29esWbOmxnE+caAKY4y5//77TZs2bQzgm3e6z21t782TWbZsmbnwwgv9BpXweDzmueeeM127dvV9xqsPkvL999+bX/ziF6Z58+amdevW5p577vEbnOXE97wxlb+PHn74Yb9p1X9PVA1UsXTpUnP55ZebZs2amZiYGPPvf//bt3xtn2tjTv/+nTt3romOjjZhYWGmdevW5vrrrzcZGRnGmMqBN/r162ciIiKMzWYzQ4cONampqb51O51O89BDD5mOHTuapk2bms6dO5uxY8f6De7xu9/9zrRt29a0aNHCjB8/3jzxxBM1fg/07t3bNyhHbc7lf2tEzlUWY6pdSCAiZ9Tx48fJzMyke/fuQQ2YIP+9vLw8Lr30UlJTU8/JG5SeqFu3bkydOpWpU6fWd1d+UocPH6Z379589913dO/evb67IwEyxjBgwAAeeeQR7rjjjnrrx759++jevTtbt26lX79+9daPM2H58uU89thjfP/99zRpUvsJR/q3RuTs0zVVInJe6dChA3//+9/rPEqanF379u3jtddeU0F1jrFYLLz++uu+kSLlp3f06FEWL1580oJKROqHPpEict6put5DGq74+Pgaw8DLuaFfv36N7tuhhuSWW26p7y6ISC1UVImInMP27dtX310QaXC6deuGrm4QkbNJp/+JiIiIiIgEQUWViIiIiIhIEFRUidQDr9db310QEZFGSv/GiJx9uqZK5Cxq1qwZVquVgwcP0q5dO5o1a3ZGbxIqIiLnD2MM5eXlFBQUYLVaadasWX13SeS8oftUiZxl5eXl5ObmcuzYsfruioiINELh4eFERUWpqBI5i1RUidQDYwxutxuPx1PfXRERkUYkJCSEJk2a6CwIkbNMRZWIiIiIiEgQNFCFiIiIiIhIEFRUiYiIiIiIBEFFlYiIiIiISBBUVImIiIiIiARBRZWIiIiIiEgQVFSJiIiIiIgEQUWViIiIiIhIEFRUiYiIiIiIBEFFlYiIiIiISBBUVImIiIiIiARBRZWIiHDVVVcxderU+u6GjzGGe++9l9atW2OxWNi2bVt9d6nOunXrxh/+8If67ka9mD17Nv369TvlMhMmTOCGG244K/0RETnTmtR3B0RERE60YsUKlixZwpo1a+jRowdt27atscySJUuYOnUqxcXFZ7+DAdiyZQsRERH13Q0RETkLVFSJiMgZ4fF4sFgsWK11Pyli7969REVFMWjQoKD7UV5eTrNmzYJeT121a9furG9TRETqh07/ExFpIK666iqmTJnCE088QevWrenQoQOzZ8/2zd+3b1+NU+GKi4uxWCysWbMGgDVr1mCxWPjyyy/p378/YWFhXH311Rw6dIjly5cTHR2NzWbjzjvv5NixY37bd7vdTJ48mVatWtG2bVueeeYZjDG++S6Xi2nTpnHhhRcSERHBgAEDfNuFym+O7HY7n376KTExMYSGhpKVlVVr1rVr13L55ZcTGhpKVFQUTz75JG63G6g8Leyhhx4iKysLi8VCt27darx+zZo1/OY3v6GkpASLxYLFYvHtq27dujF37lzGjRuHzWbj3nvvBWD9+vVceeWVhIWF0blzZ6ZMmcLRo0cDzrd//37GjBnDBRdcQEREBJdeeinLli072eGscfqfxWJh0aJF3HjjjYSHh9OzZ08+/fTTk76+Lvv8yy+/JDo6mhYtWjBq1Chyc3P99tXll19OREQEdrudwYMHs3//ft/8Tz75hNjYWJo3b06PHj149tlnfceiqt9//etfue666wgPDyc6OppNmzaxZ88errrqKiIiIhg0aBB79+6t0f+//vWvdO7cmfDwcG699VZKSkpOmtXr9TJv3jy6d+9OWFgYl112GR988MEp94+ISINhRESkQRgyZIix2Wxm9uzZZvfu3eaNN94wFovFrFy50hhjTGZmpgHM1q1bfa8pKioygPn666+NMcZ8/fXXBjADBw4069evN6mpqebiiy82Q4YMMSNGjDCpqalm3bp1pk2bNmb+/Pl+227RooV5+OGHza5du8xbb71lwsPDzeuvv+5b5u677zaDBg0y69atM3v27DEvvviiCQ0NNbt37zbGGLN48WLTtGlTM2jQILNhwwaza9cuc/To0Ro5s7OzTXh4uHnwwQeNw+EwH330kWnbtq2ZNWuWMcaY4uJiM2fOHNOpUyeTm5trDh06VGMdLpfL/OEPfzA2m83k5uaa3Nxcc+TIEWOMMV27djU2m8289NJLZs+ePb5HRESEWbhwodm9e7fZsGGD6d+/v5kwYULA+a699lozfPhw8/3335u9e/eazz77zKxdu/akx7Nr165m4cKFvueA6dSpk1m6dKlJT083U6ZMMS1atDA//vjjSdcR6D4fNmyY2bJli0lJSTHR0dHmzjvvNMYYU1FRYVq1amWmTZtm9uzZY3bu3GmWLFli9u/fb4wxZt26dcZms5klS5aYvXv3mpUrV5pu3bqZ2bNn+/X7wgsvNO+++65JS0szN9xwg+nWrZu5+uqrzYoVK8zOnTvNwIEDzahRo3yvmTVrlomIiDBXX3212bp1q1m7dq25+OKLff0yxpjx48eb66+/3vf8ueeeM5dccolZsWKF2bt3r1m8eLEJDQ01a9asOen+ERFpKFRUiYg0EEOGDDFXXHGF37SEhAQzffp0Y0zdiqqvvvrKt8y8efMMYPbu3eubdt9995mRI0f6bTs6Otp4vV7ftOnTp5vo6GhjjDH79+83ISEhJicnx69/Q4cONU899ZQxpvIPfMBs27btlDmffvpp07t3b79tJSUlmRYtWhiPx2OMMWbhwoWma9eup1zP4sWLTatWrWpM79q1q7nhhhv8pk2cONHce++9ftO++eYbY7VaTVlZWUD5+vTp41dsnE5tRdWMGTN8z0tLSw1gli9fXuvr67LP9+zZ45uflJRkIiMjjTHG/PjjjwY4aWEydOhQ8/zzz/tNe/PNN01UVNRJ+71p0yYDmL///e++af/85z9N8+bNfc9nzZplQkJCTHZ2tm/a8uXLjdVqNbm5ucYY/6Lq+PHjJjw83GzcuNGvLxMnTjR33HFHrX0XEWlIdE2ViEgD0rdvX7/nUVFRHDp0KKj1REZGEh4eTo8ePfymJScn+71m4MCBWCwW3/PExERefvllPB4PO3bswOPx0KtXL7/XuFwu2rRp43verFmzGhlO5HA4SExM9NvW4MGDKS0tJTs7my5dutQtbC3i4+P9nm/fvp3vv/+et99+2zfNGIPX6yUzM5OMjIzT5psyZQoPPPAAK1euZNiwYdx8882nzXqi6stHRERgs9lOenwD3efh4eFcdNFFvufV3zOtW7dmwoQJjBw5kuHDhzNs2DBuvfVWoqKifPtlw4YN/O53v/O93uPxcPz4cY4dO0Z4eHiNfkdGRgLQp08fv2nHjx/H6XRis9kA6NKlCxdeeKFvmcTERLxeL2lpaXTo0MEv0549ezh27BjDhw/3m15eXk7//v1r3T8iIg2JiioRkQakadOmfs8tFgterxfAN+CDqXadU0VFxWnXY7FYTrneQJSWlhISEkJKSgohISF+81q0aOFrh4WF+RVL9eXEUfdKS0u57777mDJlSo1lu3Tpwvfff3/afHfffTcjR47kiy++YOXKlcybN4+XX36Zhx56KOB+1eU4BLrPa1tn9ffI4sWLmTJlCitWrODdd99lxowZrFq1ioEDB1JaWsqzzz7LTTfdVGP7zZs3r3UbVce3tml1eU9VV1paCsAXX3zhV4gBhIaG/lfrFBE5m1RUiYicI6pGk8vNzfX97/1Pef+mb7/91u/55s2b6dmzJyEhIfTv3x+Px8OhQ4e48sorg9pOdHQ0H374IcYY3x/jGzZsoGXLlnTq1Cng9TRr1gyPxxPQsrGxsezcuZOLL7641vmB5uvcuTP3338/999/P0899RR/+9vf6lRU1cVPuc/79+9P//79eeqpp0hMTGTp0qUMHDiQ2NhY0tLSTrpfgpGVlcXBgwfp2LEjUPl+slqt9O7du8ay1Qc2GTJkyE/eFxGRM01FlYjIOSIsLIyBAwcyf/58unfvzqFDh5gxY8ZPtv6srCweffRR7rvvPlJTU/nTn/7Eyy+/DECvXr0YO3Ys48aN4+WXX6Z///4UFBSwevVq+vbty7XXXhvwdh588EH+8Ic/8NBDDzF58mTS0tKYNWsWjz76aJ2GX+/WrRulpaWsXr2ayy67jPDwcN/paieaPn06AwcOZPLkydx9991ERESwc+dOVq1axauvvhpQvqlTpzJ69Gh69epFUVERX3/9NdHR0QH3t65+in2emZnJ66+/zi9/+Us6duxIWloa6enpjBs3DoCZM2dy3XXX0aVLF2655RasVivbt2/nP//5D88991xQ/W/evDnjx4/npZdewul0MmXKFG699dYap/4BtGzZkmnTpvHII4/g9Xq54oorKCkpYcOGDdhsNsaPHx9UX0REzjQVVSIi55B//OMfTJw4kbi4OHr37s0LL7zAiBEjfpJ1jxs3jrKyMi6//HJCQkJ4+OGHfcORQ+VpZM899xyPPfYYOTk5tG3bloEDB3LdddfVaTsXXnghy5Yt4/HHH+eyyy6jdevWTJw4sc4F4qBBg7j//vu57bbb+PHHH5k1a5bfEPTV9e3bl7Vr1/Lb3/6WK6+8EmMMF110EbfddlvA+TweD5MmTSI7OxubzcaoUaNYuHBhnfpcV8Hu8/DwcHbt2sUbb7zBjz/+SFRUFJMmTeK+++4DYOTIkXz++efMmTOH3//+9zRt2pRLLrmEu+++O+i+X3zxxdx0001cc801FBYWct111/Haa6+ddPm5c+fSrl075s2bR0ZGBna7ndjYWJ5++umg+yIicqZZTPUTr0VERERERKROdPNfERERERGRIKioEhERERERCYKKKhERERERkSCoqBIREREREQmCiioREREREZEgaEj1arxeLwcPHqRly5a+G1KKiIiIiMj5xxjDkSNH6Nix42nvo6iiqpqDBw/SuXPn+u6GiIiIiIg0EAcOHKBTp06nXEZFVTUtW7YEKneczWar596IiIiIiEh9cTqddO7c2VcjnIqKqmqqTvmz2WwqqkREREREJKDLgjRQhYiIiIiISBBUVImIiIiIiARBRZWIiIiIiEgQVFSJiIiIiIgEoUEUVbNnz8Zisfg9LrnkEt/848ePM2nSJNq0aUOLFi24+eabyc/P91tHVlYW1157LeHh4bRv357HH38ct9t9tqOIiIiIiMh5psGM/nfppZfy1Vdf+Z43afJ/XXvkkUf44osveP/992nVqhWTJ0/mpptuYsOGDQB4PB6uvfZaOnTowMaNG8nNzWXcuHE0bdqU559//qxnERERERGR80eD+KYKKouoDh06+B5t27YFoKSkhL///e8sWLCAq6++mri4OBYvXszGjRvZvHkzACtXrmTnzp289dZb9OvXj9GjRzN37lySkpIoLy+vz1giInKOyc/PZ/To0bRp04bRo0fXODNCRETkRA2mqEpPT6djx4706NGDsWPHkpWVBUBKSgoVFRUMGzbMt+wll1xCly5d2LRpEwCbNm2iT58+REZG+pYZOXIkTqeTH3744aTbdLlcOJ1OvwdARUUFAG6323cKYUVFRa3t8vJyPB5PrW2v1+vbzsnaxhiMMTXaAF6v96TtqmLR4/HU2na73X45lEmZlEmZlCmwTOPGjWPlypUUFhaycuVKxo8ff85naozHSZmUSZmU6WxkClSDKKoGDBjAkiVLWLFiBX/+85/JzMzkyiuv5MiRI+Tl5dGsWTPsdrvfayIjI8nLywMgLy/Pr6Cqml8172TmzZtHq1atfI/OnTsDsGrVKgBWr17N6tWrAVi+fDnr168H4OOPP2bLli0AvPfee2zfvh2AN998k127dgGwaNEiMjIyAEhKSiInJweABQsWcPjwYQDmz5/PkSNHKC8vZ/78+ZSXl3PkyBHmz58PwOHDh1mwYAEAOTk5JCUlAZCRkcGiRYsA2LVrF2+++SYA27dv57333gNgy5YtfPzxxwCsX7+e5cuXK5MyKZMyKVMAmTZs2OD7R9br9ZKcnHzOZ2qMx0mZlEmZlOlsZAqYaYCKioqMzWYzixYtMm+//bZp1qxZjWUSEhLME088YYwx5p577jEjRozwm3/06FEDmGXLlp10O8ePHzclJSW+x4EDBwxgDh8+bIwxpqKiwlRUVBhjjCkvL6+17XK5jNvtrrXt8Xh82zlZ2+v1Gq/XW6NtjDEej+ekbZfLZYwxxu1219quqKgw5eXltbaVSZmUSZmU6eSZRowYYaxWqwGM1Wo1I0eOPOczNcbjpEzKpEzKdKYzFRcXG8CUlJSY07EYU4fvtc6ihIQEhg0bxvDhwxk6dChFRUV+31Z17dqVqVOn8sgjjzBz5kw+/fRTtm3b5pufmZlJjx49SE1NpX///gFt0+l00qpVK0pKSrDZbD9xIhERORfk5+dz4403smnTJhITE/noo49qnA0hIiKNX11qgwZx+t+JSktL2bt3L1FRUcTFxdG0aVPf130AaWlpZGVlkZiYCEBiYiI7duzg0KFDvmVWrVqFzWYjJibmrPdfRETOXZGRkbz66qsAvPrqqyqoRETktBrEkOrTpk1jzJgxdO3alYMHDzJr1ixCQkK44447aNWqFRMnTuTRRx+ldevW2Gw2HnroIRITExk4cCAAI0aMICYmhrvuuosXXniBvLw8ZsyYwaRJkwgNDa3ndCIiIiIi0pg1iKIqOzubO+64gx9//JF27dpxxRVXsHnzZtq1awfAwoULsVqt3HzzzbhcLkaOHMlrr73me31ISAiff/45DzzwAImJiURERDB+/HjmzJlTX5FEREREROQ80WCvqaoPuqZKREQAUlNTiYuLIyUlhdjY2PrujoiI1INz/poqERERERGRc4WKKhERERERkSCoqBIREREREQmCiqpaeL1e38+6tqsuUfN4PAG3jTE12kDA7WD6q0zKpEzKpEzKpEzKpEzKpEy1ZwqUiiogKSmJmJgYEhISAMjLywMqbwCZn58PQG5uLgUFBUDlaIWFhYUAZGVlUVxcDFTecNjpdAKQkZFBaWkpAOnp6ZSVlQGV99hyuVwAOBwO3G43Xq8Xh8OB1+vF7XbjcDgAcLlcpKWlAVBWVkZ6ejpQeR+vjIwMoPICuszMTACKi4vJysoCoLCwkOzsbAAKCgrIzc1VJmVSJmVSpgAzVeXIyclpNJka43FSJmVSJmU605kCpdH/qqka4aOoqAi73e6rdK1Wa8Bti8WCxWLB4/FgtVoDakNlNV29HRIS4qvgT9c2xtSpj8qkTMqkTMp06vbWrVuJj48nOTmZ+Pj4RpGpMR4nZVImZVKmM5nJ6XRit9sDGv1PRVU1GlJdRERAQ6qLiIiGVBcRERERETlrVFSJiIiIiIgEQUWViIiIiIhIEFRUiYiIiIiIBEFFlYiIiIiISBBUVImIiIiIiARBRZWIiIiIiEgQVFTVourGYV6vt87tqtt+eTyegNvGmBptIOB2MP1VJmVSJmVSJmVSJmVSJmVSptozBUpFFZCUlERMTAwJCQkA5OXlAZCfn09+fj4Aubm5FBQUAJCdnU1hYSEAWVlZFBcXA5CZmYnT6QQgIyOD0tJSANLT0ykrKwMgLS0Nl8sFgMPhwO124/V6cTgceL1e3G43DocDAJfLRVpaGgBlZWWkp6cDUFpaSkZGBlB5U7LMzEwAiouLycrKAqCwsJDs7GwACgoKyM3NVSZlUiZlUqYAM1XlyMnJaTSZGuNxUiZlUiZlOtOZAmUxdSnBGrmquyYXFRVht9t9la7Vag24bbFYsFgseDwerFZrQG2orKart0NCQnwV/Onaxpg69VGZlEmZlEmZTt3eunUr8fHxJCcnEx8f3ygyNcbjpEzKpEzKdCYzOZ1O7HY7JSUl2Gw2TkVFVTVVRVUgO05ERBqv1NRU4uLiSElJITY2tr67IyIi9aAutYFO/xMREREREQmCiioREREREZEgqKgSEREREREJgooqERERERGRIKioEhERERERCYKKKhERERERkSCoqKpF1Rj3Xq/uRq1MyqRMyqRMyqRMyqRMynS+ZgqUiiogKSmJmJgYEhISAMjLywN0N2plUiZlUqbzNVNVjpycnEaTqTEeJ2VSJmVSpjOdKVC6+W81VTf4Kioqwm63+ypd3Y1amZRJmZTp/Mq0detW4uPjSU5OJj4+vlFkaozHSZmUSZmU6Uxmcjqd2O32gG7+q6KqmrrcNVlERBqv1NRU4uLiSElJITY2tr67IyIi9aAutYFO/xMREREREQmCiioREREREZEgqKgSEREREREJgooqERERERGRIKioEhERERERCYKKKhERERERkSCoqBIREREREQmCiqpaVN04zOv11rldddsvj8cTcNsYU6MNBNwOpr/KpEzKpEzKpEzKpEzKpEzKVHumQKmoApKSkoiJiSEhIQGAvLw8APLz88nPzwcgNzeXgoICALKzsyksLAQgKyuL4uJiADIzM3E6nQBkZGRQWloKQHp6OmVlZQCkpaXhcrkAcDgcuN1uvF4vDocDr9eL2+3G4XAA4HK5SEtLA6CsrIz09HQASktLycjIACpvSpaZmQlAcXExWVlZABQWFpKdnQ1AQUEBubm5yqRMyqRMyhRgpqocOTk5jSZTYzxOyqRMyqRMZzpToCymLiVYI1d11+SioiLsdruv0rVarQG3LRYLFosFj8eD1WoNqA2V1XT1dkhIiK+CP13bGFOnPiqTMimTMinTqdtbt24lPj6e5ORk4uPjG0WmxniclEmZlEmZzmQmp9OJ3W6npKQEm83GqaioqqaqqApkx4mISOOVmppKXFwcKSkpxMbG1nd3RESkHtSlNtDpfyIiIiIiIkFQUSUiIiIiIhIEFVUiIiIiIiJBUFElIiIiIiIShAZZVM2fPx+LxcLUqVN9044fP86kSZNo06YNLVq04Oabb/YNq1glKyuLa6+9lvDwcNq3b8/jjz9ep6EQRURERERE6qrBFVVbtmzhr3/9K3379vWb/sgjj/DZZ5/x/vvvs3btWg4ePMhNN93km+/xeLj22mspLy9n48aNvPHGGyxZsoSZM2ee7QgiIiIiInIeaVBFVWlpKWPHjuVvf/sbF1xwgW96SUkJf//731mwYAFXX301cXFxLF68mI0bN7J582YAVq5cyc6dO3nrrbfo168fo0ePZu7cuSQlJVFeXl5fkUREREREpJFrUEXVpEmTuPbaaxk2bJjf9JSUFCoqKvymX3LJJXTp0oVNmzYBsGnTJvr06UNkZKRvmZEjR+J0Ovnhhx9q3Z7L5cLpdPo9ACoqKgBwu92+0wcrKipqbZeXl+PxeGptV92AzOVynbRddeOzE9tQeeOxk7WrCkWPx1Nr2+12++VQJmVSJmVSprpnqqioaHSZGuNxUiZlUiZlOlOZAtVgiqp33nmH1NRU5s2bV2NeXl4ezZo1w263+02PjIwkLy/Pt0z1gqpqftW82sybN49WrVr5Hp07dwZg1apVAKxevZrVq1cDsHz5ctavXw/Axx9/zJYtWwB477332L59OwBvvvkmu3btAmDRokVkZGQAkJSURE5ODgALFizg8OHDQOW1Y0eOHKG8vJz58+dTXl7OkSNHmD9/PgCHDx9mwYIFAOTk5JCUlARARkYGixYtAmDXrl28+eabAGzfvp333nsPqDyN8uOPPwZg/fr1LF++XJmUSZmUSZkCzLRv3z4APvroo0aTqTEeJ2VSJmVSpjOdKWCmAcjKyjLt27c327dv900bMmSIefjhh40xxrz99tumWbNmNV6XkJBgnnjiCWOMMffcc48ZMWKE3/yjR48awCxbtqzW7R4/ftyUlJT4HgcOHDCAOXz4sDHGmIqKClNRUWGMMaa8vLzWtsvlMm63u9a2x+Pxbedkba/Xa7xeb422McZ4PJ6Ttl0ulzHGGLfbXWu7oqLClJeX19pWJmVSJmVSplNnSk5ONoDZvHlzo8nUGI+TMimTMinTmcxUXFxsAFNSUmJOx2JMHb7XOkM+/vhjbrzxRkJCQnzTPB4PFosFq9XKl19+ybBhwygqKvL7tqpr165MnTqVRx55hJkzZ/Lpp5+ybds23/zMzEx69OhBamoq/fv3P20/nE4nrVq1oqSkBJvN9lNGFBGRc0hqaipxcXGkpKQQGxtb390REZF6UJfaoEGc/jd06FB27NjBtm3bfI/4+HjGjh3razdt2tT3lR9AWloaWVlZJCYmApCYmMiOHTs4dOiQb5lVq1Zhs9mIiYk565lEREREROT80KS+OwDQsmVLfvazn/lNi4iIoE2bNr7pEydO5NFHH6V169bYbDYeeughEhMTGThwIAAjRowgJiaGu+66ixdeeIG8vDxmzJjBpEmTCA0NPeuZRERERETk/NAgiqpALFy4EKvVys0334zL5WLkyJG89tprvvkhISF8/vnnPPDAAyQmJhIREcH48eOZM2dOPfZaREREREQauwZxTVVDoWuqREQEdE2ViIicg9dUiYiIiIiInKtUVImIiIiIiARBRZWIiIiIiEgQVFTVwuv1+n7WtV11iZrH4wm4bYyp0QYCbgfTX2VSJmVSJmVSJmVSJmVSJmWqPVOgVFQBSUlJxMTEkJCQAEBeXh4A+fn55OfnA5Cbm0tBQQEA2dnZFBYWApCVlUVxcTFQebNhp9MJQEZGBqWlpQCkp6dTVlYGVN5fy+VyAeBwOHC73Xi9XhwOB16vF7fbjcPhAMDlcpGWlgZAWVkZ6enpAJSWlpKRkQFUXkCXmZkJQHFxMVlZWQAUFhaSnZ0NQEFBAbm5ucqkTMqkTMoUYKaqHDk5OY0mU2M8TsqkTMqkTGc6U6A0+l81VSN8FBUVYbfbfZWu1WoNuG2xWLBYLHg8HqxWa0BtqKymq7dDQkJ8Ffzp2saYOvVRmZRJmZRJmU7d3rp1K/Hx8SQnJxMfH98oMjXG46RMyqRMynQmMzmdTux2e0Cj/6moqkZDqouICGhIdRER0ZDqIiIiIiIiZ42KKhERERERkSCoqBIREREREQmCiioREREREZEgqKgSEREREREJgooqERERERGRIKioEhERERERCYKKqlpU3TjM6/XWuV112y+PxxNw2xhTow0E3A6mv8qkTMqkTMqkTMqkTMqkTMpUe6ZAqagCkpKSiImJISEhAYC8vDwA8vPzyc/PByA3N5eCggIAsrOzKSwsBCArK4vi4mIAMjMzcTqdAGRkZFBaWgpAeno6ZWVlAKSlpeFyuQBwOBy43W68Xi8OhwOv14vb7cbhcADgcrlIS0sDoKysjPT0dABKS0vJyMgAKm9KlpmZCUBxcTFZWVkAFBYWkp2dDUBBQQG5ubnKpEzKpEzKFGCmqhw5OTmNJlNjPE7KpEzKpExnOlOgLKYuJVgjV3XX5KKiIux2u6/StVqtAbctFgsWiwWPx4PVag2oDZXVdPV2SEiIr4I/XdsYU6c+KpMyKZMyKdOp21u3biU+Pp7k5GTi4+MbRabGeJyUSZmUSZnOZCan04ndbqekpASbzcapqKiqpqqoCmTHiYhI45WamkpcXBwpKSnExsbWd3dERKQe1KU20Ol/IiIiIiIiQVBRJSIiIiIiEgQVVSIiIiIiIkFQUSUiIiIiIhIEFVUiIiIiIiJBUFElIiIiIiISBBVVIiIiIiIiQVBRVYuqG4d5vd46t6tu++XxeAJuG2NqtIGA28H0V5mUSZmUSZmUSZmUSZmUSZlqzxQoFVVAUlISMTExJCQkAJCXlwdAfn4++fn5AOTm5lJQUABAdnY2hYWFAGRlZVFcXAxAZmYmTqcTgIyMDEpLSwFIT0+nrKwMgLS0NFwuFwAOhwO3243X68XhcOD1enG73TgcDgBcLhdpaWkAlJWVkZ6eDkBpaSkZGRlA5U3JMjMzASguLiYrKwuAwsJCsrOzASgoKCA3N1eZlEmZlEmZAsxUlSMnJ6fRZGqMx0mZlEmZlOlMZwqUxdSlBGvkqu6aXFRUhN1u91W6Vqs14LbFYsFiseDxeLBarQG1obKart4OCQnxVfCnaxtj6tRHZVImZVImZTp1e+vWrcTHx5OcnEx8fHyjyNQYj5MyKZMyKdOZzOR0OrHb7ZSUlGCz2TgVFVXVVBVVgew4ERFpvFJTU4mLiyMlJYXY2Nj67o6IiNSDutQGOv1PREREREQkCCqqREREREREgqCiSkREREREJAgqqkRERERERIKgokpERERERCQIKqpERERERESCoKJKREREREQkCCqqalF14zCv11vndtVtvzweT8BtY0yNNhBwO5j+KpMyKZMyKZMyKZMyKZMyKVPtmQKlogpISkoiJiaGhIQEAPLy8gDIz88nPz8fgNzcXAoKCgDIzs6msLAQgKysLIqLiwHIzMzE6XQCkJGRQWlpKQDp6emUlZUBkJaWhsvlAsDhcOB2u/F6vTgcDrxeL263G4fDAYDL5SItLQ2AsrIy0tPTASgtLSUjIwOovClZZmYmAMXFxWRlZQFQWFhIdnY2AAUFBeTm5iqTMimTMilTgJmqcuTk5DSaTI3xOCmTMimTMp3pTIGymLqUYI1c1V2Ti4qKsNvtvkrXarUG3LZYLFgsFjweD1arNaA2VFbT1dshISG+Cv50bWNMnfqoTMqkTMqkTKdub926lfj4eJKTk4mPj28UmRrjcVImZVImZTqTmZxOJ3a7nZKSEmw2G6eioqqaqqIqkB0nIiKNV2pqKnFxcaSkpBAbG1vf3RERkXpQl9pAp/+JiIiIiIgEQUWViIiIiIhIEFRUiYiIiIiIBEFFlYiIiIiISBAaRFH15z//mb59+2Kz2bDZbCQmJrJ8+XLf/OPHjzNp0iTatGlDixYtuPnmm31DKlbJysri2muvJTw8nPbt2/P444/XaRhEERERERGR/0aDKKo6derE/PnzSUlJ4bvvvuPqq6/m+uuv54cffgDgkUce4bPPPuP9999n7dq1HDx4kJtuusn3eo/Hw7XXXkt5eTkbN27kjTfeYMmSJcycObO+IomIiIiIyHmiwQ6p3rp1a1588UVuueUW2rVrx9KlS7nlllsA2LVrF9HR0WzatImBAweyfPlyrrvuOg4ePEhkZCQAf/nLX5g+fToFBQU0a9YsoG1qSHUREQENqS4iIuf4kOoej4d33nmHo0ePkpiYSEpKChUVFQwbNsy3zCWXXEKXLl3YtGkTAJs2baJPnz6+ggpg5MiROJ1O37ddtXG5XDidTr8HQEVFBQBut9t3CmFFRUWt7fLycjweT63tqhuQuVyuk7arbnx2Yhsqbzx2snZ5eblvf9XWdrvdfjmUSZmUSZmUqe6ZKioqGl2mxniclEmZlEmZzlSmQDWYomrHjh20aNGC0NBQ7r//fj766CNiYmLIy8ujWbNm2O12v+UjIyPJy8sDIC8vz6+gqppfNe9k5s2bR6tWrXyPzp07A7Bq1SoAVq9ezerVqwFYvnw569evB+Djjz9my5YtALz33nts374dgDfffJNdu3YBsGjRIjIyMgBISkoiJycHgAULFnD48GEA5s+fz5EjRygvL2f+/PmUl5dz5MgR5s+fD8Dhw4dZsGABADk5OSQlJQGQkZHBokWLgMpv7d58800Atm/fznvvvQfAli1b+PjjjwFYv3697xo1ZVImZVImZTp9pn379gHw0UcfNZpMjfE4KZMyKZMynelMATMNhMvlMunp6ea7774zTz75pGnbtq354YcfzNtvv22aNWtWY/mEhATzxBNPGGOMueeee8yIESP85h89etQAZtmyZSfd5vHjx01JSYnvceDAAQOYw4cPG2OMqaioMBUVFcYYY8rLy2ttu1wu43a7a217PB7fdk7W9nq9xuv11mgbY4zH4zlp2+VyGWOMcbvdtbYrKipMeXl5rW1lUiZlUiZlOnWm5ORkA5jNmzc3mkyN8TgpkzIpkzKdyUzFxcUGMCUlJeZ0Guw1VcOGDeOiiy7itttuY+jQoRQVFfl9W9W1a1emTp3KI488wsyZM/n000/Ztm2bb35mZiY9evQgNTWV/v37B7RNXVMlIiKga6pEROQcv6aqStX5l3FxcTRt2tT3dR9AWloaWVlZJCYmApCYmMiOHTs4dOiQb5lVq1Zhs9mIiYk5630XEREREZHzR5P67gDAU089xejRo+nSpQtHjhxh6dKlrFmzhi+//JJWrVoxceJEHn30UVq3bo3NZuOhhx4iMTGRgQMHAjBixAhiYmK46667eOGFF8jLy2PGjBlMmjSJ0NDQek4nIiIiIiKNWYMoqg4dOsS4cePIzc2lVatW9O3bly+//JLhw4cDsHDhQqxWKzfffDMul4uRI0fy2muv+V4fEhLC559/zgMPPEBiYiIRERGMHz+eOXPm1FckERERERE5TzTYa6rqg66pEhER0DVVIiLSSK6pEhEREREROReoqBIREREREQmCiioREREREZEgqKiqhdfr9f2sa7vqEjWPxxNw2xhTow0E3A6mv8qkTMqkTMqkTMqkTMqkTMpUe6ZAqagCkpKSiImJISEhAYC8vDwA8vPzyc/PByA3N5eCggIAsrOzKSwsBCArK4vi4mKg8obDTqcTgIyMDEpLSwFIT0+nrKwMqLzHlsvlAsDhcOB2u/F6vTgcDrxeL263G4fDAYDL5SItLQ2AsrIy0tPTASgtLSUjIwOovIAuMzMTgOLiYrKysgAoLCwkOzsbgIKCAnJzc5VJmZRJmZQpwExVOXJychpNpsZ4nJRJmZRJmc50pkBp9L9qqkb4KCoqwm63+ypdq9UacNtisWCxWPB4PFit1oDaUFlNV2+HhIT4KvjTtY0xdeqjMimTMimTMp26vXXrVuLj40lOTiY+Pr5RZGqMx0mZlEmZlOlMZnI6ndjt9oBG/1NRVY2GVBcREdCQ6iIioiHVRUREREREzhoVVSIiIiIiIkFQUSUiIiIiIhIEFVUiIiIiIiJBUFElIiIiIiISBBVVIiIiIiIiQVBRVYuqMe69Xt2NWpmUSZmUSZmUSZmUSZmU6XzNFCgVVUBSUhIxMTEkJCQAkJeXB+hu1MqkTMqkTOdrpqocOTk5jSZTYzxOyqRMyqRMZzpToHTz32qqbvBVVFSE3W73Vbq6G7UyKZMyKdP5lWnr1q3Ex8eTnJxMfHx8o8jUGI+TMimTMinTmczkdDqx2+0B3fxXRVU1dblrsoiINF6pqanExcWRkpJCbGxsfXdHRETqQV1qA53+JyIiIiIiEgQVVSIiIiIiIkFQUSUiIiIiIhIEFVUiIiIiIiJBUFElIiIiIiISBBVVIiIiIiIiQVBRJSIiIiIiEgQVVbWounGY1+utc7vqtl8ejyfgtjGmRhsIuB1Mf5VJmZRJmZRJmZRJmZRJmZSp9kyBUlEFJCUlERMTQ0JCAgB5eXkA5Ofnk5+fD0Bubi4FBQUAZGdnU1hYCEBWVhbFxcUAZGZm4nQ6AcjIyKC0tBSA9PR0ysrKAEhLS8PlcgHgcDhwu914vV4cDgderxe3243D4QDA5XKRlpYGQFlZGenp6QCUlpaSkZEBVN6ULDMzE4Di4mKysrIAKCwsJDs7G4CCggJyc3OVSZmUSZmUKcBMVTlycnIaTabGeJyUSZmUSZnOdKZAWUxdSrBGruquyUVFRdjtdl+la7VaA25bLBYsFgsejwer1RpQGyqr6ertkJAQXwV/urYxpk59VCZlUiZlUqZTt7du3Up8fDzJycnEx8c3ikyN8TgpkzIpkzKdyUxOpxO73U5JSQk2m41TUVFVTVVRFciOExGRxis1NZW4uDhSUlKIjY2t7+6IiEg9qEttoNP/REREREREgqCiSkREREREJAgqqkRERERERIKgokpERERERCQIKqpERERERESCoKJKREREREQkCCqqREREREREgqCiqhZVNw7zer11blfd9svj8QTcNsbUaAMBt4PprzIpkzIpkzIpkzIpkzIpkzLVnilQKqqApKQkYmJiSEhIACAvLw+A/Px88vPzAcjNzaWgoACA7OxsCgsLAcjKyqK4uBiAzMxMnE4nABkZGZSWlgKQnp5OWVkZAGlpabhcLgAcDgdutxuv14vD4cDr9eJ2u3E4HAC4XC7S0tIAKCsrIz09HYDS0lIyMjKAypuSZWZmAlBcXExWVhYAhYWFZGdnA1BQUEBubq4yKZMyKZMyBZipKkdOTk6jydQYj5MyKZMyKdOZzhQoi6lLCdbIVd01uaioCLvd7qt0rVZrwG2LxYLFYsHj8WC1WgNqQ2U1Xb0dEhLiq+BP1zbG1KmPyqRMyqRMynTq9tatW4mPjyc5OZn4+PhGkakxHidlUiZlUqYzmcnpdGK32ykpKcFms3EqKqqqqSqqAtlxIiLSeKWmphIXF0dKSgqxsbH13R0REakHdakNdPqfiIiIiIhIEFRUiYiIiIiIBEFFlYiIiIiISBBUVImIiIiIiAShQRRV8+bNIyEhgZYtW9K+fXtuuOEG3zCMVY4fP86kSZNo06YNLVq04Oabb/YNq1glKyuLa6+9lvDwcNq3b8/jjz9ep6EQRURERERE6qpBFFVr165l0qRJbN68mVWrVlFRUcGIESM4evSob5lHHnmEzz77jPfff5+1a9dy8OBBbrrpJt98j8fDtddeS3l5ORs3buSNN95gyZIlzJw5sz4iiYiIiIjIeaJBDqleUFBA+/btWbt2LT//+c8pKSmhXbt2LF26lFtuuQWAXbt2ER0dzaZNmxg4cCDLly/nuuuu4+DBg0RGRgLwl7/8henTp1NQUECzZs1Ou10NqS4iIqAh1UVEpBEMqV5SUgJA69atAUhJSaGiooJhw4b5lrnkkkvo0qULmzZtAmDTpk306dPHV1ABjBw5EqfTyQ8//FDrdlwuF06n0+8BUFFRAYDb7fadPlhRUVFru7y8HI/HU2u76gZkLpfrpO2qG5+d2IbKG4+drF1eXg5UfkNXW9vtdvvlUCZlUiZlUqa6Z6qoqGh0mRrjcVImZVImZTpTmQLV4Ioqr9fL1KlTGTx4MD/72c8AyMvLo1mzZtjtdr9lIyMjycvL8y1TvaCqml81rzbz5s2jVatWvkfnzp0BWLVqFQCrV69m9erVACxfvpz169cD8PHHH7NlyxYA3nvvPbZv3w7Am2++ya5duwBYtGgRGRkZACQlJZGTkwPAggULOHz4MADz58/nyJEjlJeXM3/+fMrLyzly5Ajz588H4PDhwyxYsACAnJwckpKSAMjIyGDRokVA5Td2b775JgDbt2/nvffeA2DLli18/PHHAKxfv57ly5crkzIpkzIpU4CZ9u3bB8BHH33UaDI1xuOkTMqkTMp0pjMFzDQw999/v+natas5cOCAb9rbb79tmjVrVmPZhIQE88QTTxhjjLnnnnvMiBEj/OYfPXrUAGbZsmW1buv48eOmpKTE9zhw4IABzOHDh40xxlRUVJiKigpjjDHl5eW1tl0ul3G73bW2PR6Pbzsna3u9XuP1emu0jTHG4/GctO1yuYwxxrjd7lrbFRUVpry8vNa2MimTMimTMp06U3JysgHM5s2bG02mxniclEmZlEmZzmSm4uJiA5iSkhJzOg3qmqrJkyfzySefsG7dOrp37+6b/u9//5uhQ4dSVFTk921V165dmTp1Ko888ggzZ87k008/Zdu2bb75mZmZ9OjRg9TUVPr373/a7euaKhERAV1TJSIi5+A1VcYYJk+ezEcffcS///1vv4IKIC4ujqZNm/q+8gNIS0sjKyuLxMREABITE9mxYweHDh3yLbNq1SpsNhsxMTFnJ4iIiIiIiJx3mtR3BwAmTZrE0qVL+eSTT2jZsqXvGqhWrVoRFhZGq1atmDhxIo8++iitW7fGZrPx0EMPkZiYyMCBAwEYMWIEMTEx3HXXXbzwwgvk5eUxY8YMJk2aRGhoaH3GExERERGRRqxBFFV//vOfAbjqqqv8pi9evJgJEyYAsHDhQqxWKzfffDMul4uRI0fy2muv+ZYNCQnh888/54EHHiAxMZGIiAjGjx/PnDlzzlYMERERERE5DzWoa6rqm66pEhER0DVVIiJyDl5TJSIiIiIicq5SUSUiIiIiIhIEFVUiIiIiIiJBUFFVC6/X6/tZ13bVJWoejyfgtjGmRhsIuB1Mf5VJmZRJmZRJmZRJmZRJmZSp9kyBUlEFJCUlERMTQ0JCAoBvSPf8/Hzy8/MByM3NpaCgAIDs7GwKCwsByMrKori4GKi82bDT6QQgIyOD0tJSANLT0ykrKwMq76/lcrkAcDgcuN1uvF4vDocDr9eL2+3G4XAA4HK5SEtLA6CsrIz09HQASktLycjIACovoMvMzASguLiYrKwsAAoLC8nOzgagoKCA3NxcZVImZVImZQowU1WOnJycRpOpMR4nZVImZVKmM50pUBr9r5qqET6Kioqw2+2+StdqtQbctlgsWCwWPB4PVqs1oDZUVtPV2yEhIb4K/nRtY0yd+qhMyqRMyqRMp25v3bqV+Ph4kpOTiY+PbxSZGuNxUiZlUiZlOpOZnE4ndrs9oNH/VFRVoyHVRUQENKS6iIhoSHUREREREZGzRkWViIiIiIhIEFRUiYiIiIiIBEFFlYiIiIiISBBUVImIiIiIiARBRZWIiIiIiEgQVFSJiIiIiIgEQUVVLapuHOb1euvcrrrtl8fjCbhtjKnRBgJuB9NfZVImZVImZVImZVImZVImZao9U6BUVAFJSUnExMSQkJAAQF5eHgD5+fnk5+cDkJubS0FBAQDZ2dkUFhYCkJWVRXFxMQCZmZk4nU4AMjIyKC0tBSA9PZ2ysjIA0tLScLlcADgcDtxuN16vF4fDgdfrxe1243A4AHC5XKSlpQFQVlZGeno6AKWlpWRkZACVNyXLzMwEoLi4mKysLAAKCwvJzs4GoKCggNzcXGVSJmVSJmUKMFNVjpycnEaTqTEeJ2VSJmVSpjOdKVAWU5cSrJGrumtyUVERdrvdV+lardaA2xaLBYvFgsfjwWq1BtSGymq6ejskJMRXwZ+ubYypUx+VSZmUSZmU6dTtrVu3Eh8fT3JyMvHx8Y0iU2M8TsqkTMqkTGcyk9PpxG63U1JSgs1m41RUVFVTVVQFsuNERKTxSk1NJS4ujpSUFGJjY+u7OyIiUg/qUhvo9D8REREREZEgqKgSEREREREJgooqERERERGRIKioEhERERERCYKKKhERERERkSCoqBIREREREQmCiioREREREZEgqKiqRdWNw7xeb53bVbf98ng8AbeNMTXaQMDtYPqrTMqkTMqkTMqkTMqkTMqkTLVnCpSKKiApKYmYmBgSEhIAyMvLAyA/P5/8/HwAcnNzKSgoACA7O5vCwkIAsrKyKC4uBiAzMxOn0wlARkYGpaWlAKSnp1NWVgZAWloaLpcLAIfDgdvtxuv14nA48Hq9uN1uHA4HAC6Xi7S0NADKyspIT08HoLS0lIyMDKDypmSZmZkAFBcXk5WVBUBhYSHZ2dkAFBQUkJubq0zKpEzKpEwBZqrKkZOT02gyNcbjpEzKpEzKdKYzBcpi6lKCNXJVd00uKirCbrf7Kl2r1Rpw22KxYLFY8Hg8WK3WgNpQWU1Xb4eEhPgq+NO1jTF16qMyKZMyKZMynbq9detW4uPjSU5OJj4+vlFkaozHSZmUSZmU6Uxmcjqd2O12SkpKsNlsnIqKqmqqiqpAdpyIiDReqampxMXFkZKSQmxsbH13R0RE6kFdagOd/iciIiIiIhIEFVUiIiIiIiJBUFElIiIiIiISBBVVIiIiIiIiQVBRJSIiIiIiEgQVVSIiIiIiIkFoUt8dEBERaSgOHDhAQUGB7waWVT/bt29Pp06d6rNrIiLSgOmbqlpU3TjM6/XWuV112y+PxxNw2xhTow0E3A6mv8qkTMqkTMpU2S4rKyMhIYG4uDh+/etfA/DrX/+auLg4EhISOHbs2DmXqTEeJ2VSJmVSprOZKVAqqoCkpCRiYmJISEgAIC8vD4D8/Hzy8/MByM3NpaCgAIDs7GwKCwsByMrKori4GIDMzEycTicAGRkZlJaWApCenk5ZWRkAaWlpuFwuoPJ/QN1uN16vF4fDgdfrxe12+/5n1OVykZaWBkBZWRnp6ekAlJaWkpGRAVTelCwzMxOA4uJisrKyACgsLCQ7OxuAgoICcnNzlUmZlEmZlOkUmY4dO0aXLl2wWCxUZ7Va6dy5M/v37z/nMjXG46RMyqRMynQ2MwXKYupSgjVyVXdNLioqwm63+ypdq9UacNtisWCxWPB4PFit1oDaUFlNV2+HhIT4KvjTtY0xdeqjMimTMimTMtXeXrVqFaNGjeJEK1asYNiwYedkpsZ4nJRJmZRJmc5GJqfTid1up6SkBJvNxqmoqKqmqqgKZMeJiEjjY4xhwIABpKSk+P5RjYuL49tvv63xDZaIiDRudakNdPqfiIjI/7JYLMydO9f3v51er5e5c+eqoBIRkVNSUSUiIlLNiBEjfNfYJiQkMGLEiHrukYiINHQqqkRERKqxWCw8//zzREdH8/zzz+tbKhEROS3dp0pEROQEw4YNY+fOnfXdDREROUc0mG+q1q1bx5gxY+jYsSMWi4WPP/7Yb74xhpkzZxIVFUVYWBjDhg3zDc9YpbCwkLFjx2Kz2bDb7UycONE3fKKIiIiIiMiZ0GCKqqNHj3LZZZeRlJRU6/wXXniBV155hb/85S98++23REREMHLkSI4fP+5bZuzYsfzwww+sWrWKzz//nHXr1nHvvfeerQgiIiIiInIeapBDqlssFj766CNuuOEGoPJbqo4dO/LYY48xbdo0AEpKSoiMjGTJkiXcfvvtOBwOYmJi2LJlC/Hx8UDlfUWuueYasrOz6dix42m3qyHVRUREREQEGuGQ6pmZmeTl5TFs2DDftFatWjFgwAA2bdoEwKZNm7Db7b6CCvDdqPHbb7+tdb0ulwun0+n3AKioqADA7Xb77qRcUVFRa7u8vByPx1Nru2pIXpfLddJ21Y3PTmxD5VC+J2uXl5cD4PF4am273W6/HMqkTMqkTMqkTMqkTMqkTMpUt0yBOieKqry8PAAiIyP9pkdGRvrm5eXl0b59e7/5TZo0oXXr1r5lTjRv3jxatWrle3Tu3BmAVatWAbB69WpWr14NwPLly1m/fj0AH3/8MVu2bAHgvffeY/v27QC8+eab7Nq1C4BFixaRkZEBQFJSEjk5OQAsWLCAw4cPAzB//nyOHDlCeXk58+fPp7y8nCNHjjB//nwADh8+zIIFCwDIycnxnRqZkZHBokWLANi1axdvvvkmANu3b+e9994DYMuWLb7r0tavX8/y5cuVSZmUSZmUSZmUSZmUSZmUqQ6ZAnVOnP63ceNGBg8ezMGDB4mKivItd+utt2KxWHj33Xd5/vnneeONN0hLS/NbV/v27Xn22Wd54IEHamzH5XL5KmeoPKWwS5cuZGZm0rp1a1/V26RJEyoqKrBYLDXa5eXlhISEEBISUqPdpEkTrFYrLpeLpk2b1tpu1qwZUFlBV2+Hhobi9XqpqKiote12u2nWrBkejwePx1Oj7Xa7McbQtGnTGm1lUiZlUiZlUiZlUiZlUiZlOnUmp9NJly5dKC4uplWrVpzKOTGkeocOHQDIz8/3K6ry8/Pp16+fb5lDhw75vc7tdlNYWOh7/YlCQ0MJDQ31Pa86/a979+4/ZfdFREREROQcdeTIkcZRVHXv3p0OHTqwevVqXxHldDr59ttvfd9AJSYmUlxcTEpKCnFxcQD8+9//xuv1MmDAgIC207FjRw4cOEDLli11s0cRkfOY0+mkc+fOHDhwQAMXiYicp4wxHDlyJKAB7xpMUVVaWsqePXt8zzMzM9m2bRutW7emS5cuTJ06leeee46ePXvSvXt3nnnmGTp27Og7RTA6OppRo0Zxzz338Je//IWKigomT57M7bffHtCOALBarXTq1OlMxBMRkXOQzWZTUSUich473TdUVRrMNVVr1qzhF7/4RY3p48ePZ8mSJRhjmDVrFq+//jrFxcVcccUVvPbaa/Tq1cu3bGFhIZMnT+azzz7DarVy880388orr9CiRYuzGUVERM5xusWGiIjURYMpqkRERBoKFVUiIlIX58SQ6iIiImdTaGgos2bN8hvMSERE5GT0TZWIiIiIiEgQ9E2ViIiIiIhIEFRUiYiIiIiIBEFFlYiIiIiISBBUVImISKNhsVj4+OOPG812RETk3KCiSkREzhl5eXk89NBD9OjRg9DQUDp37syYMWNYvXp1fXetziZMmOC7gb2IiJzbmtR3B0RERAKxb98+Bg8ejN1u58UXX6RPnz5UVFTw5ZdfMmnSJHbt2lXfXRQRkfOUvqkSEZFzwoMPPojFYiE5OZmbb76ZXr16cemll/Loo4+yefNm33KHDx/mxhtvJDw8nJ49e/Lpp5/65nk8HiZOnEj37t0JCwujd+/e/PGPf/TbzpYtWxg+fDht27alVatWDBkyhNTU1Br9CWY7s2fP5o033uCTTz7BYrFgsVhYs2bNT7i3RETkbFJRJSIiDV5hYSErVqxg0qRJRERE1Jhvt9t97WeffZZbb72V77//nmuuuYaxY8dSWFgIgNfrpVOnTrz//vvs3LmTmTNn8vTTT/Pee+/5Xn/kyBHGjx/P+vXr2bx5Mz179uSaa67hyJEjftsMZjvTpk3j1ltvZdSoUeTm5pKbm8ugQYN+6t0mIiJniW7+KyIiDV5ycjIDBgzgX//6FzfeeONJl7NYLMyYMYO5c+cCcPToUVq0aMHy5csZNWpUra+ZPHkyeXl5fPDBB7XO93q92O12li5dynXXXfeTbWfChAkUFxdrwAsRkUZA11SJiEiDV5f//+vbt6+vHRERgc1m49ChQ75pSUlJ/OMf/yArK4uysjLKy8vp16+fb35+fj4zZsxgzZo1HDp0CI/Hw7Fjx8jKyvpJtyMiIo2HTv8TEZEGr2fPnlgsloAGo2jatKnfc4vFgtfrBeCdd95h2rRpTJw4kZUrV7Jt2zZ+85vfUF5e7lt+/PjxbNu2jT/+8Y9s3LiRbdu20aZNG79lfortiIhI46FvqkREpMFr3bo1I0eOJCkpiSlTptS4rqq4uNjvuqqT2bBhA4MGDeLBBx/0Tdu7d2+NZV577TWuueYaAA4cOMDhw4fr1N9AttOsWTM8Hk+d1isiIg2TvqkSEZFzQlJSEh6Ph8svv5wPP/yQ9PR0HA4Hr7zyComJiQGto2fPnnz33Xd8+eWX7N69m2eeeYYtW7bUWObNN9/E4XDw7bffMnbsWMLCwurU10C2061bN77//nvS0tI4fPgwFRUVddqGiIg0HCqqRETknNCjRw9SU1P5xS9+wWOPPcbPfvYzhg8fzurVq/nzn/8c0Druu+8+brrpJm677TYGDBjAjz/+6PdtEsDf//53ioqKiI2N5a677mLKlCm0b9++Tn0NZDv33HMPvXv3Jj4+nnbt2rFhw4Y6bUNERBoOjf4nIiIiIiISBH1TJSIiIiIiEgQVVSIiIiIiIkFQUSUiIiIiIhIEFVUiIiIiIiJBUFElIiIiIiISBBVVIiIiIiIiQVBRJSIiIiIiEgQVVSIiIiIiIkFQUSUiIiIiIhIEFVUiIiIiIiJBUFElIiIiIiISBBVVIiIiIiIiQVBRJSIiIiIiEgQVVSIiIiIiIkFQUSUiIiIiIhIEFVUiIiIiIiJBUFElIiIiIiISBBVVIiIiIiIiQVBRJSIiIiIiEgQVVSIiIiIiIkFQUSUiIiIiIhIEFVUiIiIiIiJBUFElIiIiIiISBBVVIiIiIiIiQVBRJSIiIiIiEgQVVSIiIiIiIkFQUSUiIiIiIhIEFVUiIiIiIiJBUFElIlKLq666iqlTp9Z3N3yMMdx77720bt0ai8XCtm3bztq29+3bd9a32a1bN/7whz+cte2d67S/Tm/27Nn069evvrshIo1Uk/rugIiInN6KFStYsmQJa9asoUePHrRt27a+uyQiIiL/S0WViMhZ4vF4sFgsWK11P0lg7969REVFMWjQoDPQM5FzR3l5Oc2aNavvboiI+NHpfyLSYF111VVMmTKFJ554gtatW9OhQwdmz57tm1/baWnFxcVYLBbWrFkDwJo1a7BYLHz55Zf079+fsLAwrr76ag4dOsTy5cuJjo7GZrNx5513cuzYMb/tu91uJk+eTKtWrWjbti3PPPMMxhjffJfLxbRp07jwwguJiIhgwIABvu0CLFmyBLvdzqeffkpMTAyhoaFkZWXVmnXt2rVcfvnlhIaGEhUVxZNPPonb7QZgwoQJPPTQQ2RlZWGxWOjWrVut66ja3pdffkl0dDQtWrRg1KhR5Obm+pbxer3MmTOHTp06ERoaSr9+/VixYoXfepKTk+nfvz/NmzcnPj6erVu31tjWf/7zH0aPHk2LFi2IjIzkrrvu4vDhw775H3zwAX369CEsLIw2bdowbNgwjh49Wmu/A1FcXMzdd99Nu3btsNlsXH311Wzfvt03f+/evVx//fVERkbSokULEhIS+Oqrr3zzn376aQYMGFBjvZdddhlz5szxPV+0aBHR0dE0b96cSy65hNdee803r7y8nMmTJxMVFUXz5s3p2rUr8+bNCziDx+Nh4sSJdO/enbCwMHr37s0f//hHv2UmTJjADTfcwEsvvURUVBRt2rRh0qRJVFRU+JY5dOgQY8aMISwsjO7du/P222+fdtuBrLeoqIhx48ZxwQUXEB4ezujRo0lPT/fND+T9daKqz+iJj6uuusq3zPr167nyyisJCwujc+fOTJkyxe+90q1bN+bOncu4ceOw2Wzce++9AHz44YdceumlhIaG0q1bN15++eXT7ocTnep4A0yfPp1evXoRHh5Ojx49eOaZZ3z7bPfu3VgsFnbt2uX3moULF3LRRRf5np/tz4qI1BMjItJADRkyxNhsNjN79myze/du88YbbxiLxWJWrlxpjDEmMzPTAGbr1q2+1xQVFRnAfP3118YYY77++msDmIEDB5r169eb1NRUc/HFF5shQ4aYESNGmNTUVLNu3TrTpk0bM3/+fL9tt2jRwjz88MNm165d5q233jLh4eHm9ddf9y1z9913m0GDBpl169aZPXv2mBdffNGEhoaa3bt3G2OMWbx4sWnatKkZNGiQ2bBhg9m1a5c5evRojZzZ2dkmPDzcPPjgg8bhcJiPPvrItG3b1syaNcsYY0xxcbGZM2eO6dSpk8nNzTWHDh2qdX9VbW/YsGFmy5YtJiUlxURHR5s777zTt8yCBQuMzWYz//znP82uXbvME088YZo2berr85EjR0y7du3MnXfeaf7zn/+Yzz77zPTo0cNvPxcVFZl27dqZp556yjgcDpOammqGDx9ufvGLXxhjjDl48KBp0qSJWbBggcnMzDTff/+9SUpKMkeOHAn42Hft2tUsXLjQ93zYsGFmzJgxZsuWLWb37t3mscceM23atDE//vijMcaYbdu2mb/85S9mx44dZvfu3WbGjBmmefPmZv/+/cYYY/7zn/8YwOzZs8e3zqpp6enpxhhj3nrrLRMVFWU+/PBDk5GRYT788EPTunVrs2TJEmOMMS+++KLp3LmzWbdundm3b5/55ptvzNKlSwPOVF5ebmbOnGm2bNliMjIyfO+pd99917fM+PHjjc1mM/fff79xOBzms88+q/G+Gz16tLnsssvMpk2bzHfffWcGDRpkwsLC/PbXiQJZ7y9/+UsTHR1t1q1bZ7Zt22ZGjhxpLr74YlNeXm6MCez9dSK3221yc3N9j61bt5o2bdqYZ555xhhjzJ49e0xERIRZuHCh2b17t9mwYYPp37+/mTBhgm8dXbt2NTabzbz00ktmz549Zs+ePea7774zVqvVzJkzx6SlpZnFixebsLAws3jx4pP2ZdasWeayyy7zPT/d8TbGmLlz55oNGzaYzMxM8+mnn5rIyEjz+9//3jc/Pj7ezJgxw287cXFxvmln47MiIg2DiioRabCGDBlirrjiCr9pCQkJZvr06caYuhVVX331lW+ZefPmGcDs3bvXN+2+++4zI0eO9Nt2dHS08Xq9vmnTp0830dHRxhhj9u/fb0JCQkxOTo5f/4YOHWqeeuopY0zlH6GA2bZt2ylzPv3006Z3795+20pKSjItWrQwHo/HGGPMwoULTdeuXU+5nqrtVS8ckpKSTGRkpO95x44dze9+9zu/1yUkJJgHH3zQGGPMX//6V9OmTRtTVlbmm//nP//Zbz/PnTvXjBgxwm8dBw4cMIBJS0szKSkpBjD79u07ZX9PpXpR9c033xibzWaOHz/ut8xFF11k/vrXv550HZdeeqn505/+5Ht+2WWXmTlz5vieP/XUU2bAgAF+6zuxSJo7d65JTEw0xhjz0EMPmauvvtrvOAVr0qRJ5uabb/Y9Hz9+vOnatatxu92+ab/61a/MbbfdZowxJi0tzQAmOTnZN9/hcBjgtEXVqda7e/duA5gNGzb45h8+fNiEhYWZ9957zxgT2PvrVMrKysyAAQPMdddd53tfT5w40dx7771+y33zzTfGarX63oNdu3Y1N9xwg98yd955pxk+fLjftMcff9zExMScdPsnFlWnO961efHFF01cXJzv+cKFC81FF13ke151fBwOh299Z/qzIiINg07/E5EGrW/fvn7Po6KiOHToUFDriYyM9J3OU33aiesdOHAgFovF9zwxMZH09HQ8Hg87duzA4/HQq1cvWrRo4XusXbuWvXv3+l7TrFmzGhlO5HA4SExM9NvW4MGDKS0tJTs7u045w8PD/U49qr6/nE4nBw8eZPDgwX6vGTx4MA6Hw9eXvn370rx5c7/c1W3fvp2vv/7aL/cll1wCVJ6Gd9lllzF06FD69OnDr371K/72t79RVFRUpxwnbq+0tJQ2bdr4bTMzM9O3r0tLS5k2bRrR0dHY7XZatGiBw+HwO91y7NixLF26FKgcTfGf//wnY8eOBeDo0aPs3buXiRMn+m3jueee821jwoQJbNu2jd69ezNlyhRWrlxZ5yxJSUnExcXRrl07WrRoweuvv17jlNBLL72UkJAQ3/Pqx9DhcNCkSRPi4uJ88y+55BLsdvtptx3IequfItmmTRt69+7te2/Aqd9f33zzjd++O/G0xP/3//4fR44cYenSpb7rCrdv386SJUv8Xjdy5Ei8Xi+ZmZm+18bHx/uty+Fw1Po+rvp8nk4gxxvg3XffZfDgwXTo0IEWLVowY8YMv+N1++23s2/fPjZv3gzA22+/TWxsrO/zcLY/KyJSfzRQhYg0aE2bNvV7brFY8Hq9AL4/zEy165yqXyNysvVYLJZTrjcQpaWlhISEkJKS4veHKkCLFi187bCwML9i6UyrLVf1/fNTKC0tZcyYMfz+97+vMS8qKoqQkBBWrVrFxo0bWblyJX/605/47W9/y7fffkv37t3/q+1FRUX5Xa9WpaqYmDZtGqtWreKll17i4osvJiwsjFtuuYXy8nLfsnfccQfTp08nNTWVsrIyDhw4wG233ebbBsDf/va3GtdeVR3f2NhYMjMzWb58OV999RW33norw4YN44MPPggoxzvvvMO0adN4+eWXSUxMpGXLlrz44ot8++23fssF+948mZ9ivad6f8XHx/td3xgZGelrP/fcc3z55ZckJyfTsmVL3/TS0lLuu+8+pkyZUmNbXbp08bUjIiLq1M/TCeR4b9q0ibFjx/Lss88ycuRIWrVqxTvvvON37VaHDh24+uqrWbp0KQMHDmTp0qU88MADfts5m58VEak/KqpE5JzVrl07AHJzc+nfvz/AT3ovpRP/2N28eTM9e/YkJCSE/v374/F4OHToEFdeeWVQ24mOjubDDz/EGOMrwDZs2EDLli3p1KlTUOuuzmaz0bFjRzZs2MCQIUN80zds2MDll1/u68ubb77J8ePHfd9WVf0vfJXY2Fg+/PBDunXrRpMmtf8zYrFYGDx4MIMHD2bmzJl07dqVjz76iEcffbTO/Y6NjSUvL48mTZqcdJCODRs2MGHCBG688Uag8o/Zffv2+S3TqVMnhgwZwttvv01ZWRnDhw+nffv2QGUB0LFjRzIyMnzfXtXGZrNx2223cdttt3HLLbcwatQoCgsLad269WlzbNiwgUGDBvHggw/6plX/ViQQl1xyCW63m5SUFBISEgBIS0ujuLi4Tus5UXR0NG63m2+//dY3wuSPP/5IWloaMTExAa0jLCyMiy++uMb0Dz/8kDlz5rB8+XK/b7mg8tju3Lmz1tedrr8bNmzwm7ZhwwZ69epV4z85ahPI8d64cSNdu3blt7/9rW/a/v37ayw3duxYnnjiCe644w4yMjK4/fbbffPO9mdFROqPTv8TkXNWWFgYAwcOZP78+TgcDtauXcuMGTN+svVnZWXx6KOPkpaWxj//+U/+9Kc/8fDDDwPQq1cvxo4dy7hx4/jXv/5FZmYmycnJzJs3jy+++KJO23nwwQc5cOAADz30ELt27eKTTz5h1qxZPProo//V8Oun8vjjj/P73/+ed999l7S0NJ588km2bdvmy3XnnXdisVi455572LlzJ8uWLeOll17yW8ekSZMoLCzkjjvuYMuWLezdu5cvv/yS3/zmN3g8Hr799luef/55vvvuO7KysvjXv/5FQUEB0dHR/1Wfhw0bRmJiIjfccAMrV65k3759bNy4kd/+9rd89913APTs2ZN//etfbNu2je3bt3PnnXfW+i3M2LFjeeedd3j//fdr/DH97LPPMm/ePF555RV2797Njh07WLx4MQsWLABgwYIF/POf/2TXrl3s3r2b999/nw4dOgR06l1VH7/77ju+/PJLdu/ezTPPPMOWLVvqtC969+7NqFGjuO+++/j2229JSUnh7rvvJiwsrE7rqa1v119/Pffccw/r169n+/bt/PrXv+bCCy/k+uuv/6/X+5///Idx48Yxffp0Lr30UvLy8sjLy6OwsBCoHF1v48aNTJ48mW3btpGens4nn3zC5MmTT7nexx57jNWrVzN37lx2797NG2+8wauvvsq0adMC7tvpjnfPnj3JysrinXfeYe/evbzyyit89NFHNdZz0003ceTIER544AF+8Ytf0LFjR9+8s/1ZEZH6o6JKRM5p//jHP3C73cTFxTF16lSee+65n2zd48aNo6ysjMsvv5xJkybx8MMP+4ZzBli8eDHjxo3jscceo3fv3txwww1s2bLF77SlQFx44YUsW7aM5ORkLrvsMu6//34mTpz4kxaIVaZMmcKjjz7KY489Rp8+fVixYgWffvopPXv2BCpPXfzss8/YsWMH/fv357e//W2NU5eqvu3yeDyMGDGCPn36MHXqVOx2O1arFZvNxrp167jmmmvo1asXM2bM4OWXX2b06NHA/w1zf+I3SSdjsVhYtmwZP//5z/nNb35Dr169uP3229m/f7/vFLMFCxZwwQUXMGjQIMaMGcPIkSOJjY2tsa5bbrmFH3/8kWPHjnHDDTf4zbv77rtZtGgRixcvpk+fPgwZMoQlS5b4TsNq2bIlL7zwAvHx8SQkJLBv3z6WLVvmK3wnTJjgN1T4ie677z5uuukmbrvtNgYMGMCPP/7o961VoBYvXkzHjh0ZMmQIN910E/fee6/vG7dgLF68mLi4OK677joSExMxxrBs2bIap/zVxXfffcexY8d47rnniIqK8j1uuukmoPJax7Vr17J7926uvPJK+vfvz8yZM/0Kk9rExsby3nvv8c477/Czn/2MmTNnMmfOHCZMmBBw3053vH/5y1/yyCOPMHnyZPr168fGjRt55plnaqynZcuWjBkzhu3bt9co1IP9rIjIucNifuqT7UVERE5h8eLFPP/88+zcuTOoP9gbmiFDhvCLX/zC715qIiJyftA1VSIiclYtW7aM559/vlEVVCUlJezdu7fOp36KiEjjoG+qREREREREgqBrqkRERERERIKgokpERERERCQIKqpERERERESCoKJKREREREQkCBr9rxqv18vBgwdp2bIlFoulvrsjIiIiIiL1xBjDkSNH6Nixo++ehCejoqqagwcP0rlz5/ruhoiIiIiINBAHDhygU6dOp1xGRVU1LVu2BCp3nM1mq+feiIiIiIhIfXE6nXTu3NlXI5yKiqpqqk75s9lsKqpERERERCSgy4I0UIWIiIiIiEgQVFSJiIiIiIgEQUWViIiIiIhIEFRUiYiIiIiIBEFFlYiIiIiISBBUVDVE+fkwejS0aVP5Mz+/vnskIiIiIiInoaKqFl6v1/ezrm1jDAAejyfgtjHGr23Gj4eVK6GwELNyJUyY4FsGqNEOpr9nK9PJ+q5MyqRMyqRMyqRMyqRMytRQMwVKRRWQlJRETEwMCQkJAOTl5QGQn59P/v9+S5Sbm0tBQQEA2dnZFBYWApCVlUVxcTEAmZmZOJ1OADIyMigtLQUgPT2dsrIyANLS0nC5XAA4HA7cbjderxeHw4HX68XtduPZvBn+9w1h8XphyxbKyspIT08HoLS0lIyMDKDypmSZmZkAFBcXk5WVBUBhYSHZ2dkAFBQUkJubW6+ZHA4HAC6Xi7S0NABlUiZlUiZlUiZlUiZlUqYGnSlQFlOXEqyRczqdtGrViqKiIux2u6/StVqtAbctFgsWiwWPx4PVag2oDZXVdFWb0aOxrFoFXi/GasUyYgRm2TK8Xi8hISG+yr6qbYypUx/rI9PJ+q5MyqRMyqRMyqRMyqRMytQQMzmdTux2OyUlJdhsNk5FRVU1VUVVIDvujMrPhxtvhE2bIDERPvoIIiPrrz8iIiIiIueZutQGOv2vIYqMhFdfrWy/+qoKKhERERGRBkxFlYiIiIiISBBUVImIiIiIiARBRZWIiIiIiEgQVFSJiIiIiIgEQUWViIiIiIhIEFRUiYiIiIiIBEFFlYiIiIiISBBUVImIiIiIiARBRVUtvF6v72dd28YYADweT8BtY0yNdpWqdvXpJ7aD6e/ZzhRoW5mUSZmUSZmUSZmUSZmUqb4zBUpFFZCUlERMTAwJCQkA5OXlAZCfn09+fj4Aubm5FBQUAJCdnU1hYSEAWVlZFBcXA5CZmYnT6QQgIyOD0tJSANLT0ykrKwMgLS0Nl8sFgMPhwO124/V6cTgceL1e3G43DofD17f9+/cDUFZWRnp6OgClpaVkZGQA4HQ6yczMBKC4uJisrCwACgsLyc7OBqCgoIDc3NwGkcnlcpGWlqZMyqRMyqRMyqRMyqRMytTgMwXKYupSgjVyTqeTVq1aUVRUhN1u91W6Vqs14LbFYsFiseDxeLBarQG1obKart4O2b4d4uLwJCcTkpDgq+ZDQkJqtI0xdepjvWWqpe/KpEzKpEzKpEzKpEzKpEwNMZPT6cRut1NSUoLNZuNUVFRVU1VUBbLjzrjUVIiLg5QUiI2t376IiIiIiJxn6lIb6PQ/ERERERGRIKioEhERERERCYKKKhERERERkSCoqBIREREREQmCiioREREREZEgqKgSEREREREJgooqERERERGRIKioEhERERERCYKKKhERERERkSCoqBIREREREQmCiioREREREZEgqKgSEREREREJgooqERERERGRIKioqoXX6/X9rGvbGAOAx+MJuG2MqdGuUtWuPv3EdjD9PduZAm0rkzIpkzIpkzIpkzIpkzLVd6ZAqagCkpKSiImJISEhAYC8vDwA8vPzyc/PByA3N5eCggIAsrOzKSwsBCArK4vi4mIAMjMzcTqdAGRkZFBaWgpAeno6ZWVlAKSlpeFyuQBwOBy43W68Xi8OhwOv14vb7cbhcPj6tn//fgDKyspIT08HoLS0lIyMDACcTieZmZkAFBcXk5WVBUBhYSHZ2dkAFBQUkJub2yAyuVwu0tLSlEmZlEmZlEmZlEmZlEmZGnymQFlMXUqwRs7pdNKqVSuKioqw2+2+StdqtQbctlgsWCwWPB4PVqs1oDZUVtPV2yHbt0NcHJ7kZEISEnzVfEhISI22MaZOfay3TLX0XZmUSZmUSZmUSZmUSZmUqSFmcjqd2O12SkpKsNlsnIqKqmqqiqpAdtwZl5oKcXGQkgKxsfXbFxERERGR80xdagOd/iciIiIiIhIEFVUiIiIiIiJBUFElIiIiIiISBBVVIiIiIiIiQVBRJSIiIiIiEgQVVSIiItXkl+Yz+u3RtHmhDaPfHk1+aX59d0lE5LyQnw+jR0ObNpU/88+hX78qqkRERKqZ8MkEVu1dRWFZIav2rmLCJxPqu0siIueFCRNg1SooLKz8OWFCffcocCqqREREqknOScZjPAB4jIctOVvquUciIueH5GTwVP76xeOBLefQr18VVSIiItVcfuHlWP/3n0crVhIuTKjnHomInB8uvxys/1udWK2QcA79+lVRJSIiUs2S65cwoNMAAAZ0GsCS65fUb4dERM4TS5bAgMpfvwwYUPn8XKGiSkREpJrIFpG8es2rALx6zatEtois5x6JiJwfIiPh1cpfv7z6auXzc4WKKhERERERkSCoqBIRETnBBc0vYGyfsVzQ/IL67oqIiJwDmtR3B0RERBqa7hd0562b3qrvboiIyDlC31SJiIic4Lj7OHsK93Dcfby+uyIiIucAFVUiIiIn2Fmwk55/6snOgp313RURETkHqKgSEREREZEGoUsX+NvfKn+eS3RNlYiIiIiINAht28Ldd9d3L+pO31SJiIiIiEiDcPgwLFpU+fNcoqKqFl6v1/ezrm1jDAAejyfgtjGmRrtKVbv69BPbwfT3bGcKtK1MyqRMyqRMyqRMyqRM51+mffu83HMPZGU1jEyBUlEFJCUlERMTQ0JCAgB5eXkA5Ofnk5+fD0Bubi4FBQUAZGdnU1hYCEBWVhbFxcUAZGZm4nQ6AcjIyKC0tBSA9PR0ysrKAEhLS8PlcgHgcDhwu914vV4cDgderxe3243D4fD1bf/+/QCUlZWRnp4OQGlpKRkZGQA4nU4yMzMBKC4uJisrC4DCwkKys7MBKCgoIDc3t0FkcrlcpKWlKZMyKZMyNehMF0dcjJllsJfZG02mxniclEmZlKnxZerWrZD9+7OIjW0YmQJlMXUpwRo5p9NJq1atKCoqwm63+ypdq9UacNtisWCxWPB4PFit1oDaUFlNV2+HbN8OcXF4kpMJSUjwVfMhISE12saYOvWx3jLV0ndlUiZlUiZlUiZlUiZlUqaGmMnpdGK32ykpKcFms3EqKqqqqSqqAtlxZ1xqKsTFQUoKxMbWb19ERM4zaYfTmPDJBJZcv4TebXvXd3dERM4baWkwYQIsWQK96/nXb11qA53+JyIicoKjFUfZnL2ZoxVH67srIiLnlaNHYfPmyp/nEhVVIiIiIiIiQVBRJSIiIiIiEgQVVSIiIiIiIkFQUSUiInKCbvZuvHnjm3Szd6vvroiIyDmgSX13QEREpKFpHdaaX/f9dX13Q0REzhH6pkpEROQEBUcLSEpOouBoQX13RUREzgEqqkRERE5wwHmAycsnc8B5oL67IiIi5wCd/iciIiIiIg3Cz34GBw5A+/b13ZO6UVElIiIiIiINQrNm0KlTffei7nT6n4iIiIiINAgZGfCrX1X+PJeoqBIRETlBy2YtGXHRCFo2a1nfXREROa94POB0Vv48l+j0PxERkRP0bNOTL3/9ZX13Q0TkvNOzJ3x5Dv761TdVIiIiJ/B4PThdTjzec+y/SkVEznHn6jdVKqpEREROsD1/O63mt2J7/vb67oqIyHll+3Zo1ary57lEp/+JiIiInCe8Xi/l5eX13Q2RkzIGunat/Hn8+JndVtOmTQkJCflJ1qWiSkREROQ8UF5eTmZmJl6vt767InJSTZrAX/5S+TMz88xvz26306FDBywWS1DrUVElIiIi0sgZY8jNzSUkJITOnTtjteoKEGmYjh2Digro3BnCw8/cdowxHDt2jEOHDgEQFRUV1PpUVImIiIg0cm63m2PHjtGxY0fCz+RfqiJBqhqgIjQUmjc/s9sKCwsD4NChQ7Rv3z6oUwFVVImIiJygT/s+HJp2CHtze313ReQn4fnfv1SbNWtWzz0RaViq/pOhoqJCRZWIiMhPqWlIU9pFtKvvboj85IK9bkSksfmpPhM6oVZEROQEewv38st//pK9hXvruysiInIOUFElIiJyghJXCZ/t/owSV0l9d0VEzkNXXXUVU6dOre9u1AurtfJaqnNtLBWd/iciIiIiIg1CWBj87Gf13Yu6O8dqQBERERERkYZFRZWIiIiINEhXXXUVU6ZM4YknnqB169Z06NCB2bNn++bv27cPi8XCtm3bfNOKi4uxWCysWbMGgDVr1mCxWPjyyy/p378/YWFhXH311Rw6dIjly5cTHR2NzWbjzjvv5NixY3Xqn8ViYdGiRdx4442Eh4fTs2dPPv30U79l1q5dy+WXX05oaChRUVE8+eSTuN1u3/yjR48ybtw4WrRoQVRUFC+//HKN7bhcLqZNm8aFF15IREQEAwYM8OUD2L9/P2PGjOGCCy4gIiKCSy+9lGXLltUpS0Nx7Bikplb+PJfo9D8REZETXNjyQl4e8TIXtrywvrsicuYcOwa7dp397V5ySZ3u6vrGG2/w6KOP8u2337Jp0yYmTJjA4MGDGT58eJ02O3v2bF599VXCw8O59dZbufXWWwkNDWXp0qWUlpZy44038qc//Ynp06fXab3PPvssL7zwAi+++CJ/+tOfGDt2LPv376d169bk5ORwzTXXMGHCBP7nf/6HXbt2cc8999C8eXNfcfj444+zdu1aPvnkE9q3b8/TTz9Namoq/fr1821j8uTJ7Ny5k3feeYeOHTvy0UcfMWrUKHbs2EHPnj2ZNGkS5eXlrFu3joiICHbu3EmLFi3qlKOhaNoUOnas/HlOMeJTUlJiAFNSUlLfXTEmJcUYqPwpIiIiEoSysjKzc+dOU1ZW9n8Tq/7WONuPOvxtM2TIEHPFFVf4TUtISDDTp083xhiTmZlpALN161bf/KKiIgOYr7/+2hhjzNdff20A89VXX/mWmTdvngHM3r17fdPuu+8+M3LkyDrsVWMAM2PGDN/z0tJSA5jly5cbY4x5+umnTe/evY3X6/Utk5SUZFq0aGE8Ho85cuSIadasmXnvvfd883/88UcTFhZmHn74YWOMMfv37zchISEmJyfHb9tDhw41Tz31lDHGmD59+pjZs2fXqe9SqdbPxv+qS22gb6pEREROUFRWxFcZXzGsxzAuCLugvrsjcmZccgmkpNTPduugb9++fs+joqI4dOhQnTdbfT2RkZGEh4fTo0cPv2nJ/5+9O4+Lqmz/B/6ZQZZhF1RERHAXTVEQCe2XLeT2ZGqWfotSetQyt4zUtCxNK31aaJPKNNNQn7Jcn8wtXEolQRA1RURAEWRVdmHW+/fHNCeHRWdEHZbP+/Wa19xz5j7nXNeZAb24z7lPXFy9tuvg4ABnZ2cpvuTkZISEhBjdC2nQoEEoLy9HVlYWioqKoFKpEBwcLL3v5uaG7t27S69Pnz4NrVaLbt26Ge1XqVTC3d0dADBr1iy8/PLL2Lt3L0JDQzF27Ngax62x0GiA0lLA2Rlo0YgqlUYUKhER0b2RUZyBcT+PQ8KLCSyqqOmytwcCAiwdxS1ZVzsPTCaTQafTAQDkf8+7LYSQ3ler1bfcjkwmu+l271R8d0J5eTmsrKyQkJAAKysro/cMp/hNnjwZQ4cOxc6dO7F3714sW7YMH3/8MWbOnHnH4rhXlEogPR3w82tcRRUnqiAiIiKiRql169YAgJycHGnZjZNWWJqfnx9iY2ONir4jR47AyckJ7du3R+fOnWFtbY1jx45J7xcVFeH8+fPS6379+kGr1SI/Px9dunQxerRt21bq5+3tjalTp2LLli147bXXsGrVqnuTJAFgUUVEREREjZRCocD999+P5cuXIzk5GYcOHcLChQvvyLZXrFiBRx99tF7bmDZtGi5fvoyZM2fi3Llz2L59OxYtWoSIiAjI5XI4Ojpi0qRJmDt3Lvbv34+//voL4eHh0ggcAHTr1g1hYWGYMGECtmzZgoyMDMTFxWHZsmXYuXMnAGD27NnYs2cPMjIykJiYiAMHDsDPz69esZN5mmRRdf36dfj4+GDOnDmWDoWIiIiI7qI1a9ZAo9EgMDAQs2fPxrvvvntHtltYWIi0tLR6bcPLywu//vor4uLi4O/vj6lTp2LSpElGhd+HH36I//f//h9GjhyJ0NBQPPDAAwgMDDTaznfffYcJEybgtddeQ/fu3TF69GjEx8ejQ4cOAACtVovp06fDz88Pw4YNQ7du3fDll1/WK3Yyj0zcOB7ZRLz55pu4cOECvL298dFHH5m8XmlpKVxcXFBSUgJnZ+e7GKEJEhOBwED9BaSN4HxnIqKmJLkgGWFbwrDhyQ3wa82/9lLjV1VVhYyMDHTs2BF2dnaWDoeoThUVQHKy/poqB4e7v7+b/WyYUxs0uZGq1NRUnDt3DsOHD7d0KERE1Ej5tfZD4kuJLKiIiMgkDaqo+v333zFy5Ei0a9cOMpkM27Ztq9EnKioKvr6+sLOzQ3BwcI2pL+fMmYNly5bdo4iJiIiIiKi5a1BFVUVFBfz9/REVFVXr+z/++CMiIiKwaNEiJCYmwt/fH0OHDpXuBbB9+3Z069atxjz+dVEqlSgtLTV6AP9MxanRaKDRaKRltbVVKhW0Wm2tbcN0mkqlss62EAJCiBptA5VKBQDQ6XTScp1OJy3XarW1tjUajVEeDSWn6nkwJ+bEnJhTQ8zpePZx2L5ri7jMuCaTU1P8nJiTeTkZrvgQQkjbuJ22YTs3bvNOtw37vLF9J2JnTg0/J+CfK5PuZU4qlarWnydTNaiiavjw4Xj33XcxZsyYWt+PjIzElClT8MILL6Bnz574+uuvYW9vjzVr1gAA/vzzT/zwww/w9fXFnDlzsGrVKixZsqTO/S1btgwuLi7Sw9vbGwCwb98+AEBMTAxiYmIAALt27cLhw4cBANu2bUN8fDwAYNOmTTh58iQAIDo6GufOnQMArF69Gunp6QD0o2vZ2dlSDoWFhQCA5cuXo6ysDCqVCsuXL4dKpUJZWRmWL18uxbh+/XoAQHZ2tlRspqenY/Xq1QCAc+fOITo6GgBw8uRJbNq0CQAQHx8vjfQdPnwYu3btahA5FRYWIjIykjkxJ+bEnBp0ThkZGVBpVdiydUuTyakpfk7MyfSc8vLyUFZWBkBfjBm2UVVVhatXrwLQT/RVVFQEQP+H7uLiYgD6+yQZ/vB84x+hS0tLUV5eDgAoLi5GRUUFAP2U4NevXwcAXL16FVVVVVI8hiIvPz9fKj7z8vKkIjM3N1f6j25ubq70n+rc3FwA+kI1Ly8PgL44NfxhnTk1nZzU6utwcamCtfW9zembb76p9efJVA12ogqZTIatW7di9OjRAPTVo729PX7++WdpGQBMnDgRxcXF2L59u9H6a9euxV9//XXTiSqUSqXRqFBpaSm8vb1RWFgId3d36SC3aNECarUaMpmsRlulUsHKygpWVlY12i1atIBcLodSqYS1tXWtbRsbGym/G9u2Z84AgYFQxcbC5v77odPpoFarYWtrC51OB41GAxsbG2i1Wmi12hptjUYDIQSsra1rtC2W09+x35gHc2JOzIk5NcScTuafxIBvB+DPF/5EUPugJpFTU/ycmJPpOV2/fh0XL15Ep06dYGtrCyEE5HK59Nd4c9rAPze4lclkd6UN6EcmbmzfbrzMiTndrK1UKnHx4kV4eXnBycnJ6OeptLQUrq6uJk1U0WiKqitXrsDLywtHjx5FSEiI1G/evHk4dOiQ0U3TANOKquo4+x8REQFAYk4iAr8JRMKLCQjw5O9gavw4+x81FlotUFkJKBSAldXd39+dmv2vxd0M0pLCw8MtHQIREREREZmhqgo4d+7eTal+pzSaoqpVq1awsrKSzs80yMvLQ9u2bS0UFRERNUV+rfzw18t/oVPLTpYOhYioWVEogF69AFtbS0dingY1UcXN2NjYIDAwULrgE9DP2BETE2N0OuCdYJhlRKfTmd02nE2p1WpNbgsharQNDO0bl1dv1yfee52TqW3mxJyYE3OyZE52LezQq00v2MhtmkxOTfFzYk7m5WRYrzG0a3s0hLiY093PSSYTsLMTkMvvbU51/TyZqkEVVeXl5UhKSkJSUhIAICMjA0lJScjMzAQAREREYNWqVVi3bh2Sk5Px8ssvo6KiAi+88EK99hsVFYWePXsiKCgIAKSZS/Ly8qSRsZycHBQUFAAAsrKycO3aNQBAZmamNOtKRkaGNCtJenq6NCtJamoqKisrAQApKSnS5BjJycnQaDTQ6XRITk6GTqe/GDY5OVmK7dKlSwCAyspKpKamSsfJMCNQaWkpMjIyAOhnPzEcq2vXriErKwsAUFBQgJycnAaRk1KpREpKCnNiTsyJOTXonP7K+guTd0zGH6f/aDI5NcXPiTmZnlNVVZU0cYZO989U61qt1qh94zTxtU0Zr1arjdo3Tg1f2zTxSqXSqG0oIKuqqozahv+8Vm8D+v/41tbW6XRGbUvkJJfLsXXr1iaVk6U/p8pKLdLTdVAq721OaWlptf48mapBTVRx8OBBPPzwwzWWT5w4EWvXrgUArFixAh9++CFyc3PRt29ffP755wgODr4j+zdcjFZUVARXV1fpgMvlcpPbhtlEtFot5HK5SW1A/yW7sW118iQQGAhtXBysgoIghP6vY1ZWVjXaQgizYrRYTrXEzpyYE3NiTg0xpxO5J9B/VX/ETYpDf6/+TSKnpvg5MSfTc6qsrERGRoY0+x+gnxTsn9GBhtWuTUOIq3pbJpNhy5YtGDNmTJPJydKfU0WFQHKyDH5+gL393c+jqqoKFy9eRIcOHWBvbw/gn5+nJjH7nyVw9j8iIgI4+x81PZz97+6oPls11V9FBZCcfO8mqrhTs/81qNP/iIiIiIgMHnroIcyaNQvz5s2Dm5sb2rZti8WLF0vvX7x4ETKZTLp0BNCfmimTyXDw4EEA+jOhZDIZ9uzZg379+kGhUOCRRx5Bfn4+du3aBT8/Pzg7O+PZZ5+Vbhx7uy5fvoxx48bB1dUVbm5uGDVqFC5evCi9Hx8fj8ceewytWrWCi4sLBg8ejMTEROn9Z599FuPHjzfaplqtRqtWrfD9998D0I+iLFu2DB07doRCoYC/vz9+/vlnqX9RURHCwsLQunVrKBQKdO3aFd9991298qJbazSz/xERERHRnZVTloOc8hyjZS3tWqJjy46o0lThbMHZGusYRm9TClNQoa4wes/X1RduCjcUVBTgcullo/ecbJzQ1b2r2TGuW7cOEREROHbsGGJjYxEeHo5BgwbhscceM2s7ixcvxooVK2Bvb49x48Zh3LhxsLW1xcaNG1FeXo4xY8bgiy++wOuvv252jIC++Bk6dChCQkLwxx9/oEWLFnj33XcxbNgwnDp1CjY2NigrK8PEiRPxxRdfQAiBjz/+GCNGjEBqaiqcnJwQFhaGp59+GuXl5XB0dAQA7NmzB9evX8eYMWMAAMuWLcP69evx9ddfo2vXrvj999/x3HPPoXXr1hg8eDDeeustnD17Frt27UKrVq1w4cIF6Zo8untYVBEREVXj4eCB+YPmw8PBw9KhEN1VKxNW4p1D7xgtC+sdhvVPrkdWaRYCvwmssY5YpL9yJHx7OP7M+tPovegx0Xiuz3PYdGYTZuyaYfTekM5DsOe5PWbH2KdPHyxatAgA0LVrV6xYsQIxMTFmF1XvvvsuBg0aBACYNGkSFixYgLS0NHTqpL91wlNPPYUDBw7cdlH1448/QqfTYfXq1ZDJZACA7777Dq6urjh48CCGDBmCRx55xGidb775Bq6urjh06BAef/xxDB06FA4ODti6dSuef/55AMDGjRvxxBNPwMnJCUqlEu+//z5+++03afbrTp064fDhw1i5ciUGDx6MzMxM9OvXD/379wcA+Pr63lY+ZB4WVbUwXDRq0QtM/45Fq9XCCv9M49qULpplTsyJOTGnhppTO6d2WBa6TJpWtynk1BQ/J+ZkXk6Gy+hvvFD/xYAXMbLbSKML+N0UbhBCwMvJCwkvJtS4sN/wM/HdE9+hQl1htG7Hlh0hhMDTPZ9GiHeI0bqO1o7SuuZMgNC7d2+j9Tw9PZGfn18jp+rbvnF59e20adMG9vb26NSpk/S+h4cH4uLiTI7RUDgZXiclJeHChQtwcnIyir+qqgoXLlzAY489hry8PCxcuBCHDh1Cfn4+tFotrl+/Ls32bGVlhaeffhobNmzAc889h4qKCmzfvh3//e9/IYTAhQsXcP369RoFpUqlQr9+/SCEwMsvv4yxY8ciMTERQ4YMwahRozBw4MBGM1EFIAAYH9u7OVGF4fWNtysw/DyZM/UEr6kCp1S/lzk19SlrmRNzYk5NI6fswmwcvHgQp1JONZmcmuLnxJzqP6V6G/s26OXWCwGeAfBv44/73O9Dx5YdodFoINfJEeAZgD6t+6B3q94I8AxA71a9pWmtO7l0Qp/WfRDgGYD73O9Dn9Z94KZwg0qlgpudGwI8A9DLrRf82/gjwDMAHRw7SEWkOVN1W1tbG03VrdPppIchlhun5zb0NSw39JHJZNK03UIIWFtbA/hnqm5DoWrOVN2GbQFASUkJAgICkJSUhD///BOJiYlISkrCqVOn8MwzzwAAnn/+eZw8eRKffPIJ9u/fj6SkJLi7u0vXcul0Ojz99NOIiYlBbm4ufvrpJygUCjz22GNQKpXSd2Lr1q1ISkpCfHw84uLicPbsWfz3v/+FWq3G8OHDkZqaipkzZ+LKlSsIDQ3Fa6+9ZnJOlp5S/cZj25imVIcgSUlJiQAgioqKhBBCaLVaodVqzWrrdDohhBAajcbktk6nq9EWCQlCAEITFyeEEP8sr6VtbowWy+kWeTAn5sScmFNDyel49nGBxRBxl+OaTE5N8XNiTqbndP36dXHmzBlRWVkp9TP0aYhtw2Pw4MFi1qxZRu+PGjVKTJw4Ueh0OlFRUSEAiF9++UXqs2fPHgFAHDhwQOh0OrF//34BQFy7dk3qs2bNGuHi4mK0z0WLFgl/f3+z4gUgtmzZIoQQYuXKlaJly5aipKSkzpwcHR3FunXrpNeZmZkCgIiMjDTq27FjR/HZZ5+J4cOHi6lTp0rLS0tLha2trVi3bp3JMX711VfCycnprn5ONz7qu+3ycp2IjxeivPzefN+uX78uzp49K8rLy2v8PBUXFwsAoqSkRNwKT/+rhWH43PB8O20rK6s72pbJZHW2DcPP9Yn3XuV0szyYE3NiTsypoeRk2L6VlZVRuzHndDuxM6emlZMhdsNzQ2/fuKyu2O3t7XH//ffjP//5Dzp16oT8/Hy89dZbda57q2Nw4+uoqChs3boVMTExJvV/7rnn8NFHH2HUqFFYsmQJ2rdvj0uXLmHLli2YN28e2rdvj65du2L9+vUICgpCaWkp5s6dC4VCUSOuZ599FitXrsT58+dx4MABabmTkxPmzJmDiIgICCHwwAMPoKSkBEeOHIGzszMmTpyIt99+G4GBgejVqxeUSiV27twJPz+/O/rZ1PY51XZMbqdtZydD166And29+b7d7Pf9rfK8EU//IyIiIqJGa82aNdBoNAgMDMTs2bPx7rvv3pHtFhYWIi0tzeT+9vb2+P3339GhQwc8+eST8PPzw6RJk1BVVSXd4+jbb79FUVERAgIC8Pzzz2PWrFlo06ZNjW2FhYXh7Nmz8PLykibXMFi6dCneeustLFu2DH5+fhg2bBh27tyJjh07AgBsbGywYMEC9OnTBw8++CCsrKzwww8/1ONI3FtWVoCLi/65MeHNf2/Am/8SERHAm/9S08Ob/1JjoVIBhYVAq1aAjc3d3x9v/ktERHSXWMut4eXkBWu5taVDISJqVjQaoKBA/9yY8JoqIiKianp79EZWRJalwyAianbs7QF/f0tHYT6OVNVCp/vnPlXmtkUtc93fqi2EqNE2MLRvXF69XZ9473VOpraZE3NiTsyJOTEn5nRnczKs1xjatT0aQlzMqenmVNfPk6lYVIH3qbqXOTX1+4AwJ+bEnJpGTrHpsWgf2R67Enc1mZya4ufEnOp/nyptHfcK0mg0Ru17da+gu3n/I+bUOHIqL9fi1Cng+vXGdZ8qTlRxA8PFaEVFRXB1dZUOuEXuhH7yJBAYCG1cHKyCgiBE07y7O3NiTsyJOTXEnE7knkD/Vf0RNykO/b36N4mcmuLnxJxMz6myshIZGRno1KkTbG1tAeinizb8N7ChtWvTEOJiTnc/p4oKgeRkGfz8AHv7u59HVVUVLl68iA4dOsDe3h7APz9PpaWlcHV1NWmiCl5TVQvDLyXD8+2078T9JW5sy2RN854ZzIk5MSfm1BBzMmy/tvuWNNacbid25tS0cjLEbnhu6O3aNIS4mNPdzQm4t3nc7Pf9rfK8kfzWXYiIiIiIiKguLKqIiIiIiIjqgUUVERFRNV3duuLAxAPo6tbV0qEQEVEjwKKKiIioGidbJzzk+xCcbJ0sHQoRUYPm6+uLTz/91NJhWByLKiIiomqyS7Ox4LcFyC7NtnQoRETUCLCoqoVhelOdjjf/vdM5mdpmTsyJOTEnS+aUW56L5UeW40rplSaTU1P8nJgTb/7bmNoNOSedTifdA8rcnG72+nbawD/Ttd/LY1DXz5OpWFSBN/+9lzk19ZsrMifmxJyaRk7lFfo8srOzm0xOTfFzYk5N/+a/Dz/8MGbNmoXXXnsNbm5u8PT0xMKFC6U8UlJSIJPJkJiYKOVx9epVyOVyHDx4EFqtFvv27YNMJsOvv/6Kfv36QaFQ4JFHHkF2djZ27doFPz8/uLi44Nlnn0VJSYlZOcnlcqxatQpjxoyBg4MDunXrhh07dhjlsW/fPgQHB8POzg6enp6YP38+1Gq1dEPchx9+GNOnT8e8efPg7u6Otm3bYvHixTf9nN566y3IZDLI5XLpNhByuRzffvutFON7772Hjh07wt7eHv7+/vj555+lnA4ePAi5XI6dO3ciMDAQdnZ2+P3336FUKjF9+nR4eHjAzs4OgwYNwrFjx276Od34XFVVheLiYkyePBlt2rSBs7MzHnnkEcTFxUmfWXJyMkaNGoW2bdvCyckJQUFB2Lt3r/R9W7JkPqZNGwAHB+Pvnr+/PxYvXix997755hv4+flBoVCgR48e+PLLL6XPSaVS4eWXX0a7du1gZ2cHHx8fvP/++zf97tX35r8QJCkpKREARFFRkRBCCK1WK7RarVltnU4nhBBCo9GY3NbpdDXaIiFBCEBo4uKEEOKf5bW0zY3RYjndIg/mxJyYE3NqKDkdzz4usBgi7nJck8mpKX5OzMn0nK5fvy7OnDkjKisrpX5CCJGdrRPHj+tEQoIQx4/X3s7P1/cvKNAvN6ybnHzrdauq9P3T0oRISfln3RtjqK1teAwePFg4OzuLRYsWifPnz4u1a9cKmUwm9u7dK3Q6nUhPTxcARGJiorSNa9euCQDiwIEDQqfTif379wsA4v777xd//PGHSExMFF26dBGDBw8WQ4YMEQkJCeLQoUPC3d1dLFu2zOQYhRACgGjfvr3YuHGjOH/+vJg5c6ZwdHQUhYWFQqfTiaysLGFvby9efvllcfbsWbFlyxbRqlUr8fbbb0vbMOS4ePFikZKSIuW4Z8+eOvdfWloqcnJyxJUrV8SVK1fERx99JOzt7cWpU6eEEEIsXbpU9OjRQ+zevVtcuHBBrFmzRtja2krH5MCBAwKA6NOnj9i7d69ITU0VhYWFYtasWaJdu3Zi586d4syZM2LixImiZcuW4urVq3V+Tj4+PiIyMlJ6HRoaKkaOHCni4uLE+fPnRUREhHB3d5e2ceLECfH111+LU6dOiZSUFLFw4UJhZ2cnLl68KIQQ4vTp0wKAuHDhgrTNv/76SwAQ58+fF0IIER0dLTw9PcXmzZtFWlqa+Pnnn4Wbm5v47rvvhE6nEx9++KHw9vYWhw4dEhcvXhS///672LBhQ63H8vr16+Ls2bOivLy8xs9TcXGxACBKSkrErbCouoGhqDLlwN11fxdVIiHB0pEQETU7CVcSBBZDJFzh72BqGiorK8XZs2dFZWWl0fJFi/T/3bjZY9Uqfd9Vq/SvDe6//9brXr6s7/vUU0IMGWJ+3IMHDxYPPPCA0bKgoCDx+uuvCyGEyMjIEADEiRMnpPeLioqkokoIIRUQv/32m9Rn2bJlAoBIS0uTlr300kti6NChZsUHQCxcuFB6XV5eLgCIXbt2CSGEeOONN0T37t2l/8QLIURUVJRwdHSUiulb5XgrsbGxws7OTvz4449CCCGqqqqEvb29OHr0qFG/SZMmiWeeeUYI8c8x2bZtm1Hs1tbWUvEhhBAqlUq0a9dOfPDBB3Xu38fHR3zyySdCCCH++OMP4ezsLKqqqoz6dO7cWaxcubLObfTq1Ut88cUXQgghKiuF6N7dX7z99hLp/QULFojg4GCj7W3cuNFoG0uXLhUhISFCCCFmzpwpHnnkEaPjXpe6fjaEMK82aGH6mBYREVHz4K5wx6R+k+CucLd0KER31UsvAU88cfM+HTron0ePBgIC/lm+di1QUXHzddu00T//5z/ADZeMm6VPnz5Grz09PZGfn1+v7Xh4eMDe3h6dOnUyWmY4Te12t+vg4ABnZ2cpvuTkZISEhEAmk0l9Bg0ahPLycmRlZaHD3wf3ZjlOnToV69evl94znBYK6E8fHT16NObMmYNx48YBAC5cuIDr16/jscceM9qmSqVCv379jJb1799faqelpUGtVmPQoEHSMmtrawwYMMDo0pSbOXnyJMrLy+Hubvy7s7KyEmlpaVL8ixcvxs6dO5GTkwONRoPKykrpdFqZDBgzJgw//rgG77zzFoQQ+O9//4uIiAgAQEVFBdLS0jBp0iRMmTJF2odGo4GLiwsAIDw8HI899hi6d++OYcOG4fHHH8eQIUNMyuF2sagiIiKqxsfVB6ufWG3pMIjuOk9P/cMUrVrpHwbdu5u+nxtqF7NZW1sbvZbJZNI1MXK5fnoAccOEAoZrcG62HZlMdtPt3qn47sQ2lixZgjlz5tRYp6KiAk888QRCQkKwZMkSabmh6Nq5cye8vLyM1rG1tTV67eDgYFact1JeXg5PT08cPHiwxnuurq4AgDlz5mDfvn346KOP0KVLFygUCjz11FPSNVW2tsD06c/gP/95HYmJiaisrMTly5cxfvx4o/xWrVqF4OBgo31YWVkBAAICApCRkYFdu3bht99+w7hx4xAaGoqff/75juZ7IxZVRERE1VSqK5FelI5OLTtBYa2wdDhEVIfWrVsD0E8EYhiFSUpKsmBExvz8/LB582YIIaTRqiNHjsDJyQnt27c3aRtt2rRBG8OQ39+EEHjuueeg0+kQHR1tNBLWs2dP2NraIjMzE4MHDzY51s6dO8PGxgZHjhyBj48PAH2BGh8fj9mzZ5u0jYCAAOTm5qJFixbw9fWttc+RI0cQHh6OMWPGANAXSRcvXpTe1+kAd/f2GDx4MDZs2IDKyko89thj0jHw8PBAu3btkJ6ejrCwsDpjcXZ2xvjx4zF+/Hg89dRTGDZsGK5duwY3NzeTcjEXiyoiIqJqkguTEfhNIBJeTECAZ8CtVyAii1AoFLj//vuxfPlydOzYEfn5+dLsgPW1YsUKbN26FTExMbe9jWnTpuHTTz/FzJkzMWPGDKSkpGDRokWIiIiQRtlux+LFi/Hbb79h7969KC8vl0ZvXFxc4OTkhDlz5uDVV1+FTqfDAw88gJKSEhw5cgTOzs6YOHFirdt0cHDAyy+/jLlz58LNzQ0dOnTABx98gOvXr2PSpEkmxRUaGoqQkBCMHj0aH3zwAbp164YrV65g586dGDNmDPr374+uXbtiy5YtGDlyJGQyGd566y2jkb3KSiA5GXjqqTC8//4iqFQqfPLJJ0b7eeeddzBr1iy4uLhg2LBhUCqVOH78OIqKihAREYHIyEh4enqiX79+kMvl+Omnn9C2bVtptOxuYFFFRERERI3WmjVrMGnSJAQGBqJ79+744IMP7sj1M4WFhdJ1QLfLy8sLv/76K+bOnQt/f3+4ublh0qRJ9S78Dh06hPLycgwcONBo+XfffYfw8HAsXboUrVu3xrJly5Ceng5XV1cEBATgjTfeuOl2ly9fDp1Oh+effx5lZWXo378/9uzZg5YtW5oUl2Hq+jfffBMvvPACCgoK0LZtWzz44IPw8PAAAERGRuLf//43Bg4ciFatWuH111+XbiVwo9Gjn8Jrr82AlZUVRo8ebfTe5MmTYW9vjw8//BBz586Fg4MDevfuLY2oOTk54YMPPkBqaiqsrKwQFBSEX3/9tV6F7C1zFzeehNrMlZaWwsXFBSUlJXB2drZsMImJQGAgkJBgfFUoERHddYk5iRypoialqqoKGRkZ6NixI+zs7CwdDlGdKir0I1V+fsAdvuSrVjf72TCnNuDNf2thGILUWfhO6IY+QNO9uztzYk7MiTkxJ+bEnO5NTob1GkO7tkdDiIs53f2cgH/Ge+5lTnX9PJmKRRWAqKgo9OzZE0FBQQCA3NxcAJa/EzoAXLp0CUDTvLs7c2JOzIk5NdScKioqYGNlgyvZV5pMTk3xc2JOpudUVVUFjUYDQF+8Gbah1WqN2oYZ2DQajVHbMKOeWq02ahu2qVKpjNqGYk6pVBq1DQVkVVWVUdvwn9fqbUD/H9/a2jqdzqjNnJpGTjcOLtzLnNLS0mr9eTIVT/+7gWGIr6ioCK6urtIBl8vlJrdlMhlkMhm0Wi3kcrlJbUD/JbuxbXXyJBAYCG1cHKyCgiCE/q9jVlZWNdpCCLNitFhOtcTOnJgTc2JOzIk5Mae7n1NlZSUyMjLQqVMnaVptmUwm/YeyobVr0xDiYk53P6eKCoHkZBn8/AB7+7ufR1VVFS5evIgOHTrA3t4ewD8/T6WlpXB1dTXp9D8WVTfgNVVERETUFPGaKmoseE0VERFRE5FckIyAlQFILki+dWeiRoR/Sycydqd+JlhUERERVVOpqcSJ3BOo1FRaOhSiO8LKygoApOtWiBoqKyvA2Vn/fC9cv34dAGBtbV2v7fA+VURERERNXIsWLWBvb4+CggJYW1tL118RNUQdOuif/57f4q4QQuD69evIz8+Hq6ur9IeH28WiioiIiKiJk8lk8PT0REZGhjSzMFFDJIT+IZPpH3ebq6sr2rZtW+/tsKgiIiIiagZsbGzQtWtXngJIDdqZM8DYscDmzUCvXnd3X9bW1vUeoTJgUUVERFRNR9eO2PTUJnR07WjpUIjuKLlcztn/qEHr1An48EP9c2P6qrKoIiIiqqaloiWe7vW0pcMgImp2WrYEnm6Ev355lSIREVE1eeV5iIyNRF55nqVDISJqVvLygMhI/XNjwqKKiIiomuyybLy29zVkl2VbOhQiomYlOxt47TX9c2PCoqoWOp1Oeja3bbiBmFarNbkthKjRNjC0b1xevV2feO91Tqa2mRNzYk7MiTkxJ+bEnJhT88zJoCHkZCoWVQCioqLQs2dPBAUFAQByc3MBAHl5ecj7e+wxJycHBQUFAICsrCxcu3YNAJCZmYni4mIAQEZGBkpLSwEA6enpKC8vBwCkpqaislJ/A8mUlBQolUoAQHJyMjQaDXQ6HZKTk6HT6aDRaJCcnCzFZpj2tLKyEqmpqQCA8vJypKenAwBKS0uRkZEBACguLkZmZiYA4Nq1a8jKygIAFBQUICcnp0HkpFQqkZKSwpyYE3NiTg06p/IKfR7Z2dlNJqem+DkxJ+bEnJpeTiUlJTBoCDmZSibMKcGauNLSUri4uKCoqAiurq5SpSuXy01uy2QyyGQyaLVayOVyk9qAvpqWy+XA5cvQ5eXB6vx54LnnoP3+e1j16gXRujV07drByspKquwNbSGEWTHe85z+btcWe11t5sScmBNzsmROJ3JPoP+q/oibFIf+Xv2bRE5N8XNiTsyJOTW9nI4f1yEoSI6EBKBvX8vmVFpaCldXV5SUlMDZ2Rk3w6LqBoaiypQDd1colYCPT+1X5rVtC1y8CNja3vOwiIiam7RraXh1z6v4ZOgn6OzW2dLhEBE1G4mJQGAgkJAABARYNhZzagNOqd6Q2NgAHToABQXA35U2AEAuB7y99e8TEdFd19mtM3Y8s8PSYRARUSPBa6oaEpkMWLrUuKAC9K+XLtW/T0REd51aq0ZBRQHUWrWlQyEiokaARVVDM2QIEBSkH50C9M9BQfrlRER0T5zOP402H7XB6fzTlg6FiIgaARZVDU310SqOUhERERFRM+HlBXz8sf65MWFR1RAZRqsAjlIRERERUbPh4QFEROifGxMWVQ2RTAa8/z7g56d/5igVERERETUDRUXATz/pnxsTzv7XUIWGAmfPWjoKIiIiIqJ7JiMDGDdOP6V6y5aWjsZ0LKqIiIiq8ffwR8n8EjhYO1g6FCKiZsXfHygpARwa2a9fFlVERETVWMmt4GxrgZvAExE1c1ZWwC3us9sg8ZoqIiKialKvpmLo+qFIvZpq6VCIiJqV1FRg6FD9c2PCooqIiKiaMlUZ9qbtRZmqzNKhEBE1K2VlwN69+ufGhEUVERERERFRPbCoIiIiIiIiqgcWVbXQ6XTSs7ltIQQAQKvVmtwWQtRoAzC5XZ94mRNzYk7MiTkxJ+bEnJgTc2pIORk0hJxMxaIKQFRUFHr27ImgoCAAQG5uLgAgLy8PeXl5AICcnBwUFBQAALKysnDt2jUAQGZmJoqLiwEAGRkZKC0tBQCkp6ejvLwcAJCamorKykoAQEpKCpRKJQAgOTkZGo0GOp0OycnJ0Ol00Gg0SE5OBgAolUqkpKQAACorK5H69xV75eXlSE9PBwCUlpYiIyMDAFBcXIzMzEwAwLVr15CVlQUAKCgoQE5ODnNiTsyJOTEnE3NylblixfAV0BXpmkxOTfFzYk7MiTk1vZxKSkpg0BByMpVMmFOCNXGlpaVwcXFBUVERXF1dpUpXLpeb3JbJZJDJZNBqtZDL5Sa1AX01fWPbyspKquBv1RZCmBUjc2JOzIk5MSfmxJyYE3NiTg0xp+PHdQgKkiMhAejb17I5lZaWwtXVFSUlJXC+xTzvLKpuYCiqTDlwRETUdF2rvIZfU3/FiK4j4KZws3Q4RETNRmIiEBgIJCQAAQGWjcWc2oCn/xEREVVzsfgint/6PC4WX7R0KERE1Ai0sHQAREREREREAODnB/z1F9Cpk6UjMQ+LKiIiIiIiahAUCqBXL0tHYT6e/kdERERERA3CpUvA5Mn658aERRUREVE1DtYOuL/9/XCwdrB0KEREzUpVFXDmjP65MeHsfzfg7H9ERERERARw9j8iIiIiIqJ7hkUVERFRNYk5iZC9I0NiTqKlQyEialYSEwGZTP/cmLCoIiIiIiIiqgcWVURERERERPXAooqIiIiIiKgeWFQRERERERHVQwtLB0BERNTQ9GzdE6kzU9Heub2lQyEiokaARRUREVE1di3s0MWti6XDICKiRoKn/xEREVWTUZSB57Y8h4yiDEuHQkREjQCLKiIiomqKqoqw4fQGFFUVWToUIqJmxdoa8PLSPzcmPP2PiIiIiIgahN69gawsS0dhPo5UERERERER1QOLKiIiIiIiahBOnwbat9c/NyY8/Y+IiOhvl0suo+B6AQquF+DFgBdRcL0AiTmJaOPQhtOrExHdA61aAZMn658bE5kQQlg6iIaitLQULi4uKCoqgqurK3Q6HQBALpeb3JbJZJDJZNBqtZDL5Sa1AUCn0xm1raysIIQwqS2EMCtG5sScmBNzYk4120qNEh0/74i8ijxU19axLdJmpEFho2hUOTXFz4k5MSfmxJzuVU6lpaVwdXVFSUkJnJ2dcTM8/Q9AVFQUevbsiaCgIABAbm4uACAvLw95efp/XHNyclBQUAAAyMrKwrVr1wAAmZmZKC4uBgBkZGSgtLQUAJCeno7y8nIAQGpqKiorKwEAKSkpUCqVAIDk5GRoNBrodDokJydDp9NBo9EgOTkZAKBUKpGSkgIAqKysRGpqKgCgvLwc6enpAPSFYEaGfsrf4uJiZGZmAgCuXbuGrL+v8isoKEBOTg5zYk7MiTkxp5vkdL3sOjq4dIAMMtxIDjm8nb1xKf1So8upKX5OzIk5MaemndOlS0XYsKEQpaUNIydTcaTqBhypYk7MiTkxp+ad0770fRi2YRiq2x22G6EdQxtlTk3xc2JOzIk5Nd2cjh/XIShIjoQEoG9fy+ZkzkgVi6obGIoqUw4cERE1PUIIBK8ORsKVBOiggxxyBLYLxLHJxyCTyW69ASIiqpfERCAwEEhIAAICLBuLObUBT/8jIiL6m0wmw9KHl0IH/V87ddBh6cNLWVAREdFNsagiIiK6wZDOQxDUTn+NbVC7IAzpPMTCERERUUPHooqIiOgGMpkM7z/6Pvxa+eH9R9/nKBUREd0S71NFRERUTWinUJydftbSYRARUSPBkSoiIiIiIqJ6YFFFRERERERUDyyqiIiIiIioQXB3ByZN0j83JrymioiIiIiIGgQfH2D1aktHYT6OVBERERERUYNQWQmcOaN/bkxYVBERERERUYOQnAzcd5/+uTFhUUVERERERA1Ct27A0aP658aE11QREREREVGD4OgIhIRYOgrzcaSKiIiIiIgahKwsICJC/9yYsKgiIiIiIqIGIT8f+OQT/XNjwqKKiIiIiIioHlhUERERERER1QMnqiAiIiIiIou6fBkoKPhnKnXDc5s2QPv2lovLVCyqiIiIiIjIYpRKICgIyMv7Z9lzz+mf27YFLl4EbG0tEprJePofERERERFZjI0N0KEDIK9WmcjlgLe3/v2GjkUVERERERFZjEwGLF0K6HTGy3U6/XKZzDJxmYNFFRERERERWdSQIfpTAA2jVXK5/vWQIZaNy1QsqoiIiIiIyKKqj1Y1plEqgEUVERERERE1AIbRKqBxjVIBLKqIiIiIiKgBkMmA998H/Pz0z41llArglOpERERERNRAhIYCZ89aOgrzcaSKiIiIiIioHlhUERERERER1QOLKiIiIiIionpgUUVERERERFQPLKqIiIiIiIjqgbP/3UAIAQAoLS21cCRERERERGRJhprAUCPcDIuqG5SVlQEAvL29LRwJERERERE1BGVlZXBxcblpH5kwpfRqJnQ6Ha5cuQInJyfIGtPdxoiI6I4qLS2Ft7c3Ll++DGdnZ0uHQ0REFiCEQFlZGdq1awe5/OZXTbGoIiIiqqa0tBQuLi4oKSlhUUVERLfEiSqIiIiIiIjqgUUVERERERFRPbCoIiIiqsbW1haLFi2Cra2tpUMhIqJGgNdUERERERER1QNHqoiIiIiIiOqBRRUREREREVE9sKgiIiIiIiKqBxZVRETUZMhkMmzbtq3J7IeIiBoHFlVERNRo5ObmYubMmejUqRNsbW3h7e2NkSNHIiYmxtKhmS08PByjR4+2dBhERHQHtLB0AERERKa4ePEiBg0aBFdXV3z44Yfo3bs31Go19uzZg+nTp+PcuXOWDpGIiJopjlQREVGjMG3aNMhkMsTFxWHs2LHo1q0bevXqhYiICPz5559Sv8LCQowZMwb29vbo2rUrduzYIb2n1WoxadIkdOzYEQqFAt27d8dnn31mtJ/4+Hg89thjaNWqFVxcXDB48GAkJibWiKc++1m8eDHWrVuH7du3QyaTQSaT4eDBg3fwaBER0b3EooqIiBq8a9euYffu3Zg+fTocHBxqvO/q6iq133nnHYwbNw6nTp3CiBEjEBYWhmvXrgEAdDod2rdvj59++glnz57F22+/jTfeeAObNm2S1i8rK8PEiRNx+PBh/Pnnn+jatStGjBiBsrIyo33WZz9z5szBuHHjMGzYMOTk5CAnJwcDBw6804eNiIjuEd78l4iIGry4uDgEBwdjy5YtGDNmTJ39ZDIZFi5ciKVLlwIAKioq4OjoiF27dmHYsGG1rjNjxgzk5ubi559/rvV9nU4HV1dXbNy4EY8//vgd2094eDiKi4s54QURURPAa6qIiKjBM+fvf3369JHaDg4OcHZ2Rn5+vrQsKioKa9asQWZmJiorK6FSqdC3b1/p/by8PCxcuBAHDx5Efn4+tFotrl+/jszMzDu6HyIiajp4+h8RETV4Xbt2hUwmM2kyCmtra6PXMpkMOp0OAPDDDz9gzpw5mDRpEvbu3YukpCS88MILUKlUUv+JEyciKSkJn332GY4ePYqkpCS4u7sb9bkT+yEioqaDI1VERNTgubm5YejQoYiKisKsWbNqXFdVXFxsdF1VXY4cOYKBAwdi2rRp0rK0tLQafb788kuMGDECAHD58mUUFhaaFa8p+7GxsYFWqzVru0RE1DBxpIqIiBqFqKgoaLVaDBgwAJs3b0ZqaiqSk5Px+eefIyQkxKRtdO3aFcePH8eePXtw/vx5vPXWW4iPj6/RJzo6GsnJyTh27BjCwsKgUCjMitWU/fj6+uLUqVNISUlBYWEh1Gq1WfsgIqKGg0UVERE1Cp06dUJiYiIefvhhvPbaa7jvvvvw2GOPISYmBl999ZVJ23jppZfw5JNPYvz48QgODsbVq1eNRpMA4Ntvv0VRURECAgLw/PPPY9asWWjTpo1ZsZqynylTpqB79+7o378/WrdujSNHjpi1DyIiajg4+x8REREREVE9cKSKiIiIiIioHlhUERERERER1QOLKiIiIiIionpgUUVERERERFQPLKqIiIiIiIjqgUUVERERERFRPbCoIiIiIiIiqgcWVURERERERPXAooqIiIiIiKgeWFQRERERERHVA4sqIiIiIiKiemBRRUREREREVA8sqoiIiIiIiOqBRRUREREREVE9sKgiIiIiIiKqBxZVRERERERE9cCiioiIiIiIqB5YVBEREREREdUDiyoiIiIiIqJ6YFFFRERERERUDyyqiIiIiIiI6oFFFRERERERUT2wqCIiIiIiIqoHFlVERERERET1wKKKiIiIiIioHlhUERERERER1QOLKiIiIiIionpgUUVERERERFQPLKqIqEF76KGHMHv2bJP7r127Fq6urnctnoZs8eLF6Nu3r6XDsKiLFy9CJpMhKSnJ5HUa83Gr/n03JZfw8HCMHj263vu+U9uxFHN/txAR3QyLKiKiJmLOnDmIiYmxdBhmk8lk2LZt2x3Zlre3N3JycnDfffeZvE5jPW61uRu51FWofvbZZ1i7du0d3VdDcjeK7Xv1Rx9fX198+umnd30/RPSPFpYOgIiouVOpVLCxsan3dhwdHeHo6HgHImp4TD1GVlZWaNu2rVnbbkrH7V7m4uLick/2Q0TUGHCkiojM9tBDD2HmzJmYPXs2WrZsCQ8PD6xatQoVFRV44YUX4OTkhC5dumDXrl1G6x06dAgDBgyAra0tPD09MX/+fGg0Gun9iooKTJgwAY6OjvD09MTHH39cY99KpRJz5syBl5cXHBwcEBwcjIMHD5oV/+uvv45u3brB3t4enTp1wltvvQW1Wm3U53//+x+CgoJgZ2eHVq1aYcyYMUYxvP766/D29oatrS26dOmCb7/9FkDtf4netm0bZDKZ9NrwF/DVq1ejY8eOsLOzAwDs3r0bDzzwAFxdXeHu7o7HH38caWlpRtvKysrCM888Azc3Nzg4OKB///44duyY0XZvtHr1avj5+cHOzg49evTAl19+Kb2nUqkwY8YMeHp6ws7ODj4+Pli2bFmdx02n02HJkiVo3749bG1t0bdvX+zevVt63zCisWXLFjz88MOwt7eHv78/YmNj69ymr68vAGDMmDGQyWTS69s9RtVHVQ4ePAiZTIaYmBj0798f9vb2GDhwIFJSUmp8HgaG09o++ugjeHp6wt3dHdOnTzf6juTk5OBf//oXFAoFOnbsiI0bN5o1OqDT6dC+fXt89dVXRstPnDgBuVyOS5cuAQAiIyPRu3dvODg4wNvbG9OmTUN5eXmd262ei1arRUREhHS85s2bByGE0Tq3OqYdO3YEAPTr1w8ymQwPPfSQ0XEyUCqVmDVrFtq0aQM7Ozs88MADiI+Pl9435bOoza1+Xg05R0dHw9fXFy4uLvi///s/lJWVSX1M+d1yo7Vr1+Kdd97ByZMnIZPJIJPJpFG54uJiTJ48Ga1bt4azszMeeeQRnDx5Ulr35MmTePjhh+Hk5ARnZ2cEBgbi+PHjOHjwIF544QWUlJRI21y8eHGt+69rGwaHDx/G//t//w8KhQLe3t6YNWsWKioqAOh/P1+6dAmvvvqqtB8AuHTpEkaOHImWLVvCwcEBvXr1wq+//nrT40BEpmNRRUS3Zd26dWjVqhXi4uIwc+ZMvPzyy3j66acxcOBAJCYmYsiQIXj++edx/fp1AEB2djZGjBiBoKAgnDx5El999RW+/fZbvPvuu9I2586di0OHDmH79u3Yu3cvDh48iMTERKP9zpgxA7Gxsfjhhx9w6tQpPP300xg2bBhSU1NNjt3JyQlr167F2bNn8dlnn2HVqlX45JNPpPd37tyJMWPGYMSIEThx4gRiYmIwYMAA6f0JEybgv//9Lz7//HMkJydj5cqVZo8OXLhwAZs3b8aWLVukAqCiogIRERE4fvw4YmJiIJfLMWbMGOh0OgBAeXk5Bg8ejOzsbOzYsQMnT57EvHnzpPer27BhA95++2289957SE5Oxvvvv4+33noL69atAwB8/vnn2LFjBzZt2oSUlBRs2LBBKmpq89lnn+Hjjz/GRx99hFOnTmHo0KF44oknahz7N998E3PmzEFSUhK6deuGZ555xqh4vpHhP93fffcdcnJyjP4TfjvHqC5vvvkmPv74Yxw/fhwtWrTAv//975v2P3DgANLS0nDgwAGsW7cOa9euNTrVbcKECbhy5QoOHjyIzZs345tvvkF+fv5Nt3kjuVyOZ555Bhs3bjRavmHDBgwaNAg+Pj5Sv88//xxnzpzBunXrsH//fsybN8/k/Xz88cdYu3Yt1qxZg8OHD+PatWvYunWrUZ9bHdO4uDgAwG+//YacnBxs2bKl1n3NmzcPmzdvxrp165CYmIguXbpg6NChuHbtmlE/cz+LW/28AkBaWhq2bduGX375Bb/88gsOHTqE5cuXS++b8rvlRuPHj8drr72GXr16IScnBzk5ORg/fjwA4Omnn0Z+fj527dqFhIQEBAQE4NFHH5XyDAsLQ/v27REfH4+EhATMnz8f1tbWGDhwID799FM4OztL25wzZ06t+69rG4Zchw0bhrFjx+LUqVP48ccfcfjwYcyYMQMAsGXLFrRv3x5LliyR9gMA06dPh1KpxO+//47Tp0/jP//5T5MZoSVqEAQRkZkGDx4sHnjgAem1RqMRDg4O4vnnn5eW5eTkCAAiNjZWCCHEG2+8Ibp37y50Op3UJyoqSjg6OgqtVivKysqEjY2N2LRpk/T+1atXhUKhEK+88ooQQohLly4JKysrkZ2dbRTPo48+KhYsWCCEEOK7774TLi4uZuXz4YcfisDAQOl1SEiICAsLq7VvSkqKACD27dtX6/u17X/r1q3ixl+3ixYtEtbW1iI/P/+mcRUUFAgA4vTp00IIIVauXCmcnJzE1atXa+2/aNEi4e/vL73u3Lmz2Lhxo1GfpUuXipCQECGEEDNnzhSPPPKI0WdyM+3atRPvvfee0bKgoCAxbdo0IYQQGRkZAoBYvXq19P6ZM2cEAJGcnFzndgGIrVu31sjldo6RIYYTJ04IIYQ4cOCAACB+++03aZ2dO3cKAKKyslLa143HbeLEicLHx0doNBpp2dNPPy3Gjx8vhBAiOTlZABDx8fHS+6mpqQKA+OSTT24a741OnDghZDKZuHTpkhBCCK1WK7y8vMRXX31V5zo//fSTcHd3l15X/75Vz8XT01N88MEH0mu1Wi3at28vRo0aVec+bnVMDSZOnChtp7y8XFhbW4sNGzZI76tUKtGuXTtp/6Z8Fqao/vO6aNEiYW9vL0pLS6Vlc+fOFcHBwUIIYdLvltpUP5ZCCPHHH38IZ2dnUVVVZbS8c+fOYuXKlUIIIZycnMTatWtr3aapv59uto1JkyaJF198sUZccrlcOo4+Pj41vou9e/cWixcvvuW+iej2cKSKiG5Lnz59pLaVlRXc3d3Ru3dvaZmHhwcASH+9T05ORkhIiNFpcIMGDUJ5eTmysrKQlpYGlUqF4OBg6X03Nzd0795den369GlotVp069ZNunbE0dERhw4dqnGa3M38+OOPGDRoENq2bQtHR0csXLgQmZmZ0vtJSUl49NFHa103KSkJVlZWGDx4sMn7q42Pjw9at25ttCw1NRXPPPMMOnXqBGdnZ2nUyBBbUlIS+vXrBzc3t1tuv6KiAmlpaZg0aZLRsXr33XelYxUeHo6kpCR0794ds2bNwt69e+vcXmlpKa5cuYJBgwYZLR80aBCSk5ONlt343fD09AQAs0ZxDG7nGNXF3Jh69eoFKysro3UM/VNSUtCiRQsEBARI73fp0gUtW7Y0LbG/9e3bF35+ftJo1aFDh5Cfn4+nn35a6vPbb7/h0UcfhZeXF5ycnPD888/j6tWr0gjwzZSUlCAnJ8foZ6pFixbo37+/Ub/bPaY3SktLg1qtNvp+WFtbY8CAAfX+ftzq5xXQn0bq5ORktF3DNk353WKqkydPory8HO7u7kY/VxkZGdLPVUREBCZPnozQ0FAsX77crN9NBjfbxsmTJ7F27Vqj/Q8dOhQ6nQ4ZGRl1bnPWrFl49913MWjQICxatAinTp0yOy4iqhuLKiK6LYZTUQxkMpnRMkPxdKvTssxRXl4OKysrJCQkICkpSXokJyfjs88+M2kbsbGxCAsLw4gRI/DLL7/gxIkTePPNN6FSqaQ+CoWizvVv9h6gP11LVLtmpfr1WgDg4OBQY9nIkSNx7do1rFq1CseOHZOulTLEdqt938hw3c2qVauMjtVff/2FP//8EwAQEBCAjIwMLF26FJWVlRg3bhyeeuopk/dRlzv1PbidY3SnYqrt+30nv8sGYWFhUlG1ceNGDBs2DO7u7gD014c9/vjj6NOnDzZv3oyEhARERUUBuHW+5rjdY3q7zPksTPl5rb5Nw3bvxudVXl4OT09Po5+ppKQkpKSkYO7cuQD013idOXMG//rXv7B//3707NmzximXt3KzbZSXl+Oll14y2v/JkyeRmpqKzp0717nNyZMnIz09Hc8//zxOnz6N/v3744svvrj9g0FERlhUEdE94efnh9jYWKOC48iRI3ByckL79u3RuXNnWFtbS/+hA4CioiKcP39eet2vXz9otVrk5+ejS5cuRg9TZ3w7evQofHx88Oabb6J///7o2rWrNCmAQZ8+feqclrp3797Q6XQ4dOhQre+3bt0aZWVl0kXjAEy6Z9LVq1eRkpKChQsX4tFHH4Wfnx+KiopqxJWUlFTjGpXaeHh4oF27dkhPT69xrAwTDwCAs7Mzxo8fj1WrVuHHH3/E5s2ba92+s7Mz2rVrhyNHjhgtP3LkCHr27HnLeG7G2toaWq32lv1MOUb3Qvfu3aHRaHDixAlp2YULF24rlmeffRZ//fUXEhIS8PPPPyMsLEx6LyEhATqdDh9//DHuv/9+dOvWDVeuXDF52y4uLvD09DT6mdJoNEhISJBem3JMDbMu3uwz6ty5M2xsbIy+H2q1GvHx8fX6fpjy83orpvxuqY2NjU2NnAMCApCbm4sWLVrU+Llq1aqV1K9bt2549dVXsXfvXjz55JP47rvv6txmXeraRkBAAM6ePVtj/126dJE+q7r24+3tjalTp2LLli147bXXsGrVKpNiIaJbY1FFRPfEtGnTcPnyZcycORPnzp3D9u3bsWjRIkREREAul8PR0RGTJk3C3LlzsX//fvz1118IDw+HXP7Pr6lu3bohLCwMEyZMwJYtW5CRkYG4uDgsW7YMO3fuNCmOrl27IjMzEz/88APS0tLw+eef1/gr8qJFi/Df//4XixYtQnJysnRRN6A/zWjixIn497//jW3btiEjIwMHDx7Epk2bAADBwcGwt7fHG2+8gbS0NGzcuNGke/m0bNkS7u7u+Oabb3DhwgXs378fERERRn2eeeYZtG3bFqNHj8aRI0eQnp6OzZs31zm73jvvvINly5bh888/x/nz53H69Gl89913iIyMBKCfWe6///0vzp07h/Pnz+Onn35C27Zt67yPzty5c/Gf//wHP/74I1JSUjB//nwkJSXhlVdeuWV+N+Pr64uYmBjk5ubetDAx5RjdCz169EBoaChefPFFxMXF4cSJE3jxxRehUCiMTm81ha+vLwYOHIhJkyZBq9XiiSeekN7r0qUL1Go1vvjiC6SnpyM6Ohpff/21Wdt/5ZVXsHz5cmzbtg3nzp3DtGnTUFxcLL1vyjFt06YNFAoFdu/ejby8PJSUlNTYj4ODA15++WXMnTsXu3fvxtmzZzFlyhRcv34dkyZNMivmG5ny83orpvxuqY2vry8yMjKQlJSEwsJCKJVKhIaGIiQkBKNHj8bevXtx8eJFHD16FG+++SaOHz+OyspKzJgxAwcPHsSlS5dw5MgRxMfHw8/PT9pmeXk5YmJiUFhYWOtpnLfaxuuvv46jR49ixowZSEpKQmpqKrZv3y5NVGHYz++//47s7GwUFhYCAGbPno09e/YgIyMDiYmJOHDggLRNIroDLH1RFxE1PoMHD65xgXdtF0aj2gQEBw8eFEFBQcLGxka0bdtWvP7660KtVkvvl5WVieeee07Y29sLDw8P8cEHH9TYl0qlEm+//bbw9fUV1tbWwtPTU4wZM0acOnVKCGHaheBz584V7u7uwtHRUYwfP1588sknNdbZvHmz6Nu3r7CxsRGtWrUSTz75pPReZWWlePXVV4Wnp6ewsbERXbp0EWvWrJHe37p1q+jSpYtQKBTi8ccfF998802NiSqqXwAvhBD79u0Tfn5+wtbWVvTp00ccPHiwxjG8ePGiGDt2rHB2dhb29vaif//+4tixY3Vud8OGDVIeLVu2FA8++KDYsmWLEEKIb775RvTt21c4ODgIZ2dn8eijj4rExMQ6j5tWqxWLFy8WXl5ewtraWvj7+4tdu3ZJ79c2oUFRUZEAIA4cOFDndnfs2CG6dOkiWrRoIXx8fOp1jOqaqKKoqEjaxokTJwQAkZGRUeu+bpyAweCVV14RgwcPll5fuXJFDB8+XNja2gofHx+xceNG0aZNG/H1118bbefGdery5ZdfCgBiwoQJNd6LjIwUnp6eQqFQiKFDh4rvv//eKJ9bTVShVqvFK6+8IpydnYWrq6uIiIgQEyZMMMrPlO/dqlWrhLe3t5DL5VJO1Y9TZWWlmDlzpmjVqpWwtbUVgwYNEnFxcdL7pnwWtbnVz2tt35VPPvlE+i4JYdrvluqqqqrE2LFjhaurqwAgvvvuOyGEEKWlpWLmzJmiXbt2wtraWnh7e4uwsDCRmZkplEql+L//+z/h7e0tbGxsRLt27cSMGTOMJuKYOnWqcHd3FwDEokWLauzXlG3ExcWJxx57TDg6OgoHBwfRp08fo0lkYmNjRZ8+fYStra30u2fGjBmic+fOwtbWVrRu3Vo8//zzorCwsM78icg8MiGqnfxPREREZsnKyoK3t7c0sQQADB48GA8//HCd9yIiIqKmg0UVERGRmfbv34/y8nL07t0bOTk5mDdvHrKzs3H+/HlYW1ujpKQEvXr1wrlz53gvICKiZqCFpQMgIiJqbNRqNd544w2kp6fDyckJAwcOxIYNG6RZ6FxcXJCVlWXhKImI6F7hSBUREREREVE9cPY/IiIiIiKiemBRRUREREREVA8sqoiIiIiIiOqBE1XcQKfT4cqVK3BycjL7Bo5ERERERNR0CCFQVlaGdu3a3fKG4SyqbnDlyhV4e3tbOgwiIiIiImogLl++jPbt29+0D4uqGzg5OQHQHzhnZ2cLR0NERERERJZSWloKb29vqUa4GRZVNzCc8ufs7MyiioiIiIiITLosiBNVEBERERER1QOLKiIiIiIionpgUUVERERERFQPLKqIiIiIiIjqgUUVERERERFRPbCoaojy8oDhwwF3d/1zXp6lIyIiIiIiojqwqGqIwsOBvXuBa9f0z+Hhlo6IiIiIiIjqwKKqIYqLA3Q6fVunA+LjLRsPERERERHViUVVQzRgACD/+6ORy4GgIMvGQ0REREREdWJR1RCtXQsEB+vbwcH610RERERE1CCxqGqIPDyAFSv07RUr9K+JiIiIiKhBYlFFRERERERUD7dVVEVFRcHX1xd2dnYIDg5GXFxcnX3VajWWLFmCzp07w87ODv7+/ti9e7dRn7KyMsyePRs+Pj5QKBQYOHAg4qtNzpCXl4fw8HC0a9cO9vb2GDZsGFJTU436VFVVYfr06XB3d4ejoyPGjh2LPE5HTkREREREd5HZRdWPP/6IiIgILFq0CImJifD398fQoUORn59fa/+FCxdi5cqV+OKLL3D27FlMnToVY8aMwYkTJ6Q+kydPxr59+xAdHY3Tp09jyJAhCA0NRXZ2NgBACIHRo0cjPT0d27dvx4kTJ+Dj44PQ0FBUVFRI23n11Vfxv//9Dz/99BMOHTqEK1eu4MknnzQ3RSIiIiIiIpPJhBDCnBWCg4MRFBSEFX9f86PT6eDt7Y2ZM2di/vz5Nfq3a9cOb775JqZPny4tGzt2LBQKBdavX4/Kyko4OTlh+/bt+Ne//iX1CQwMxPDhw/Huu+/i/Pnz6N69O/766y/06tVL2m/btm3x/vvvY/LkySgpKUHr1q2xceNGPPXUUwCAc+fOwc/PD7Gxsbj//vtvmVtpaSlcXFxQUlICZ2dncw7LnZeYCAQGAgkJQECAZWMhIiIiImpmzKkNzBqpUqlUSEhIQGho6D8bkMsRGhqK2NjYWtdRKpWws7MzWqZQKHD48GEAgEajgVarvWkfpVIJAEZ95HI5bG1tpT4JCQlQq9VGsfXo0QMdOnS4aWylpaVGD0B/yqIhNo1GIy2rra1SqaDVamtt6/6+15RSqayzLYSAEKJG20ClUgHQF5GG5TqdTlqu1WprbWs0GqM8GkpO1fNgTsyJOTEn5sScmBNzYk7MqaHmZCqziqrCwkJotVp4VJuNzsPDA7m5ubWuM3ToUERGRiI1NRU6nQ779u3Dli1bkJOTAwBwcnJCSEgIli5diitXrkCr1WL9+vWIjY2V+hiKowULFqCoqAgqlQr/+c9/kJWVJfXJzc2FjY0NXF1dTY5t2bJlcHFxkR7e3t4AgH379gEAYmJiEBMTAwDYtWuXVMBt27ZNuuZr06ZNOHnyJAAgOjoa586dAwCsXr0a6enpAPTXoBlOZYyMjERhYSEAYPny5SgrK4NKpcLy5cuhUqlQVlaG5cuXSzGuX78eAJCdnY2oqCgAQHp6OlavXg1APxoXHR0NADh58iQ2bdoEAIiPj8e2bdsAAIcPH8auXbsaRE6FhYWIjIxkTsyJOTEn5sScmBNzYk7MqcHnZDJhhuzsbAFAHD161Gj53LlzxYABA2pdJz8/X4waNUrI5XJhZWUlunXrJqZNmybs7OykPhcuXBAPPvigACCsrKxEUFCQCAsLEz169JD6HD9+XPj7+0t9hg4dKoYPHy6GDRsmhBBiw4YNwsbGpsb+g4KCxLx582qNraqqSpSUlEiPy5cvCwCisLBQCCGEWq0WarVaCCGESqWqta1UKoVGo6m1rdVqpf3U1dbpdEKn09Voi4QEIQChjI0VQgih1Wr1y/9uK5VKIYQQGo2m1rZarRYqlarWtsVyqiUP5sScmBNzYk7MiTkxJ+bEnBpiTsXFxQKAKCkpEbdi1jVVKpUK9vb2+PnnnzF69Ghp+cSJE1FcXIzt27fXuW5VVRWuXr2Kdu3aYf78+fjll19w5swZoz4VFRUoLS2Fp6cnxo8fj/LycuzcudOoT0lJCVQqFVq3bo3g4GD0798fUVFR2L9/Px599FEUFRUZjVb5+Phg9uzZePXVV2+ZH6+pIiIiIiIi4C5eU2VjY4PAwEBp6A3QnxsZExODkJCQm65rZ2cHLy8vaDQabN68GaNGjarRx8HBAZ6enigqKsKePXtq7ePi4oLWrVsjNTUVx48fl/oEBgbC2traKLaUlBRkZmbeMjYiIiIiIqLb1cLcFSIiIjBx4kT0798fAwYMwKeffoqKigq88MILAIAJEybAy8sLy5YtAwAcO3YM2dnZ6Nu3L7Kzs7F48WLodDrMmzdP2uaePXsghED37t1x4cIFzJ07Fz169JC2CQA//fQTWrdujQ4dOuD06dN45ZVXMHr0aAwZMgSAvtiaNGkSIiIi4ObmBmdnZ8ycORMhISEmzfxHRERERER0O8wuqsaPH4+CggK8/fbbyM3NRd++fbF7925p8orMzEzI5f8MgFVVVWHhwoVIT0+Ho6MjRowYgejoaKNT9EpKSrBgwQJkZWXBzc0NY8eOxXvvvQdra2upT05ODiIiIpCXlwdPT09MmDABb731llFsn3zyCeRyOcaOHQulUomhQ4fiyy+/NDdFIiIiIiIik5l9n6qmjNdUERERERERcBevqSIiIiIiIiJjLKqIiIiIiIjqgUUVERERERFRPbCoIiIiIiIiqgcWVURERERERPXAooqIiIiIiKgeWFQRERERERHVA4sqIiIiIiKiemBRRUREREREVA8sqoiIiIiIiOqBRRUREREREVE9sKgiIiIiIiKqBxZVRERERERE9cCiioiIiIiIqB5YVBEREREREdUDiyoiIiIiIqJ6YFFFRERERERUDyyqiIiIiIiI6oFFFRERERERUT2wqCIiIiIiIqoHFlVERERERET1wKKKiIiIiIioHlhUERERERER1QOLKiIiIiIionq4raIqKioKvr6+sLOzQ3BwMOLi4ursq1arsWTJEnTu3Bl2dnbw9/fH7t27jfqUlZVh9uzZ8PHxgUKhwMCBAxEfH2/Up7y8HDNmzED79u2hUCjQs2dPfP3110Z9HnroIchkMqPH1KlTbydFIiIiIiIik5hdVP3444+IiIjAokWLkJiYCH9/fwwdOhT5+fm19l+4cCFWrlyJL774AmfPnsXUqVMxZswYnDhxQuozefJk7Nu3D9HR0Th9+jSGDBmC0NBQZGdnS30iIiKwe/durF+/HsnJyZg9ezZmzJiBHTt2GO1vypQpyMnJkR4ffPCBuSkSERERERGZzOyiKjIyElOmTMELL7wgjRbZ29tjzZo1tfaPjo7GG2+8gREjRqBTp054+eWXMWLECHz88ccAgMrKSmzevBkffPABHnzwQXTp0gWLFy9Gly5d8NVXX0nbOXr0KCZOnIiHHnoIvr6+ePHFF+Hv719jlMze3h5t27aVHs7OznXmolQqUVpaavQA9KNrAKDRaKDRaKRltbVVKhW0Wm2tbZ1OJ+2nrrYQAkKIGm0DlUoFANDpdNJynU4nLddqtbW2NRqNUR4NJafqeTAn5sScmBNzYk7MiTkxJ+bUUHMylVlFlUqlQkJCAkJDQ//ZgFyO0NBQxMbG1rqOUqmEnZ2d0TKFQoHDhw8D0B8grVZ70z4AMHDgQOzYsQPZ2dkQQuDAgQM4f/48hgwZYrTehg0b0KpVK9x3331YsGABrl+/Xmc+y5Ytg4uLi/Tw9vYGAOzbtw8AEBMTg5iYGADArl27pHi2bdsmnZ64adMmnDx5EoC+gDx37hwAYPXq1UhPTwegP13SMOoWGRmJwsJCAMDy5ctRVlYGlUqF5cuXQ6VSoaysDMuXL5diXL9+PQAgOzsbUVFRAID09HSsXr0aAHDu3DlER0cDAE6ePIlNmzYBAOLj47Ft2zYAwOHDh7Fr164GkVNhYSEiIyOZE3NiTsyJOTEn5sScmBNzavA5mUyYITs7WwAQR48eNVo+d+5cMWDAgFrXeeaZZ0TPnj3F+fPnhVarFXv37hUKhULY2NhIfUJCQsTgwYNFdna20Gg0Ijo6WsjlctGtWzepT1VVlZgwYYIAIFq0aCFsbGzEunXrjPa1cuVKsXv3bnHq1Cmxfv164eXlJcaMGVNnPlVVVaKkpER6XL58WQAQhYWFQggh1Gq1UKvVQgghVCpVrW2lUik0Gk2tba1WK+2nrrZOpxM6na5GWyQkCAEIZWysEEIIrVarX/53W6lUCiGE0Gg0tbbVarVQqVS1ti2WUy15MCfmxJyYE3NiTsyJOTEn5tQQcyouLhYARElJibgVmRCmj2tduXIFXl5eOHr0KEJCQqTl8+bNw6FDh3Ds2LEa6xQUFGDKlCn43//+B5lMhs6dOyM0NBRr1qxBZWUlACAtLQ3//ve/8fvvv8PKygoBAQHo1q0bEhISkJycDAD46KOPsGrVKnz00Ufw8fHB77//jgULFmDr1q1GI2c32r9/Px599FFcuHABnTt3vmV+paWlcHFxQUlJyU1PG7wnEhOBwEAgIQEICLBsLEREREREzYw5tYFZp/+1atUKVlZWyMvLM1qel5eHtm3b1rpO69atsW3bNlRUVODSpUs4d+4cHB0d0alTJ6lP586dcejQIZSXl+Py5cuIi4uDWq2W+lRWVuKNN95AZGQkRo4ciT59+mDGjBkYP348PvroozrjDQ4OBgBcuHDBnDSJiKgZyyvPw/ANw+H+gTuGbxiOvPK8W69ERETNmllFlY2NDQIDA6XzGQH9BWcxMTFGI1e1sbOzg5eXFzQaDTZv3oxRo0bV6OPg4ABPT08UFRVhz549Uh+1Wg21Wg253DhcKysr6eKy2iQlJQEAPD09TU2RiIiaufDt4diXtg/XKq9hX9o+hG8Pt3RIRETUwLUwd4WIiAhMnDgR/fv3x4ABA/Dpp5+ioqICL7zwAgBgwoQJ8PLywrJlywAAx44dQ3Z2Nvr27Yvs7GwsXrwYOp0O8+bNk7a5Z88eCCHQvXt3XLhwAXPnzkWPHj2kbTo7O2Pw4MGYO3cuFAoFfHx8cOjQIXz//ffSRXBpaWnYuHEjRowYAXd3d5w6dQqvvvoqHnzwQfTp06feB4qIiJqHuOw4aIV+xiit0CI+O/4WaxARUXNndlE1fvx4FBQU4O2330Zubi769u2L3bt3w8PDAwCQmZlpNKJUVVWFhQsXIj09HY6OjhgxYgSio6Ph6uoq9SkpKcGCBQuQlZUFNzc3jB07Fu+99x6sra2lPj/88AMWLFiAsLAwXLt2DT4+Pnjvvfekm/va2Njgt99+k4o8b29vjB07FgsXLrzdY0NERM3QAK8B2HthL3TQQQ45gryCLB0SERE1cGZNVNHUcaIKIiLKK8/DmB/HIDYrFiHtQ7B1/FZ4OHpYOiwiIrrH7tpEFURERE2dh6MHVoxYAQBYMWIFCyoiIrolFlVERERERET1wKKKiIiompZ2LRHWOwwt7VpaOhQiImoEzJ6ogoiIqKnr2LIj1j+53tJhEBFRI8GRKiIiomqqNFW4cO0CqjRVlg6FiIgaARZVRERE1ZwtOIuuX3TF2YKzlg6FiIgaARZVRERERERE9cCiioiIiIiIqB5YVBEREREREdUDiyoiIiIiIqJ64JTqRERE1QR4BkAsEpYOg4iIGgmOVBEREREREdUDiyoiIqJqUgpTEPJtCFIKUywdChERNQIsqoiIiKqpUFfgz6w/UaGusHQoRETUCLCoIiIiIiIiqgcWVURERERERPXAooqIiIiIiKgeWFQRERFV4+vqi+gx0fB19bV0KERE1AjwPlVERETVuCnc8Fyf5ywdBhERNRIcqSIiIqqmoKIAUXFRKKgosHQoRETUCLCoIiIiquZy6WXM2DUDl0svWzoUIiJqBFhUERERERER1QOLKiIiIiIionq4raIqKioKvr6+sLOzQ3BwMOLi4ursq1arsWTJEnTu3Bl2dnbw9/fH7t27jfqUlZVh9uzZ8PHxgUKhwMCBAxEfH2/Up7y8HDNmzED79u2hUCjQs2dPfP3110Z9qqqqMH36dLi7u8PR0RFjx45FXl7e7aRIRERERERkErOLqh9//BERERFYtGgREhMT4e/vj6FDhyI/P7/W/gsXLsTKlSvxxRdf4OzZs5g6dSrGjBmDEydOSH0mT56Mffv2ITo6GqdPn8aQIUMQGhqK7OxsqU9ERAR2796N9evXIzk5GbNnz8aMGTOwY8cOqc+rr76K//3vf/jpp59w6NAhXLlyBU8++aS5KRIRUTPnZOOEIZ2HwMnGydKhEBFRIyATQghzVggODkZQUBBWrFgBANDpdPD29sbMmTMxf/78Gv3btWuHN998E9OnT5eWjR07FgqFAuvXr0dlZSWcnJywfft2/Otf/5L6BAYGYvjw4Xj33XcBAPfddx/Gjx+Pt956q9Y+JSUlaN26NTZu3IinnnoKAHDu3Dn4+fkhNjYW999//y1zKy0thYuLC0pKSuDs7GzOYbnzEhOBwEAgIQEICLBsLEREREREzYw5tYFZI1UqlQoJCQkIDQ39ZwNyOUJDQxEbG1vrOkqlEnZ2dkbLFAoFDh8+DADQaDTQarU37QMAAwcOxI4dO5CdnQ0hBA4cOIDz589jyJAhAICEhASo1Wqj2Hr06IEOHTrcNLbS0lKjB6A/ZdEQm0ajkZbV1lapVNBqtbW2dTqdtJ+62kIICCFqtG885oC+eDUs1+l00nKtVltrW6PRGOXRUHKqngdzYk7MiTk1xJxUahVKlaWorKpsMjk1xc+JOTEn5sSc7nZOpjKrqCosLIRWq4WHh4fRcg8PD+Tm5ta6ztChQxEZGYnU1FTodDrs27cPW7ZsQU5ODgDAyckJISEhWLp0Ka5cuQKtVov169cjNjZW6gMAX3zxBXr27In27dvDxsYGw4YNQ1RUFB588EEAQG5uLmxsbODq6mpybMuWLYOLi4v08Pb2BgDs27cPABATE4OYmBgAwK5du6Qib9u2bdI1X5s2bcLJkycBANHR0Th37hwAYPXq1UhPTwegvwbNcCpjZGQkCgsLAQDLly9HWVkZVCoVli9fDpVKhbKyMixfvlyKcf369QCA7OxsREVFAQDS09OxevVqAPrRuOjoaADAyZMnsWnTJgBAfHw8tm3bBgA4fPgwdu3a1SByKiwsRGRkJHNiTsyJOTXonLYf2w6X5S54Z+U7TSanpvg5MSfmxJyY093OyWTCDNnZ2QKAOHr0qNHyuXPnigEDBtS6Tn5+vhg1apSQy+XCyspKdOvWTUybNk3Y2dlJfS5cuCAefPBBAUBYWVmJoKAgERYWJnr06CH1+fDDD0W3bt3Ejh07xMmTJ8UXX3whHB0dxb59+4QQQmzYsEHY2NjU2H9QUJCYN29erbFVVVWJkpIS6XH58mUBQBQWFgohhFCr1UKtVgshhFCpVLW2lUql0Gg0tba1Wq20n7raOp1O6HS6Gm2RkCAEIJSxsUIIIbRarX75322lUimEEEKj0dTaVqvVQqVS1dq2WE615MGcmBNzYk4NMae4y3ECiyH+vPRnk8mpKX5OzIk5MSfmdDdzKi4uFgBESUmJuBWzrqlSqVSwt7fHzz//jNGjR0vLJ06ciOLiYmzfvr3OdauqqnD16lW0a9cO8+fPxy+//IIzZ84Y9amoqEBpaSk8PT0xfvx4lJeXY+fOnaisrISLiwu2bt1qdN3V5MmTkZWVhd27d2P//v149NFHUVRUZDRa5ePjg9mzZ+PVV1+9ZX68poqIiAAgMScRgd8EIuHFBAR48ncwEVFzdNeuqbKxsUFgYKA09Aboz42MiYlBSEjITde1s7ODl5cXNBoNNm/ejFGjRtXo4+DgAE9PTxQVFWHPnj1SH7VaDbVaDbncOFwrKyvpPMjAwEBYW1sbxZaSkoLMzMxbxkZERERERHS7Wpi7QkREBCZOnIj+/ftjwIAB+PTTT1FRUYEXXngBADBhwgR4eXlh2bJlAIBjx44hOzsbffv2RXZ2NhYvXgydTod58+ZJ29yzZw+EEOjevTsuXLiAuXPnokePHtI2nZ2dMXjwYMydOxcKhQI+Pj44dOgQvv/+e+l8TRcXF0yaNAkRERFwc3ODs7MzZs6ciZCQEJNm/iMiIiIiIsvJywPCw4G4OGDAAGDtWqDaVA4NltlF1fjx41FQUIC3334bubm56Nu3L3bv3i1NXpGZmWk0olRVVYWFCxciPT0djo6OGDFiBKKjo41O0SspKcGCBQuQlZUFNzc3jB07Fu+99x6sra2lPj/88AMWLFiAsLAwXLt2DT4+PnjvvfcwdepUqc8nn3wCuVyOsWPHQqlUYujQofjyyy9v57gQEVEz1rtNb+TPyYernaulQyEiajbCw4F9+wCtVv8cHg78PU9Fg2f2faqaMl5TRURERERkGe7uwLVrxq//npzPIu7aNVVERETNQdq1NDzx3yeQdi3N0qEQETUbAwYAhhPe5HIgKMiy8ZiDRRUREVE1JcoS/O/8/1CiLLF0KEREzcbatUBwsL4dHKx/3ViwqCIiIiIiIovz8ABWrNC3V6xoPJNUACyqiIiIiIiI6oVFFRERERERUT2wqCIiIqrGy8kLHw/5GF5OXpYOhYiIGgGz71NFRETU1Hk4eiAiJMLSYRARUSPBkSoiIqJqiiqL8NOZn1BUWWTpUIiIqBFgUUVERFRNRnEGxv08DhnFGZYOhYioWenQAVi1Sv/cmPD0PyIiIiIiahBatQImT7Z0FObjSBURERERETUIhYXA6tX658aERRURERERETUImZnAlCn658aEp/8RERFVo2ihQL+2/aBoobB0KEREzUpAACCEpaMwH4sqIiKiavxa+yHxpURLh0FERI0ET/8jIiIiIqIGISUFCAnRPzcmLKqIiIiqOZFzArbv2uJEzglLh0JE1KxUVAB//ql/bkxYVBEREVUjIKDSqiDQCE/sJyKie45FFRERERERUT2wqCIiIiIiIqoHzv7X0Fy+DBQUAMnJ+teG5zZtgPbtLRcXERERERHVikVVQ6JUAkFBQF7eP8uee07/3LYtcPEiYGtrkdCIiJoTv1Z++Ovlv9CpZSdLh0JERI0AT/9rSGxsgA4dAHm1j0UuB7y99e8TEdFdp7BWoFebXlBY8+a/RER0ayyqGhKZDFi6FNDpjJfrdPrlMpll4iIiamYuFV/C5B2Tcan4kqVDISKiRuC2iqqoqCj4+vrCzs4OwcHBiIuLq7OvWq3GkiVL0LlzZ9jZ2cHf3x+7d+826lNWVobZs2fDx8cHCoUCAwcORHx8vFEfmUxW6+PDDz+U+vj6+tZ4f/ny5beTouUMGaI/BdAwWiWX618PGWLZuIiImpGrlVfx7YlvcbXyqqVDISJqVu67Tz/FwH33WToS85hdVP3444+IiIjAokWLkJiYCH9/fwwdOhT5+fm19l+4cCFWrlyJL774AmfPnsXUqVMxZswYnDjxzw0VJ0+ejH379iE6OhqnT5/GkCFDEBoaiuzsbKlPTk6O0WPNmjWQyWQYO3as0f6WLFli1G/mzJnmpmhZ1UerOEpFRERERM2EjY1+brbGdtWL2UVVZGQkpkyZghdeeAE9e/bE119/DXt7e6xZs6bW/tHR0XjjjTcwYsQIdOrUCS+//DJGjBiBjz/+GABQWVmJzZs344MPPsCDDz6ILl26YPHixejSpQu++uoraTtt27Y1emzfvh0PP/wwOnUyvojYycnJqJ+Dg4O5KVqeYbQK4CgVERERETUb6enA00/rnxsTs4oqlUqFhIQEhIaG/rMBuRyhoaGIjY2tdR2lUgk7OzujZQqFAocPHwYAaDQaaLXam/apLi8vDzt37sSkSZNqvLd8+XK4u7ujX79++PDDD6HRaOrMR6lUorS01OgB6E9ZNMRmWF+tVtfaVqlU0Gq1tbZ1f482KZXKOttCCAghjNsqFfD++xB+flAtXgzIZNDpdFAqlQAAnU4HlUoFANBqtbW2NRqNUR4Wz+mG2OtqMyfmxJyYU0PLSa1WN7mcmuLnxJyYE3NqOjkplRqUlOig1TaMnExlVlFVWFgIrVYLDw8Po+UeHh7Izc2tdZ2hQ4ciMjISqamp0Ol02LdvH7Zs2YKcnBwA+pGlkJAQLF26FFeuXIFWq8X69esRGxsr9alu3bp1cHJywpNPPmm0fNasWfjhhx9w4MABvPTSS3j//fcxb968OvNZtmwZXFxcpIe3tzcAYN++fQCAmJgYxMTEAAB27dolFXnbtm2TrvnatGkTTp48CUA/Knfu3DkAwOrVq5H+d4kdFRUlncoYGRmJwsJCAPoCsKysDCqVCsuXL4dKpUJZWZn+OrDQUBQcPIiP/952dnY2oqKiAADp6elYvXo1AODcuXOIjo4GAJw8eRKbNm0CAMTHx2Pbtm0AgMOHD2PXrl2Wzwn671BkZCRzYk7MiTk16JzKcsswf9B8xGyPaTI5NcXPiTkxJ+bU9HIqLo7HlCmb0bVrw8jJZMIM2dnZAoA4evSo0fK5c+eKAQMG1LpOfn6+GDVqlJDL5cLKykp069ZNTJs2TdjZ2Ul9Lly4IB588EEBQFhZWYmgoCARFhYmevToUes2u3fvLmbMmHHLeL/99lvRokULUVVVVev7VVVVoqSkRHpcvnxZABCFhYVCCCHUarVQq9VCCCFUKlWtbaVSKTQaTa1trVYr7aeutk6nEzqdrkZbCCG0Wm2dbaVSKYQQQqPR1NpWq9VCpVLV2mZOzIk5MSfmxJyYE3NiTsypIeZUVaUWhYUqodFYPqfi4mIBQJSUlIhbkQlh+riWSqWCvb09fv75Z4wePVpaPnHiRBQXF2P79u11rltVVYWrV6+iXbt2mD9/Pn755RecOXPGqE9FRQVKS0vh6emJ8ePHo7y8HDt37jTq88cff+DBBx9EUlIS/P39bxrvmTNncN999+HcuXPo3r37LfMrLS2Fi4sLSkpK4OzsfMv+RETUNJUpy5CQk4BAz0A42TpZOhwiomYjMREIDAQSEoCAAMvGYk5tYNbpfzY2NggMDJSG3gD9uZExMTEICQm56bp2dnbw8vKCRqPB5s2bMWrUqBp9HBwc4OnpiaKiIuzZs6fWPt9++y0CAwNvWVABQFJSEuRyOdq0aWNCdkRERHqp11Lx8LqHkXot1dKhEBFRI9DC3BUiIiIwceJE9O/fHwMGDMCnn36KiooKvPDCCwCACRMmwMvLC8uWLQMAHDt2DNnZ2ejbty+ys7OxePFi6HQ6o2ud9uzZAyEEunfvjgsXLmDu3Lno0aOHtE2D0tJS/PTTT9LMgTeKjY3FsWPH8PDDD8PJyQmxsbF49dVX8dxzz6Fly5bmpklERERERGQSs4uq8ePHo6CgAG+//TZyc3PRt29f7N69W5q8IjMzE3L5PwNgVVVVWLhwIdLT0+Ho6IgRI0YgOjoarq6uUp+SkhIsWLAAWVlZcHNzw9ixY/Hee+/B2traaN8//PADhBB45plnasRla2uLH374AYsXL4ZSqUTHjh3x6quvIiIiwtwUiYiIiIiITGbWNVVNHa+pIiIiAEjMSUTgN4FIeDEBAZ4WPqmfiKgZaRbXVBERETUH1nJreDl5wVpufevORETU7Jl9+h8REVFT19ujN7IisiwdBhERNRIcqSIiIiIiIqoHFlVERETVnM47jfaR7XE677SlQyEialbs7ICePfXPjQlP/yMiIqpGrVMjuywbap3a0qEQETUrPXsCZ85YOgrzsai6DVqtFmo1/6El01hbW8PKysrSYRARERHRXcKiygxCCOTm5qK4uNjSoVAj4+rqirZt20Imk1k6FCIiIqIGKykJePBB4Pffgb59LR2N6VhUmcFQULVp0wb29vb8DzLdkhAC169fR35+PgDA09PTwhERERERNVyensDixfrnxoRFlYm0Wq1UULm7u1s6HGpEFAoFACA/Px9t2rThqYBEjUBXt644MPEAurp1tXQoRETNiocHEBFh6SjMx9n/TGS4hsre3t7CkVBjZPje8Fo8osbBydYJD/k+BCdbJ0uHQkTUrBQVAT/9pH9uTFhUmYmn/NHt4PeGqHHJLs3Ggt8WILs029KhEBE1KxkZwLhx+ufGhEUVERFRNXkVeVh+ZDnyKvIsHQoRETUCLKqIiIiIiIjqgUUVmcXX1xeffvqppcMgIiIiImowOPtfM/DQQw+hb9++d6QYio+Ph4ODQ/2DIiIiIiJqIlhUEYQQ0Gq1aNHi1l+H1q1b34OIiIgsy13hjkn9JsFdwVtoEBHRrfH0vyYuPDwchw4dwmeffQaZTAaZTIa1a9dCJpNh165dCAwMhK2tLQ4fPoy0tDSMGjUKHh4ecHR0RFBQEH777Tej7VU//U8mk2H16tUYM2YM7O3t0bVrV+zYseMeZ0lEdGf5uPpg9ROr4ePqY+lQiIioEeBIVX1dvw6cO3fv99ujB2DCPbM+++wznD9/Hvfddx+WLFkCADhz5gwAYP78+fjoo4/QqVMntGzZEpcvX8aIESPw3nvvwdbWFt9//z1GjhyJlJQUdOjQoc59vPPOO/jggw/w4Ycf4osvvkBYWBguXboENze3O5MrEdE9VqmuRHpROjq17ASFtcLS4RARUQPHoqq+zp0DAgPv/X4TEoCAgFt2c3FxgY2NDezt7dG2bVsAwLm/i8AlS5bgsccek/q6ubnB399fer106VJs3boVO3bswIwZM+rcR3h4OJ555hkAwPvvv4/PP/8ccXFxGDZs2G2lRkRkacmFyQj8JhAJLyYgwPPWv2uJiOjOaNMGePVV/XNjwqKqvnr00Bc4lthvPfXv39/odXl5ORYvXoydO3ciJycHGo0GlZWVyMzMvOl2+vTpI7UdHBzg7OyM/Pz8esdHRERERM1L+/ZAZKSlozAfi6r6src3acSoIao+i9+cOXOwb98+fPTRR+jSpQsUCgWeeuopqFSqm27H2tra6LVMJoNOp7vj8RIRERFR01ZeDpw+DfTuDTg6Wjoa07GoagZsbGyg1Wpv2e/IkSMIDw/HmDFjAOhHri5evHiXoyMiIiIi0jt/Hhg40OQrXRoMzv7XDPj6+uLYsWO4ePEiCgsL6xxF6tq1K7Zs2YKkpCScPHkSzz77LEeciKhZkkEGGysbyCCzdChERM2Knx/w11/658aERVUzMGfOHFhZWaFnz55o3bp1nddIRUZGomXLlhg4cCBGjhyJoUOHIqAx/YmAiOgO6efZD8qFSvTz7GfpUIiImhWFAujVS//cmNxWURUVFQVfX1/Y2dkhODgYcXFxdfZVq9VYsmQJOnfuDDs7O/j7+2P37t1GfcrKyjB79mz4+PhAoVBg4MCBiI+PN+pjuMdS9ceHH34o9bl27RrCwsLg7OwMV1dXTJo0CeXl5beTYpPSrVs3xMbG4vr16xBCIDw8HEIIuLq6GvXz9fXF/v37cf36dWRmZmL69Ok4ePCg0X2pLl68iNmzZ0uvhRAYPXq00XaKi4sRHh5+1/IhIiIioqbp0iVg8mT9c2NidlH1448/IiIiAosWLUJiYiL8/f0xdOjQOmd7W7hwIVauXIkvvvgCZ8+exdSpUzFmzBicOHFC6jN58mTs27cP0dHROH36NIYMGYLQ0FBkZ2dLfXJycowea9asgUwmw9ixY6U+YWFhOHPmDPbt24dffvkFv//+O1588UVzUyQiomYuuSAZASsDkFyQbOlQiIialatXgW+/1T83JjIhhDBnheDgYAQFBWHFihUAAJ1OB29vb8ycORPz58+v0b9du3Z48803MX36dGnZ2LFjoVAosH79elRWVsLJyQnbt2/Hv/71L6lPYGAghg8fjnfffbfWOEaPHo2ysjLExMQAAJKTk9GzZ0/Ex8dLU4Xv3r0bI0aMQFZWFtq1a3fL3EpLS+Hi4oKSkhI4OzsbvVdVVYWMjAx07NgRdnZ2t9wW0Y34/SFqXBJzEnmfKiIiC0hM1N8CtiFMVHGz2qA6s0aqVCoVEhISEBoa+s8G5HKEhoYiNja21nWUSmWN/0QqFAocPnwYAKDRaKDVam/ap7q8vDzs3LkTkyZNkpbFxsbC1dXV6N5LoaGhkMvlOHbsWJ2xlZaWGj0A/SmLhtg0Go20zFB/6nS6O9oWQtRoA7hr7buVB3MyLUatVitNU6/RaIy+b3V992prq1QqaVbH6m3DfpVKZZ1tQ0zV24ac62obYr8xD+bEnJpqTmq1usnl1BQ/J+bEnJhT08rJoCHkZCqziqrCwkJotVp4eHgYLffw8EBubm6t6wwdOhSRkZFITU2FTqfDvn37sGXLFuTk5AAAnJycEBISgqVLl+LKlSvQarVYv349YmNjpT7VrVu3Dk5OTnjyySelZbm5uWhT7dbLLVq0gJubW52xLVu2DC4uLtLD29sbALBv3z4AQExMjDQS9scff0hfiOLiYlRUVAAAioqKcP36dQDA1atXUVVVJR0rQ//8/HzpC5GXlyd98Lm5udJ/0nNzc6X/hBvi1Wg0yMvLA6D/whhOsVQqlSgsLASgHwG5+vf46PXr11FUVAQAqKioQHFxMQD91OiGgvHG4rG0tFS65ow53d2clEqlFMvJkyexadMmAEB8fDy2bdsGADh8+DB27doFwPi7t2vXLukPDNu2bZOuN9y0aRNOnjwJAIiOjsa5c+cAAKtXr0Z6ejoA/fWPhtNoIyMjpXiWL1+OsrIyqFQqLF++HCqVCmVlZVi+fLl0XCL/vvNednY2oqKiAADp6elYvXo1AODcuXOIjo5mTsypSeZkuJ3E1q1bm0xOTfFzYk7MiTk1vZzOnj0Lg4aQk8mEGbKzswUAcfToUaPlc+fOFQMGDKh1nfz8fDFq1Cghl8uFlZWV6Natm5g2bZqws7OT+ly4cEE8+OCDAoCwsrISQUFBIiwsTPTo0aPWbXbv3l3MmDHDaNl7770nunXrVqNv69atxZdfflnrdqqqqkRJSYn0uHz5sgAgCgsLhRBCqNVqoVarhRBClJaWijNnzojKykqh1WqFTqcTQog70tbpdDXaQoi71r6TsTMn02K8fv269P3RaDRCqVQKIfTfMZVKVWvb8N1TqVS1tpVKpdBoNLW2Dfutqqqqs23Io3rbkHNdbUPsN+bBnJhTU8sp7nKcwGKIPy/92WRyaoqfE3NiTsyp6eV07JhaAEIkJFg+p+LiYgFAlJSUiFsx65oqlUoFe3t7/Pzzz0Yzvk2cOBHFxcXYvn17nesaRh/atWuH+fPn45dffsGZM2eM+lRUVKC0tBSenp4YP348ysvLsXPnTqM+f/zxBx588EEkJSXB399fWr5mzRq89tpr0qgGoB9BsLOzw08//STd0PZmeE0V3S38/hA1LkWVRfgt/TeEdgpFS0VLS4dDRNRsNItrqmxsbBAYGCgNvQH6cyNjYmIQEhJy03Xt7Ozg5eUFjUaDzZs3Y9SoUTX6ODg4wNPTE0VFRdizZ0+tfb799lsEBgYaFVQAEBISguLiYiQkJEjL9u/fD51Oh+DgYHPSJCKiZq6loiWe7vU0CyoiIjKJ2VOqR0REYNWqVVi3bh2Sk5Px8ssvo6KiAi+88AIAYMKECViwYIHU/9ixY9iyZQvS09Pxxx9/YNiwYdDpdJg3b57UZ8+ePdi9ezcyMjKwb98+PPzww+jRo4e0TYPS0lL89NNPmDx5co24/Pz8MGzYMEyZMgVxcXE4cuQIZsyYgf/7v/8zaeY/IiIig7zyPETGRiKvPM/SoRARUSPQwtwVxo8fj4KCArz99tvIzc1F3759sXv3bmnyiszMTMjl/9RqVVVVWLhwIdLT0+Ho6IgRI0YgOjra6MazJSUlWLBgAbKysuDm5oaxY8fivffeg7W1tdG+f/jhBwgh8Mwzz9Qa24YNGzBjxgw8+uijkMvlGDt2LD7//HNzUyQiomYuuywbr+19DQ/5PgQPR49br0BERHdEly7A7t3658bE7PtUNWW8pqp2vr6+mD17NmbPng0AkMlk2Lp1q9F1dTe6ePEiOnbsiBMnTqBv3763vd87tZ2GoDl/f4gaI96nioiIzLmmyuyRKqKcnBy0bHlnrzMIDw9HcXGxNC0oAHh7eyMnJwetWrW6o/siIiIiooYpJwdYuRJ46SXA09PS0ZjO7GuqiNq2bQtbW9u7vh8rKyu0bdsWLVqw9iciIiJqDgoLgdWr9c+NCYuqJu6bb75Bu3btpLtFG4waNQr//ve/kZaWhlGjRsHDwwOOjo4ICgrCb7/9dtNtymQyoxGluLg49OvXD3Z2dujfvz9OnDhh1F+r1WLSpEno2LEjFAoFunfvjs8++0x6f/HixVi3bh22b98OmUwGmUyGgwcP4uLFi5DJZEhKSpL6Hjp0CAMGDICtrS08PT0xf/58oztvP/TQQ5g1axbmzZsHNzc3tG3bFosXLzb/wBFRs+Zi64KR3UbCxdbF0qEQETUrvXsDWVn658aEQwB3QE5ZDnLKc4yWtbRriY4tO6JKU4WzBWdrrGM4Rz+lMAUV6gqj93xdfeGmcENBRQEul142es/Jxgld3buaHNvTTz+NmTNn4sCBA3j00UcBANeuXcPu3bvx66+/ory8HCNGjMB7770HW1tbfP/99xg5ciRSUlLQoUOHW26/vLwcjz/+OB577DGsX78eGRkZeOWVV4z66HQ6tG/fHj/99BPc3d1x9OhRvPjii/D09MS4ceMwZ84cJCcno7S0FN999x0AwM3NDVeuXDHaTnZ2NkaMGIHw8HB8//33OHfuHKZMmQI7OzujwmndunWIiIjAsWPHEBsbi/DwcAwaNAiPPfaYyceNiJq3zm6dseOZHZYOg4iIGgkWVXfAyoSVeOfQO0bLwnqHYf2T65FVmoXAbwJrrCMW6ecHCd8ejj+z/jR6L3pMNJ7r8xw2ndmEGbtmGL03pPMQ7Hluj8mxtWzZEsOHD8fGjRulournn39Gq1at8PDDD0Mulxvd82vp0qXYunUrduzYgRkzZtS1WcnGjRuh0+nw7bffws7ODr169UJWVhZefvllqY+1tTXeeeef49OxY0fExsZi06ZNGDduHBwdHaFQKKBUKtG2bds69/Xll1/C29sbK1asgEwmQ48ePXDlyhW8/vrrePvtt6VZJ/v06YNFixYBALp27YoVK1YgJiaGRRURmUytVaO4qhiudq6wtrK+9QpERHRHnD4NDB8O7NrVuEarWFTdAS8FvoQnuj9htKylnX4ih/bO7ZHwYkJtqwEA1o5aW+tIFQCM6zUOId7GN1V2snEyO76wsDBMmTIFX375JWxtbbFhwwb83//9H+RyOcrLy7F48WLs3LkTOTk50Gg0qKysRGZmpknbTk5ORp8+fYxmtKvtRtBRUVFYs2YNMjMzUVlZCZVKZfaMfsnJyQgJCYFMJpOWDRo0COXl5cjKypJG1vr06WO0nqenJ/Lz883aFxE1b6fzT3P2PyIiC1Crgexs/XNjwqLqDvB08oSnU+3Tk9i1sLvpP8jdW3Wv873WDq3R2qF1veMbOXIkhBDYuXMngoKC8Mcff+CTTz4BAMyZMwf79u3DRx99hC5dukChUOCpp56CSqWq934NfvjhB8yZMwcff/wxQkJC4OTkhA8//BDHjh27Y/u4UfX7m8lkshrXlBERERER3SksqpoBOzs7PPnkk9iwYQMuXLiA7t27IyBAX+gdOXIE4eHhGDNmDAD9NVIXL140edt+fn6Ijo5GVVWVNFr155/GpzMeOXIEAwcOxLRp06RlaWlpRn1sbGyg1Wpvua/NmzdDCCGNVh05cgROTk5o3769yTETEREREd1JnP2vmQgLC8POnTuxZs0ahIWFScu7du2KLVu2ICkpCSdPnsSzzz5r1qjOs88+C5lMhilTpuDs2bP49ddf8dFHHxn16dq1K44fP449e/bg/PnzeOuttxAfH2/Ux9fXF6dOnUJKSgoKCwuhrmXMd9q0abh8+TJmzpyJc+fOYfv27Vi0aBEiIiKk66mIiIiIiO41/k+0mXjkkUfg5uaGlJQUPPvss9LyyMhItGzZEgMHDsTIkSMxdOhQaRTLFI6Ojvjf//6H06dPo1+/fnjzzTfxn//8x6jPSy+9hCeffBLjx49HcHAwrl69ajRqBQBTpkxB9+7d0b9/f7Ru3RpHjhypsS8vLy/8+uuviIuLg7+/P6ZOnYpJkyZh4cKFZh4NIiIiIqI7RyaEEJYOoqEoLS2Fi4sLSkpK4OzsbPReVVUVMjIy0LFjR6NJGYhMwe8PUeOi1WlRoa6Ag7UDrORWlg6HiKjZSEwEAgOBhATAjL/z3xU3qw2q4zVVRERE1VjJreBse/N/QImIiAx4+h8REVE1qVdTMXT9UKReTbV0KERE1AhwpIqIiKiaMlUZ9qbtRZmqzNKhEBE1KwEBQGO8OIkjVURERERERPXAooqIiIiIiBqElBQgJET/3Jjw9D8iIiIiIrKoy5eBggIgJwfw8AAuXAAqKoA2bYD27S0d3a2xqCIiIqrG29kbK4avgLezt6VDISJq8pRKICgIyMv7Z9n27frntm2BixcBW1uLhGYyFlVERETVtHZojekDpls6DCKiZsHGBujQQT9SpdP9s1wuB7y99e83dLymioiIqJprldew/tR6XKu8ZulQiIiaPJkMWLrUuKAC9K+XLtW/39CxqCIiIqrmYvFFPL/1eVwsvmjpUIiImoUhQ/SnAMr/rk7kcv3rIUMsG5epWFQ1Aw899BBmz559x7YXHh6O0aNH37HtEREREVHzVn20qjGNUgEsqoiIiIiIqAEwjFYBjWuUCrjNoioqKgq+vr6ws7NDcHAw4uLi6uyrVquxZMkSdO7cGXZ2dvD398fu3buN+pSVlWH27Nnw8fGBQqHAwIEDER8fX2NbycnJeOKJJ+Di4gIHBwcEBQUhMzNTev+hhx6CTCYzekydOvV2UmwywsPDcejQIXz22WfSMbl48SL++usvDB8+HI6OjvDw8MDzzz+PwsJCab2ff/4ZvXv3hkKhgLu7O0JDQ1FRUYHFixdj3bp12L59u7S9gwcPWi5BIiIiImoSZDLg/fcBPz/9c2MZpQJuY/a/H3/8EREREfj6668RHByMTz/9FEOHDkVKSgratGlTo//ChQuxfv16rFq1Cj169MCePXswZswYHD16FP369QMATJ48GX/99Reio6PRrl07rF+/HqGhoTh79iy8vLwAAGlpaXjggQcwadIkvPPOO3B2dsaZM2dgZ2dntL8pU6ZgyZIl0mt7e3tzUzRbTo7+cTMdOgCtWgGFhUBmJhAQoF+ekqKfg/9m7rtPP+tJejqg1QJdu5oe22effYbz58/jvvvuk46LtbU1BgwYgMmTJ+OTTz5BZWUlXn/9dYwbNw779+9HTk4OnnnmGXzwwQcYM2YMysrK8Mcff0AIgTlz5iA5ORmlpaX47rvvAABubm6mB0RE1Ag4WDvg/vb3w8HawdKhEBE1K6GhwNmzlo7iNggzDRgwQEyfPl16rdVqRbt27cSyZctq7e/p6SlWrFhhtOzJJ58UYWFhQgghrl+/LqysrMQvv/xi1CcgIEC8+eab0uvx48eL55577qaxDR48WLzyyivmpGOkpKREABAlJSU13qusrBRnz54VlZWVNd5btEgI4OaPVav0fVet0r82uP/+W697+bK+71NPCTFkiPl5VT8uS5cuFUOqbejy5csCgEhJSREJCQkCgLh48WKt25s4caIYNWqU+YE0Yzf7/hARERFRw3Oz2qA6s07/U6lUSEhIQGhoqLRMLpcjNDQUsbGxta6jVCprjCYpFAocPnwYAP5/e3ceVVW5/gH8e0DgIMNhyWDiFCrgkAYoEGVyvWkolYV0RdOirmEtNX5Xc4jCgWUFZWqGVFcpq4NGTqHlBfSSaIADaKjXBAc0DBkEgQMoZ3x/fxD7dmQQwhuC389ae/Wy97v3+zzn7LXsOe8eoNPpoNfrW+1jMBiwd+9euLm5ISAgAE5OTvD19UVSUlKT8bZs2QIHBwc88MADiIiIwI0bN1rMR61WQ6VSGS1AwyWLjbHpdDppnRBCiuf37TlzBI4fB7KzDcjJab799NMN/adMaVjfuO/mzQI5OULq39g+fhxS28kJEEIgOtqADRsa2obf7uJrS7vx78b/njx5EgcOHIC1tbW0DB06FEDDjODIkSPx2GOPYeTIkXj22WexceNGVFZWGuUthGjx82jcdmu7rfH+kXZrsdypdkdzajyeXq+HRqORzrHfn28tnXvNtTUaDfR6fbPtxnHVanWL7caYbm035txSuzH23+fBnJgTc2JOzIk5MSfm1B1zaqt2FVXl5eXQ6/Xo3bu30frevXujpKSk2X0CAgKwdu1anD9/HgaDAfv378euXbtQ/Nv1cjY2NvDz88OqVatw9epVMQoAtQAAHJ9JREFU6PV6JCQk4PDhw1KfsrIy1NbWIiYmBpMmTcK+ffsQFBSEqVOn4uDBg9JYzz33HBISEnDgwAFERERAqVRi1qxZLeYTHR0NhUIhLf379wcA7N+/HwCQlpaGtLQ0AMCPP/4onRBVVVWo++2avcrKSigUN+DlBQwcWIHhw+vh5QUMGFCOESPU8PIC+vUrg0LRcELo9aUYNarhiy8pKYGrqwGengLOziXw9BTw8DDA2bkEXl7AqFE69O1bCnPzhhPG2roMrq4NX3rj/U/19fWoqKgAANy4cQOVlZUAgLq6OlRVVf025n9PVJVKhaqqKjz11FM4dOgQMjIykJubi/T0dOTm5mLcuHFQqVRISkpCcnIy7r//fsTGxsLd3R3Hjx+XPgO1Wi2d5KWlpdLJXFJSIhUeJSUlUmHReH7odDqU/va6bK1Wi7KyMul47cmptrZWKoJ/XxCrVCrU1tY2+z01FtgVFRWor68H0HBON+ZUVlb2P8tJrVZLsZw8eRLbtm0DAGRnZ0s/DmRkZCA5ObnJuZecnCz9wJCUlCTdb7ht2zacPHkSAKBUKpGXlwcAiI+PR0FBAYCG+x+LiooAAGvXrpXiiYmJQU1NDTQaDWJiYqDRaFBTU4OYmBjpc1m7di0AoKioCHFxcQCAgoICxMfHAwDy8vKgVCqZE3PqljntOrwLsigZln+yvNvk1B2/J+bEnJgTc/pf59Rmt53L+p2ioiIBQGRlZRmtX7x4sfDx8Wl2n7KyMvH0008LExMTYWpqKtzc3MTcuXOFXC6X+ly4cEGMGzdOABCmpqbC29tbzJw5UwwdOtRo3BkzZhgd+6mnnhLTp09vMd60tDQBQFy4cKHZ7fX19aK6ulpaGi+BKy8vF0IIodVqhVarFUIIoVKpxJkzZ8TNmzeFXq8XBoNBCCHuSNtgMDRpCyHuWHvixInSJZsGg0FEREQId3d3oVar2xSjVqsVffv2FR988IEwGAwiLCxMPPHEE52a063tO/l9/C9yunHjhnT+6HQ6oVarpXNMo9E022489zQaTbNttVotdDpds+3Gcevr61tsN+Zxa7sx55bajbH/Pg/mxJy6W07HrhwTWAlx5Jcj3San7vg9MSfmxJyY0/8yp6qqqjZf/icTou3zWhqNBj179sSOHTuM3lMUGhqKqqoq7N69u8V9G2cfnJ2d8cYbb+D777/HmTNnjPrU1dVBpVKhT58+CAkJQW1tLfbu3QuNRgMrKyusWLECkZGRUv+lS5ciIyMDmZmZzY5ZV1cHa2trpKSkICAg4Lb5qVQqKBQKVFdXw9bWtkn8ly5dgouLS5NLFe92c+bMQW5uLrZt2wZra2toNBp4eHjA398fS5YsQa9evXDhwgUkJiYiPj4eOTk5SEtLw+OPPw4nJyccPXoUs2bNQlJSEiZPnox3330X//znP7Fv3z7Y29tDoVDAzMyss9O8q3Xl84foXnSi+ARGbxyN43OOw6uPV2eHQ0REnaC12uBW7br8z9zcHKNHj5am3oCGayPT0tLg5+fX6r5yuRx9+/aFTqfDzp078fTTTzfpY2VlhT59+qCyshKpqalSH3Nzc3h7eyM/P9+o/7lz5zBw4MAWx8zNzQUA9OnTp60pdkuLFi2Cqakphg8fDkdHR2g0GmRmZkKv1+Pxxx/HyJEj8Y9//AN2dnYwMTGBra0tDh06hMDAQLi5uSEyMhJr1qzB5MmTATQ8YdHd3R1jxoyBo6Nji0UtEREREdG9oN2PVF+4cCFCQ0MxZswY+Pj44MMPP0RdXR1eeuklAMALL7yAvn37Ijo6GgBw9OhRFBUVwcPDA0VFRVi5ciUMBgOWLFkiHTM1NRVCCLi7u+PChQtYvHgxhg4dKh0TABYvXoyQkBCMGzcO48ePR0pKCr777jvpHUkXL17E1q1bERgYCHt7e5w6dQoLFizAuHHjMGrUqI58Rl2em5tbsw8S2bVrV7P9hw0b1uRdYr/n6OiIffv23bH4iIiIiIi6snYXVSEhIbh27RqWL1+OkpISeHh4ICUlRXp4RWFhIUxM/jsBVl9fj8jISBQUFMDa2hqBgYFQKpWws7OT+lRXVyMiIgK//vorevXqheDgYLzzzjtGl5QFBQXh008/RXR0NMLDw+Hu7o6dO3di7NixABpms/79739LRV7//v0RHBxsdLkgERFRWwx3HI7zr51HP9t+nR0KERF1Ae26p6q76673VFHn4/lDRERE1LX8z+6pIiIiuhdcqryEWbtm4VLlpc4OhYiIugAWVURERLeorK/EltNbUFlf2dmhEBFRF8CiioiIiIiIqANYVBEREREREXUAiyoiIiIiIqIOYFFFRER0iz7WfbDCfwX6WN/bL48nIqK2afd7qoiIiLq7PjZ9sPIvKzs7DCIi6iI4U3WPu3z5MmQyGXJzcztlfJlMhqSkpA4dY+XKlfDw8Lgj8RARAYBKrULqhVSo1KrODoWIiLoAFlVERES3uHD9AiZtmYQL1y90dihERNQFsKgiIiIiIiLqABZV9wCDwYD3338fQ4YMgYWFBQYMGIB33nmn2b56vR6zZ8+Gi4sLLC0t4e7ujvXr1xv1SU9Ph4+PD6ysrGBnZ4dHHnkEv/zyCwDg5MmTGD9+PGxsbGBra4vRo0cjJyen1fiKi4sxefJkWFpaYtCgQdixY4fR9qVLl8LNzQ09e/bEoEGDsGzZMmi12haPl52djYkTJ8LBwQEKhQL+/v44ceKEUR+ZTIb4+HgEBQWhZ8+ecHV1xZ49e4z6nDlzBk8++SRsbW1hY2ODRx99FBcvXpS2x8fHY9iwYZDL5Rg6dCg+/vjjVvMkIiIiou6JD6roIK1Wi/Ly8j99XAcHB5iZmbWpb0REBDZt2oR169Zh7NixKC4uRl5eXrN9DQYD+vXrh+3bt8Pe3h5ZWVmYM2cO+vTpg2nTpkGn0+GZZ55BWFgYvv76a2g0Ghw7dgwymQwAMHPmTHh6euKTTz6BqakpcnNzbxvnsmXLEBMTg/Xr10OpVGL69Ok4ffo0hg0bBgCwsbHBF198AWdnZ5w+fRphYWGwsbHBkiVLmj1eTU0NQkNDERsbCyEE1qxZg8DAQJw/fx42NjZSv6ioKLz//vtYvXo1YmNjMXPmTPzyyy/o1asXioqKMG7cOPzlL3/BDz/8AFtbW2RmZkKn0wEAtmzZguXLl2PDhg3w9PTETz/9hLCwMFhZWSE0NLRN3wsRERERdQ8yIYTo7CDuFiqVCgqFAtXV1bC1tTXaVl9fj0uXLsHFxQVyuVxaX1xcjI0bN/7ZoUqFzu3U1NTA0dERGzZswMsvv9xk++XLl+Hi4oKffvqpxYc9zJ8/HyUlJdixYweuX78Oe3t7pKenw9/fv0lfW1tbxMbGtrmwkMlkePXVV/HJJ59I6x566CF4eXm1OPPzwQcfIDExUZoBW7lyJZKSklp82IbBYICdnR22bt2KJ598Uho3MjISq1atAgDU1dXB2toaycnJmDRpEt58800kJiYiPz+/2aJwyJAhWLVqFWbMmCGte/vtt/Gvf/0LWVlZTfq3dP4Q0d3pTNkZPJ34NHZP340RTiM6OxwiIuoErdUGt+JMVQc5ODhgzpw5nTJuW5w9exZqtRqPPfZYm48dFxeHzz//HIWFhbh58yY0Go1UcPXq1QsvvvgiAgICMHHiREyYMAHTpk2TCryFCxfi5ZdfhlKpxIQJE/C3v/0NgwcPbnU8Pz+/Jn//vkD65ptv8NFHH+HixYuora2FTqdr9cQuLS1FZGQk0tPTUVZWBr1ejxs3bqCwsNCo36hRo6S2lZUVbG1tUVZWBgDIzc3Fo48+2mxBVVdXh4sXL2L27NkICwuT1ut0OigUilZzJaKuYYTTCFwI50MqiIiobVhUdZCZmVmbZow6i6WlZbv6JyYmYtGiRVizZg38/PxgY2OD1atX4+jRo1KfzZs3Izw8HCkpKfjmm28QGRmJ/fv346GHHsLKlSvx3HPPYe/evUhOTsaKFSuQmJiIoKCgPxT/4cOHMXPmTERFRSEgIAAKhQKJiYlYs2ZNi/uEhoaioqIC69evx8CBA2FhYQE/Pz9oNBqjfrcWTDKZDAaDAUDrn1ttbS0AYNOmTfD19TXaZmpq2q78iIiIiKjr44MqujlXV1dYWloiLS2tTf0zMzPx8MMPY+7cufD09MSQIUOMHs7QyNPTExEREcjKysIDDzyArVu3Stvc3NywYMEC7Nu3D1OnTsXmzZtbHfPIkSNN/m68nyorKwsDBw7EW2+9hTFjxsDV1VV6KEZrOYSHhyMwMBAjRoyAhYVFu+97GzVqFH788cdmH4jRu3dvODs7o6CgAEOGDDFaXFxc2jUOEd2dTpWeguNqR5wqPdXZoRARURfAmapuTi6XY+nSpViyZAnMzc3xyCOP4Nq1azhz5gxmz57dpL+rqyu++uorpKamwsXFBUqlEtnZ2VKxcOnSJWzcuBFTpkyBs7Mz8vPzcf78ebzwwgu4efMmFi9ejGeffRYuLi749ddfkZ2djeDg4FZj3L59O8aMGYOxY8diy5YtOHbsGD777DMpnsLCQiQmJsLb2xt79+7Ft99+2+rxXF1doVQqMWbMGKhUKixevLjdM3bz589HbGwspk+fjoiICCgUChw5cgQ+Pj5wd3dHVFQUwsPDoVAoMGnSJKjVauTk5KCyshILFy5s11hEdPfRGXQov1EOnUHX2aEQEVEXwJmqe8CyZcvw+uuvY/ny5Rg2bBhCQkKke4du9corr2Dq1KkICQmBr68vKioqMHfuXGl7z549kZeXh+DgYLi5uWHOnDmYN28eXnnlFZiamqKiogIvvPAC3NzcMG3aNEyePBlRUVGtxhcVFYXExESMGjUKX331Fb7++msMHz4cADBlyhQsWLAA8+fPh4eHB7KysrBs2bJWj/fZZ5+hsrISXl5eeP755xEeHg4nJ6d2fWb29vb44YcfUFtbC39/f4wePRqbNm2SLhl8+eWXER8fj82bN2PkyJHw9/fHF198wZkqIiIionsQn/73O3/k6X9EbcHzh6hruFJ9BdduXMPZ8rOYtWsWEqYmYJjDMDhZOaGfbb/ODo+IiP5EfPofERFRO6l1anhv8kZpXam0btauWQCA+6zvw+X/uwyLHhadFR4REd3FePkfERERAHNTcwxQDIDJLf80msAE/W37w9zUvJMiIyKiux2LKiIiIjS8VmHV+FUwwGC03gADVo1fBZlM1kmRERHR3Y5FFRER0W8eH/w4vJ29pdkqE5jA29kbjw9+vJMjIyKiuxmLqnZqfDksUXvwvCHqGm6dreIsFRERtcUfelBFXFwcVq9ejZKSEjz44IOIjY2Fj49Ps321Wi2io6Px5ZdfoqioCO7u7njvvfcwadIkqU9NTQ2WLVuGb7/9FmVlZfD09MT69evh7e1tdKyzZ89i6dKlOHjwIHQ6HYYPH46dO3diwIABABqesPb6668jMTERarUaAQEB+Pjjj9G7d+8/kqYRc3NzmJiY4OrVq3B0dIS5uTn/kaXbEkJAo9Hg2rVrMDExgbk578kguts1zlZlX83mLBUREbVJu4uqb775BgsXLsSnn34KX19ffPjhhwgICEB+fn6z7wKKjIxEQkICNm3ahKFDhyI1NRVBQUHIysqCp6cngIZ3/vznP/+BUqmEs7MzEhISMGHCBPz888/o27cvAODixYsYO3YsZs+ejaioKNja2uLMmTNGj6desGAB9u7di+3bt0OhUGD+/PmYOnUqMjMz/+jnIzExMYGLiwuKi4tx9erVDh+P7i09e/bEgAEDYGLCyWGiu51MJsO7j72L8ORwvPvYu/wBjYiIbqvd76ny9fWFt7c3NmzYAKDhsqb+/fvjtddewxtvvNGkv7OzM9566y3MmzdPWhccHAxLS0skJCTg5s2bsLGxwe7du/HEE09IfUaPHo3Jkyfj7bffBgBMnz4dZmZmUCqVzcZVXV0NR0dHbN26Fc8++ywAIC8vD8OGDcPhw4fx0EMPNdlHrVZDrVZLf6tUKvTv3x/l5eWwt7eHTqcDAPTo0QNarRYymQympqaor6+HwWCAqakpNBoNTE1Nm2336NEDJiYmrbYbXyar1WqN2ubm5jAYDNDpdK229Xo99Hp9k7ZOp4MQAmZmZk3at+Z0a5s53bmc9Ho95HI5ZDIZDAbDn56TWq2GmZlZs+3GWTONRmPUtrCwgMFggFarbbbd2d8Tc2JOzIk5MSfmxJyY05+Rk0qlgp2dXZveU9Wun801Gg2OHz+OCRMm/PcAJiaYMGECDh8+3Ow+arW6yctOLS0tkZGRAQDQ6XTS/3i21MdgMGDv3r1wc3NDQEAAnJyc4Ovri6SkJKn/8ePHodVqjWIbOnQoBgwY0GJs0dHRUCgU0tK/f38AwP79+wEAaWlpSEtLAwAkJycjIyMDMpkM33//PU6dOgW5XI49e/YgPz8fcrkc27dvx+XLlyGXy5GQkICrV69CLpcjPj4eFRUVkMvliIuLQ21tLeRyOdatWwetVgsTExOsW7cOJiYm0Gq1WLduHeRyOWpraxEXFwe5XI6KigrEx8dDLpfj6tWrSEhIgFwux+XLl7F9+3bI5XLk5+djz549kMvlOH36NFJSUiCXy5GTk4MDBw5ALpcjMzMTmZmZkMvlOHDgAHJyciCXy5GSkoLTp08zpzuck1KphJmZGfLz86UfBE6ePIlt27YBALKzs6XzOCMjA8nJyS2eewCQlJSE7OxsAMC2bdtw8uRJAIBSqUReXh4AID4+HgUFBQAaLtUtKioCAKxduxbl5eUAgJiYGNTU1ECj0SAmJgYajQY1NTWIiYkBAJSXl2Pt2rUAgKKiIsTFxQEACgoKEB8fD6DhRwvmxJyYE3NiTsyJOTGn7pxTm4l2KCoqEgBEVlaW0frFixcLHx+fZveZMWOGGD58uDh37pzQ6/Vi3759wtLSUpibm0t9/Pz8hL+/vygqKhI6nU4olUphYmIi3NzchBBCFBcXCwCiZ8+eYu3ateKnn34S0dHRQiaTifT0dCGEEFu2bDE6ZiNvb2+xZMmSZmOrr68X1dXV0nLlyhUBQJSXlwshhNBqtUKr1QohhNBoNM221Wq10Ol0zbb1er00Tkttg8EgDAZDk7YQQuj1+hbbarVaCCGETqdrtq3VaoVGo2m2zZyYE3NiTsyJOTEn5sScmBNzaj2nqqoqAUBUV1eL22nX5X9Xr15F3759kZWVBT8/P2n9kiVLcPDgQRw9erTJPteuXUNYWBi+++47yGQyDB48GBMmTMDnn3+OmzdvAmi4X+rvf/87Dh06BFNTU3h5ecHNzQ3Hjx/H2bNnpXFnzJiBrVu3SseeMmUKrKys8PXXX2Pr1q146aWXjC7nAwAfHx+MHz8e77333m3zU6lUUCgUbZriIyIiIiKi7qs9tUG7Lv9zcHCAqakpSktLjdaXlpbivvvua3YfR0dHJCUloa6uDr/88gvy8vJgbW2NQYMGSX0GDx6MgwcPora2FleuXMGxY8eg1WqlPg4ODujRoweGDx9udOxhw4ahsLAQAHDfffdBo9GgqqqqzbERERERERF1VLue/mdubo7Ro0cjLS0NzzzzDICG+53S0tIwf/78VveVy+Xo27cvtFotdu7ciWnTpjXpY2VlBSsrK1RWViI1NRXvv/++NK63tzfy8/ON+p87dw4DBw4E0PBgCzMzM6SlpSE4OBgAkJ+fj8LCQqNZtdY0TtqpVKo29SciIiIiou6psSZo04V9t71A8BaJiYnCwsJCfPHFF+Lnn38Wc+bMEXZ2dqKkpEQIIcTzzz8v3njjDan/kSNHxM6dO8XFixfFoUOHxF//+lfh4uIiKisrpT4pKSkiOTlZFBQUiH379okHH3xQ+Pr6StdQCiHErl27hJmZmdi4caM4f/68iI2NFaampuLHH3+U+rz66qtiwIAB4ocffhA5OTnCz89P+Pn5tTm3xnuquHDhwoULFy5cuHDhwgWAuHLlym3riHa/pyokJATXrl3D8uXLUVJSAg8PD6SkpEgv2C0sLDR6F099fT0iIyNRUFAAa2trBAYGQqlUws7OTupTXV2NiIgI/Prrr+jVqxeCg4PxzjvvSI+uBoCgoCB8+umniI6ORnh4ONzd3bFz506MHTtW6tP4ZLbg4GCjl/+2lbOzM65cuQIbGxu+l4SI6B7W+IqNK1eu8B5bIqJ7lBACNTU1cHZ2vm3fdr+nioiIqLvjg4uIiKg92vWgCiIiIiIiIjLGooqIiIiIiKgDWFQRERHdwsLCAitWrICFhUVnh0JERF0A76kiIiIiIiLqAM5UERERERERdQCLKiIiIiIiog5gUUVERERERNQBLKqIiIiIiIg6gEUVERF1GzKZDElJSd1mHCIi6hpYVBERUZdRUlKC1157DYMGDYKFhQX69++Pp556CmlpaZ0dWru9+OKLeOaZZzo7DCIiugN6dHYAREREbXH58mU88sgjsLOzw+rVqzFy5EhotVqkpqZi3rx5yMvL6+wQiYjoHsWZKiIi6hLmzp0LmUyGY8eOITg4GG5ubhgxYgQWLlyII0eOSP3Ky8sRFBSEnj17wtXVFXv27JG26fV6zJ49Gy4uLrC0tIS7uzvWr19vNE52djYmTpwIBwcHKBQK+Pv748SJE03i6cg4K1euxJdffondu3dDJpNBJpMhPT39Dn5aRET0Z2JRRUREd73r168jJSUF8+bNg5WVVZPtdnZ2UjsqKgrTpk3DqVOnEBgYiJkzZ+L69esAAIPBgH79+mH79u34+eefsXz5crz55pvYtm2btH9NTQ1CQ0ORkZGBI0eOwNXVFYGBgaipqTEasyPjLFq0CNOmTcOkSZNQXFyM4uJiPPzww3f6YyMioj+JTAghOjsIIiKi1hw7dgy+vr7YtWsXgoKCWuwnk8kQGRmJVatWAQDq6upgbW2N5ORkTJo0qdl95s+fj5KSEuzYsaPZ7QaDAXZ2dti6dSuefPLJOzbOiy++iKqqKj7wgoioG+A9VUREdNdrz+9/o0aNktpWVlawtbVFWVmZtC4uLg6ff/45CgsLcfPmTWg0Gnh4eEjbS0tLERkZifT0dJSVlUGv1+PGjRsoLCy8o+MQEVH3wcv/iIjorufq6gqZTNamh1GYmZkZ/S2TyWAwGAAAiYmJWLRoEWbPno19+/YhNzcXL730EjQajdQ/NDQUubm5WL9+PbKyspCbmwt7e3ujPndiHCIi6j44U0VERHe9Xr16ISAgAHFxcQgPD29yX1VVVZXRfVUtyczMxMMPP4y5c+dK6y5evNikz8cff4zAwEAAwJUrV1BeXt6ueNsyjrm5OfR6fbuOS0REdyfOVBERUZcQFxcHvV4PHx8f7Ny5E+fPn8fZs2fx0Ucfwc/Pr03HcHV1RU5ODlJTU3Hu3DksW7YM2dnZTfoolUqcPXsWR48excyZM2FpadmuWNsyzv33349Tp04hPz8f5eXl0Gq17RqDiIjuHiyqiIioSxg0aBBOnDiB8ePH4/XXX8cDDzyAiRMnIi0tDZ988kmbjvHKK69g6tSpCAkJga+vLyoqKoxmkwDgs88+Q2VlJby8vPD8888jPDwcTk5O7Yq1LeOEhYXB3d0dY8aMgaOjIzIzM9s1BhER3T349D8iIiIiIqIO4EwVERERERFRB7CoIiIiIiIi6gAWVURERERERB3AooqIiIiIiKgDWFQRERERERF1AIsqIiIiIiKiDmBRRURERERE1AEsqoiIiIiIiDqARRUREREREVEHsKgiIiIiIiLqABZVREREREREHfD/0rNY/awjcJkAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x1000 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=(10, 10))\n",
    "\n",
    "axs[0].set_prop_cycle(color=['black'])\n",
    "for k in [1, 2]:\n",
    "    axs[k].set_prop_cycle(color=['red', 'green', 'blue'])\n",
    "\n",
    "for k, dname, c in zip(range(len(combined_models)),          \n",
    "                       combined_models.keys(),\n",
    "                       combined_models.values()):\n",
    "    dmt = dmts[dname]\n",
    "    \n",
    "    before = c[0]\n",
    "    after = c[-1]\n",
    "\n",
    "    i = k-0.15\n",
    "    j = k+0.15\n",
    "    \n",
    "    lk, = axs[0].plot(k, before.ntrees, \".\")\n",
    "    axs[0].plot(k, after.ntrees, \"v\", color=lk.get_color())\n",
    "    axs[0].plot([k, k], [before.ntrees, after.ntrees],\n",
    "                color=lk.get_color())\n",
    "    \n",
    "    li, = axs[1].plot(i, before.nnodes, \".\")\n",
    "    axs[1].plot(i, after.nnodes, \"v\", color=li.get_color())\n",
    "    axs[1].plot([i, i], [before.nnodes, after.nnodes], \"-\",\n",
    "                color=li.get_color())\n",
    "    \n",
    "    lk, = axs[1].plot(k, before.nleafs, \".\")\n",
    "    axs[1].plot(k, after.nleafs, \"v\", color=lk.get_color())\n",
    "    axs[1].plot([k, k], [before.nleafs, after.nleafs], \"--\",\n",
    "                color=lk.get_color())\n",
    "\n",
    "    lj, = axs[1].plot(j, before.nnz_leafs, \".\")\n",
    "    axs[1].plot(j, after.nnz_leafs, \"v\", color=lj.get_color())\n",
    "    axs[1].plot([j, j], [before.nnz_leafs, after.nnz_leafs], \"-.\",\n",
    "                color=lj.get_color())\n",
    "    \n",
    "    li, = axs[2].plot(i, before.mtrain, \".\")\n",
    "    axs[2].plot(i, after.mtrain, \"v\", color=li.get_color())\n",
    "    axs[2].plot([i, i], [before.mtrain, after.mtrain], \"-\",\n",
    "                color=li.get_color())\n",
    "    \n",
    "    lk, = axs[2].plot(k, before.mvalid, \".\")\n",
    "    axs[2].plot(k, after.mvalid, \"v\", color=lk.get_color())\n",
    "    axs[2].plot([k, k], [before.mvalid, after.mvalid], \"--\",\n",
    "                color=lk.get_color())\n",
    "\n",
    "    lj, = axs[2].plot(j, before.mtest, \".\")\n",
    "    axs[2].plot(j, after.mtest, \"v\", color=lj.get_color())\n",
    "    axs[2].plot([j, j], [before.mtest, after.mtest], \"-.\",\n",
    "                color=lj.get_color())\n",
    "\n",
    "    #classes, class_counts = np.unique(dmt.y, return_counts=True)\n",
    "    #class_balance = max(class_counts) / sum(class_counts)\n",
    "    #axs[2].plot([i, j], [class_balance, class_balance], \"-\", c=\"gray\")\n",
    "\n",
    "for k in [0, 1, 2]:\n",
    "    axs[k].set_xticks(range(len(combined_models)))\n",
    "    axs[k].set_xticklabels(list(combined_models.keys()))\n",
    "    axs[k].grid(visible=True, axis=\"y\", ls=\":\", color=\"gray\", which=\"major\")\n",
    "    axs[k].grid(visible=True, axis=\"y\", ls=\":\", color=\"lightgray\", which=\"minor\")\n",
    "\n",
    "fig.legend([\n",
    "                Line2D([0], [0], c=\"black\", ls=\"\", marker=\".\"),\n",
    "                Line2D([0], [0], c=\"black\", ls=\"\", marker=\"v\")\n",
    "           ], [\n",
    "                \"before (uncompressed)\",\n",
    "                \"after (compressed)\"\n",
    "           ], loc=\"upper center\", ncols=2)\n",
    "\n",
    "axs[1].legend([\n",
    "                Line2D([0], [0], c=\"red\", ls=\"-\"),\n",
    "                Line2D([0], [0], c=\"green\", ls=\"--\"),\n",
    "                Line2D([0], [0], c=\"blue\", ls=\"-.\"),\n",
    "           ], [\n",
    "                \"num. nodes\",\n",
    "                \"num. leaves\",\n",
    "                \"num. non-zero leaves\",\n",
    "           ])\n",
    "\n",
    "axs[2].legend([\n",
    "                Line2D([0], [0], c=\"red\", ls=\"-\"),\n",
    "                Line2D([0], [0], c=\"green\", ls=\"--\"),\n",
    "                Line2D([0], [0], c=\"blue\", ls=\"-.\"),\n",
    "                Line2D([0], [0], c=\"gray\", ls=\"-\"),\n",
    "           ], [\n",
    "                \"train\",\n",
    "                \"validation\",\n",
    "                \"test\",\n",
    "                \"class balance\",\n",
    "           ])\n",
    "\n",
    "axs[0].set_title(\"number of trees in ensemble\")\n",
    "axs[1].set_title(\"number of nodes, leaves, and non-zero leaves\")\n",
    "axs[2].set_title(\"model accuracies on training, validation and test sets\")\n",
    "\n",
    "axs[1].set_yscale(\"log\")\n",
    "axs[0].yaxis.set_minor_locator(MultipleLocator(20))\n",
    "axs[2].yaxis.set_minor_locator(MultipleLocator(0.02))\n",
    "\n",
    "fig.subplots_adjust(top=0.94, hspace=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5914b0e0-14bf-4947-82d9-b50ea2c44e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>validation</th>\n",
       "      <th>test</th>\n",
       "      <th>ntrees</th>\n",
       "      <th>nnodes</th>\n",
       "      <th>nleaves</th>\n",
       "      <th>nnz_leaves</th>\n",
       "      <th>nleaf_ratio</th>\n",
       "      <th>nnz_ratio</th>\n",
       "      <th>test_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Chaahat</th>\n",
       "      <th>before</th>\n",
       "      <td>99.9</td>\n",
       "      <td>99.8</td>\n",
       "      <td>99.8</td>\n",
       "      <td>500</td>\n",
       "      <td>49576</td>\n",
       "      <td>25038</td>\n",
       "      <td>25017</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>after</th>\n",
       "      <td>99.8</td>\n",
       "      <td>99.6</td>\n",
       "      <td>99.7</td>\n",
       "      <td>115</td>\n",
       "      <td>3697</td>\n",
       "      <td>1906</td>\n",
       "      <td>1332</td>\n",
       "      <td>13.14</td>\n",
       "      <td>18.78</td>\n",
       "      <td>0.0013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train validation  test ntrees nnodes nleaves nnz_leaves  \\\n",
       "Chaahat before  99.9       99.8  99.8    500  49576   25038      25017   \n",
       "        after   99.8       99.6  99.7    115   3697    1906       1332   \n",
       "\n",
       "               nleaf_ratio nnz_ratio test_ratio  \n",
       "Chaahat before           -         -          -  \n",
       "        after        13.14     18.78     0.0013  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "nleaf_ratio     13.14\n",
       "nnz_ratio       18.78\n",
       "test_ratio     0.0013\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index_acc = pd.MultiIndex.from_product((dnames, [\"before\", \"after\"]))\n",
    "df = pd.DataFrame(\"-\", columns=[\"train\", \"validation\", \"test\",\n",
    "                                \"ntrees\", \"nnodes\", \"nleaves\", \"nnz_leaves\",\n",
    "                                \"nleaf_ratio\", \"nnz_ratio\",\n",
    "                                \"test_ratio\"], index=index_acc)\n",
    "\n",
    "for k, (dname, c) in enumerate(combined_models.items()):\n",
    "    before = c[0]\n",
    "    after = c[-1]\n",
    "\n",
    "    rbefore = df.loc[(dname, \"before\"), :]\n",
    "    rafter = df.loc[(dname, \"after\"), :]\n",
    "\n",
    "    rbefore[\"train\"] = np.round(before.mtrain*100, 1)\n",
    "    rafter[\"train\"] = np.round(after.mtrain*100, 1)\n",
    "    rbefore[\"validation\"] = np.round(before.mvalid*100, 1)\n",
    "    rafter[\"validation\"] = np.round(after.mvalid*100, 1)\n",
    "    rbefore[\"test\"] = np.round(before.mtest*100, 1)\n",
    "    rafter[\"test\"] = np.round(after.mtest*100, 1)\n",
    "\n",
    "    rbefore[\"ntrees\"] = before.ntrees\n",
    "    rafter[ \"ntrees\"] = after.ntrees\n",
    "    rbefore[\"nnodes\"] = before.nnodes\n",
    "    rafter[ \"nnodes\"] = after.nnodes\n",
    "    rbefore[\"nleaves\"] = before.nleafs\n",
    "    rafter[ \"nleaves\"] = after.nleafs\n",
    "    rbefore[\"nnz_leaves\"] = before.nnz_leafs\n",
    "    rafter[ \"nnz_leaves\"] = after.nnz_leafs\n",
    "    rafter[ \"nleaf_ratio\"] = np.round(before.nleafs / after.nleafs, 2)\n",
    "    rafter[ \"nnz_ratio\"] = np.round(before.nnz_leafs / after.nnz_leafs, 2)\n",
    "    rafter[ \"test_ratio\"] = 1.0-np.round(after.mtest / before.mtest, 4)\n",
    "\n",
    "display(df)\n",
    "df_after = df.xs(\"after\", level=1)\n",
    "display(df_after[[\"nleaf_ratio\", \"nnz_ratio\", \"test_ratio\"]].mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370953ab-7970-4852-a5b0-66ccc01bfdf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
